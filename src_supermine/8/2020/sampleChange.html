<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
answer.add(Dropout(0.3))
answer.add(Dense(vocab_size))
&#47&#47 we output a probability distribution over the vocabulary
answer.add(<a id="change">Activation(&quotsoftmax&quot)</a>)

answer.compile(optimizer=&quotrmsprop&quot, loss=&quotcategorical_crossentropy&quot,
               metrics=[&quotaccuracy&quot])</code></pre><h3>After Change</h3><pre><code class='java'>
match = Activation(&quotsoftmax&quot)(match)

&#47&#47 add the match matrix with the second input vector sequence
<a id="change">response = add([match, input_encoded_c])</a>  &#47&#47 (samples, story_maxlen, query_maxlen)
response = Permute((2, 1))(<a id="change">response</a>)  &#47&#47 (samples, query_maxlen, story_maxlen)

&#47&#47 concatenate the match matrix with the question vector sequence
answer = concatenate(<a id="change">[response, question_encoded]</a>)

&#47&#47 the original paper uses a matrix multiplication for this reduction step.
&#47&#47 we choose to use a RNN instead.
answer = LSTM(32)(answer)  &#47&#47 (samples, 32)

&#47&#47 one regularization layer -- more would probably be needed.
answer = Dropout(0.3)(answer)
answer = Dense(vocab_size)(answer)  &#47&#47 (samples, vocab_size)
&#47&#47 we output a probability distribution over the vocabulary
answer = Activation(&quotsoftmax&quot)(answer)

&#47&#47 build the final model
<a id="change">model = Model([input_sequence, question], answer)</a>
model.compile(optimizer=&quotrmsprop&quot, loss=&quotcategorical_crossentropy&quot,
               metrics=[&quotaccuracy&quot])

&#47&#47 train</code></pre>