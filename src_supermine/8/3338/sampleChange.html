<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                swords = [ &quot &quot.join([vocab._id_to_token_map_py[i] for i in sent]) for sent in source ]
                dwords = [ &quot &quot.join([vocab._id_to_token_map_py[i] for i in sent]) for sent in dec_in ]
                twords = [ &quot &quot.join([vocab._id_to_token_map_py[i] for i in sent]) for sent in target ]
                <a id="change">if step &lt; 1000:
                    logging.info(&quotsource:{}&quot.format(source))
                    logging.info(&quotswords:{}&quot.format(swords))
                    logging.info(&quotdec_in:{}&quot.format(dec_in))
                    logging.info(&quotdwords:{}&quot.format(dwords))
                    logging.info(&quottarget:{}&quot.format(target))
                    logging.info(&quottwords:{}&quot.format(twords))
               </a> writer.add_summary(mgd, global_step=step)
                if step % 1000 == 0:
                    print(&quotstep:{} loss:{}&quot.format(step, loss))
                    saver.save(sess, logdir+&quotmy-model&quot, global_step=step)</code></pre><h3>After Change</h3><pre><code class='java'>
    else:
        learning_rate = 2 * tf.minimum(1.0, (fstep / opt_hparams[&quotwarmup_steps&quot])) \
            * tf.rsqrt(tf.maximum(fstep, opt_hparams[&quotwarmup_steps&quot])) \
            * <a id="change">encoder_hparams</a>[<a id="change">&quotembedding&quot</a>][&quotdim&quot]**-0.5
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
            beta1=0.9, beta2=0.98, epsilon=1e-6)
    train_op = optimizer.minimize(mle_loss, global_step)</code></pre>