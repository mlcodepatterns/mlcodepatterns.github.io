<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.train_loader = None
        self.validation_loader = None
        self.use_fp16 = use_fp16
        <a id="change">self.apex_args = apex_args or {}</a>
        if use_fp16 and not amp:
            raise ImportError(
                "Please install apex from "
                "https://www.github.com/nvidia/apex to use fp16 training.")</code></pre><h3>After Change</h3><pre><code class='java'>
                return loaders
            else:
                raise ValueError(
                    "Number of loaders must be &lt;= <a id="change">2. Got {}".format(loaders))
        &#47&#47 No great way of checking type otherwise
        return loaders, None

    def _initialize_dataloaders(self):
        logger.debug("Instantiating dataloaders.")
        &#47&#47 When creating loaders, a filelock will be used to ensure no
        &#47&#47 race conditions in data downloading among different workers.
        with FileLock(os.path.join(tempfile.gettempdir(), ".ray_data.l</a>ock")):
            loaders = self.data_creator(self.config)
            train_loader, val_loader = self._validate_loaders(loaders)
            if not isinstance(train_loader, torch.utils.data.DataLoader):</code></pre>