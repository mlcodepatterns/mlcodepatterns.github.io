<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def get_constants(self, X, train=False):
        nb_samples = K.shape(X)[0]
        <a id="change">if K._BACKEND == &quottensorflow&quot:
            nb_samples = int(nb_samples)
       </a> retain_p_W = 1. - self.p_W
        retain_p_U = 1. - self.p_U
        if train and self.p_W &gt; 0 and self.p_U &gt; 0:
            B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)</code></pre><h3>After Change</h3><pre><code class='java'>

    def get_constants(self, X, train=False):
        nb_samples = K.shape(X)[0]
        <a id="change">if K._BACKEND == &quottensorflow&quot and train and self.p_W &gt; 0 and self.p_U &gt; 0:
            if not self.input_shape[0]:
                raise Exception(&quotFor RNN dropout in tensorflow, a complete &quot +
                                &quotinput_shape must be provided (including batch size).&quot)
            nb_samples = self.input_shape[0]
       </a> retain_p_W = 1. - self.p_W
        retain_p_U = 1. - self.p_U
        if train and self.p_W &gt; 0 and self.p_U &gt; 0:
            B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)</code></pre>