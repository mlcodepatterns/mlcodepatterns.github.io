<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 shape = [B, L, D]
        embed_query = embedding(query)
        &#47&#47 shape = [B, L]
        atten_mask = <a id="change">K.any(K.not_equal(query, self._params[&quotmask_value&quot]),
                           axis=-1, keepdims=True)</a>
        atten_mask = K.cast(atten_mask, K.floatx())
        atten_mask = K.expand_dims(atten_mask, axis=2)
        &#47&#47 shape = [B, L, D]
        attention_probs = self.attention_layer(embed_query, atten_mask)

        &#47&#47 Process right input.
        &#47&#47 shape = [B, L, 1]
        dense_output = self._make_multi_layer_perceptron_layer()(match_hist)

        &#47&#47 shape = [B, 1, 1]
        dot_score = keras.layers.Dot(axes=[1, 1])(
            [attention_probs, dense_output])

        flatten_score = keras.layers.Flatten()(dot_score)

        x_out = self._make_output_layer()(flatten_score)
        <a id="change">self._backend</a> = keras.Model(inputs=[query, match_hist], outputs=x_out)

    @classmethod
    def attention_layer(cls, attention_input: typing.Any,</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 shape = [B, L, D]
        embed_query = embedding(query)
        &#47&#47 shape = [B, L]
        atten_mask = <a id="change">K.not_equal(query, self._params[&quotmask_value&quot])</a>
        &#47&#47 shape = [B, L]
        atten_mask = K.cast(atten_mask, K.floatx())
        &#47&#47 shape = [B, L, D]
        atten_mask = K.expand_dims(atten_mask, axis=2)
        &#47&#47 shape = [B, L, D]
        attention_probs = self.attention_layer(embed_query, atten_mask)

        &#47&#47 Process right input.
        &#47&#47 shape = [B, L, 1]
        dense_output = self._make_multi_layer_perceptron_layer()(match_hist)

        &#47&#47 shape = [B, 1, 1]
        dot_score = keras.layers.Dot(axes=[1, 1])(
            [attention_probs, dense_output])

        flatten_score = keras.layers.Flatten()(dot_score)

        x_out = self._make_output_layer()(flatten_score)
        <a id="change">self._backend</a> = keras.Model(inputs=[query, match_hist], outputs=x_out)

    @classmethod
    def attention_layer(cls, attention_input: typing.Any,</code></pre>