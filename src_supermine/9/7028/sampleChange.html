<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def __call__(self, prev_layer, **kwargs):

        <a id="change">if self.__class__.__name__ in tl.layers.inputs.__all__:
            &#47&#47 1. for input layers
            &#47&#47 Input layers should use tf.convert_to_tensor to make sure the inputs is converted into tf.Tensor

            &#47&#47 code in tl 1.0
            &#47&#47 raise RuntimeError("Please use layers in `tl.layers.inputs` to convert Variable/Tensor/Placeholder/Numpy arrays to a TL layer")
            &#47&#47 FIXME: not sure convert_to_tensor here or ask user to do it
            self.inputs = tf.convert_to_tensor(prev_layer)
            self._input_layer = None
            self._built = True
            self.build(self._inputs_shape)
            self.outputs = self.forward(self.inputs, **kwargs)

        elif isinstance(prev_layer, Layer):
            &#47&#47 2. for normal layer have only 1 input i.e. DenseLayer
            &#47&#47 Hint : list(), dict() is pass by value (shallow), without them,
            &#47&#47 it is pass by reference.

            self.inputs = prev_layer.outputs
            self._input_layer = prev_layer

            if not self._built:
                self.build(self._inputs_shape)
                self._built = True

            self.outputs = self.forward(self.inputs, **kwargs)
            &#47&#47 self._outputs_shape = self.outputs.get_shape().as_list()

            &#47&#47 TODO: need update
            &#47&#47 self._add_layers(prev_layer.all_layers)
            &#47&#47 self._add_weights(self._weights)
            &#47&#47 self._add_weights(prev_layer.all_weights)
            &#47&#47 self._add_dropout_layers(prev_layer.all_drop)

        elif isinstance(prev_layer, list):
            &#47&#47 3. for layer have multiply inputs i.e. ConcatLayer

            self.inputs = [layer.outputs for layer in prev_layer]
            self._input_layer = prev_layer &#47&#47 FIXME: not sure how to deal with it

            &#47&#47 FIXME: only support concat/elementwise, where build does nothing
            if not self._built:
                self._built = True

            self.outputs = self.forward(self.inputs, **kwargs)

            &#47&#47 TODO: need update
            &#47&#47 self._add_layers(sum([l.all_layers for l in prev_layer], []))
            &#47&#47 self._add_weights(sum([l.all_weights for l in prev_layer], []))
            &#47&#47 self._add_dropout_layers(sum([list(l.all_drop.items()) for l in prev_layer], []))

        else:
            &#47&#47 FIXME: not sure if there is other cases
            pass
            &#47&#47 elif prev_layer is not None:
            &#47&#47     &#47&#47 4. tl.models
            &#47&#47     self._add_layers(prev_layer.all_layers)
            &#47&#47     self._add_weights(prev_layer.all_weights)
            &#47&#47     self._add_dropout_layers(prev_layer.all_drop)
            &#47&#47
            &#47&#47     if hasattr(prev_layer, "outputs"):
            &#47&#47         self.inputs = prev_layer.outputs

       </a> return self

    def _release_memory(self):
        &quot&quot&quot</code></pre><h3>After Change</h3><pre><code class='java'>
        return self._weights

    def __call__(self, inputs, **kwargs):
        <a id="change">if self.__class__.__name__ in tl.layers.inputs.__all__:
            self.inputs = tf.convert_to_tensor(inputs)
        else:
            self.inputs = inputs

       </a> if not self._built:
            self.build(self._inputs_shape)
            self._built = True
</code></pre>