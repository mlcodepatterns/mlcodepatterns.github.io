<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        else:
            self.vocab_size = self.num_classes

        <a id="change">self.embeddings_dec</a> = Embedding(num_classes, embedding_size)
        <a id="change">self.decoder_cell</a> = LSTMCell(state_size)

        if attention_mechanism:
            if attention_mechanism == &quotbahdanau&quot:
                pass
            elif attention_mechanism == &quotluong&quot:
                self.attention_mechanism = tfa.seq2seq.LuongAttention(
                    state_size,
                    None,  &#47&#47 todo tf2: confirm on need
                    memory_sequence_length=max_sequence_length  &#47&#47 todo tf2: confirm inputs or output seq length
                )
            else:
                raise ValueError(
                    "Attention specificaiton &quot{}&quot is invalid.  Valid values are "
                    "&quotbahdanau&quot or &quotluong&quot.".format(self.attention_mechanism))

            <a id="change">self.decoder_cell</a> = tfa.seq2seq.AttentionWrapper(
                self.decoder_cell,
                self.attention_mechanism
            )

        self.sampler = tfa.seq2seq.sampler.TrainingSampler()

        <a id="change">self.projection_layer = Dense(
            units=num_classes,
            use_bias=use_bias,
            kernel_initializer=weights_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=weights_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer
        )</a>

        self.decoder = \
            tfa.seq2seq.basic_decoder.BasicDecoder(self.decoder_cell,
                                                    self.sampler,</code></pre><h3>After Change</h3><pre><code class='java'>
        self.decoder_embedding = tf.keras.layers.Embedding(
            input_dim=output_vocab_size,
            output_dim=embedding_dims)
        <a id="change">self.dense_layer</a> = tf.keras.layers.Dense(output_vocab_size)
        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)

        &#47&#47 Sampler
        self.sampler = tfa.seq2seq.sampler.TrainingSampler()

        self.attention_mechanism = None
        self.rnn_units = rnn_units

        print(&quotsetting up attention for&quot, attention_mechanism)
        <a id="change">if attention_mechanism is not None:
            self.attention_mechanism = self.build_attention_mechanism(
                attention_mechanism,
                dense_units
            )
            self.decoder_rnncell = self.build_rnn_cell()

       </a> self.decoder = tfa.seq2seq.BasicDecoder(self.decoder_rnncell,
                                                sampler=self.sampler,
                                                output_layer=self.dense_layer)
</code></pre>