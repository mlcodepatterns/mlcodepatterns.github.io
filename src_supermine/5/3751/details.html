<html><h3>f19ace982075ea009af81f5e9f687cc2276f50ea,scripts/bert/fp16_utils.py,,grad_global_norm,#Any#Any#,24
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 reduce
    ctx, dtype = arrays[0].context, &quotfloat32&quot
    norms = [nd.add_n(*g).as_in_context(ctx) <a id="change">for</a> g in norm_groups.values()]
    total_norm = nd.add_n(*norms).sqrt()
    scale = total_norm / max_norm
    &#47&#47 is_finite = 0 if NaN or Inf, 1 otherwise.</code></pre><h3>After Change</h3><pre><code class='java'>
        batch_size : int
            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
        m<a id="change">ax_norm : NDArray, optional, default is None
            max value for global 2-norm of gradients.
        
        self.fp32_trainer.allreduce_grads()
        step_size = batch_size * self._scaler.loss_scale
       </a> if max_norm:
            _, ratio, is_finite = nlp.utils.grad_global_norm(self.fp32_trainer._params,
                                                             max_norm * self._scaler.loss_scale)
            step_size = ratio * step_size</code></pre><img src="17649933.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dmlc/gluon-nlp/commit/f19ace982075ea009af81f5e9f687cc2276f50ea#diff-978189d71446b5d60c49613b692de0fa5d50ae07bd06f3d620526f814c3a677eL64' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/gluon-nlp</div><div id='commit'> Commit Name: f19ace982075ea009af81f5e9f687cc2276f50ea</div><div id='time'> Time: 2020-01-20</div><div id='author'> Author: 50716238+MoisesHer@users.noreply.github.com</div><div id='file'> File Name: scripts/bert/fp16_utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: grad_global_norm</div><BR><BR><div id='link'><a href='https://github.com/williamFalcon/pytorch-lightning/commit/9f12ca095ab6e3295bd03fd1e50130a12b11569c#diff-8cc348656e5d5c12f8bb6930e41240da015887af602c1fcc4a0190420d4f7588L176' target='_blank'>Link</a></div><div id='project'> Project Name: williamFalcon/pytorch-lightning</div><div id='commit'> Commit Name: 9f12ca095ab6e3295bd03fd1e50130a12b11569c</div><div id='time'> Time: 2021-02-11</div><div id='author'> Author: carlossmocholi@gmail.com</div><div id='file'> File Name: pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py</div><div id='class'> Class Name: HookResultStore</div><div id='method'> Method Name: auto_reduce_results_on_epoch_end</div><BR><BR><div id='link'><a href='https://github.com/OpenMined/PySyft/commit/489972a9e467eb20a7ebbe2b5e37c63b3d5bb2aa#diff-b2ea4f890728d0b8aa407b0e8081cd7747fe2207f1cbde15a1053ec3522e5dfbL1146' target='_blank'>Link</a></div><div id='project'> Project Name: OpenMined/PySyft</div><div id='commit'> Commit Name: 489972a9e467eb20a7ebbe2b5e37c63b3d5bb2aa</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: murarugeorgec@gmail.com</div><div id='file'> File Name: syft/frameworks/torch/tensors/interpreters/additive_shared.py</div><div id='class'> Class Name: AdditiveSharingTensor</div><div id='method'> Method Name: get_garbage_collect_data</div><BR>