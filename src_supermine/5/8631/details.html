<html><h3>3a71ecffa3a8aff931a0ff865434e11e8ea08ba3,onmt/translate/translator.py,Translator,_fast_translate_batch,#Translator#Any#Any#Any#Any#Any#Any#,328
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    if alive_attn is not None else None)
                for i in range(is_finished.size(0)):
                    b = batch_offset[i]
                    if <a id="change">end_condition[i]</a>:
                        is_finished[i].fill_(1)
                    finished_hyp = is_finished[i].nonzero().view(-1)
                    &#47&#47 Store finished hypotheses for this batch.</code></pre><h3>After Change</h3><pre><code class='java'>
        memory_bank = tile(memory_bank, beam_size, dim=1)
        memory_lengths = tile(src_lengths, beam_size)

        top_beam_finished = <a id="change">torch.zeros([batch_size], dtype=torch.uint8)</a>
        batch_offset = torch.arange(batch_size, dtype=torch.long)
        beam_offset = torch.arange(
            0,
            batch_size * beam_size,
            step=beam_size,
            dtype=torch.long,
            device=memory_bank.device)
        alive_seq = torch.full(
            [batch_size * beam_size, 1],
            start_token,
            dtype=torch.long,
            device=memory_bank.device)
        alive_attn = None

        &#47&#47 Give full probability to the first beam on the first step.
        topk_log_probs = (
            torch.tensor([0.0] + [float("-inf")] * (beam_size - 1),
                         device=memory_bank.device).repeat(batch_size))

        &#47&#47 Structure that holds finished hypotheses.
        hypotheses = [[] for _ in range(batch_size)]  &#47&#47 noqa: F812

        results = {}
        results["predictions"] = [[] for _ in range(batch_size)]  &#47&#47 noqa: F812
        results["scores"] = [[] for _ in range(batch_size)]  &#47&#47 noqa: F812
        results["attention"] = [[] for _ in range(batch_size)]  &#47&#47 noqa: F812
        results["gold_score"] = [0] * batch_size
        results["batch"] = batch

        for step in range(max_length):
            decoder_input = alive_seq[:, -1].view(1, -1, 1)

            &#47&#47 Decoder forward.
            dec_out, dec_states, attn = self.model.decoder(
                decoder_input,
                memory_bank,
                dec_states,
                memory_lengths=memory_lengths,
                step=step)

            &#47&#47 Generator forward.
            log_probs = self.model.generator.forward(dec_out.squeeze(0))
            vocab_size = log_probs.size(-1)

            if step &lt; min_length:
                log_probs[:, end_token] = -1e20

            &#47&#47 Multiply probs by the beam probability.
            log_probs += topk_log_probs.view(-1).unsqueeze(1)

            alpha = self.global_scorer.alpha
            length_penalty = ((5.0 + (step + 1)) / 6.0) ** alpha

            &#47&#47 Flatten probs into a list of possibilities.
            curr_scores = log_probs / length_penalty
            curr_scores = curr_scores.reshape(-1, beam_size * vocab_size)
            topk_scores, topk_ids = curr_scores.topk(beam_size, dim=-1)

            &#47&#47 Recover log probs.
            topk_log_probs = topk_scores * length_penalty

            &#47&#47 Resolve beam origin and true word ids.
            topk_beam_index = topk_ids.div(vocab_size)
            topk_ids = topk_ids.fmod(vocab_size)

            &#47&#47 Map beam_index to batch_index in the flat representation.
            batch_index = (
                    topk_beam_index
                    + beam_offset[:topk_beam_index.size(0)].unsqueeze(1))
            select_indices = batch_index.view(-1)

            &#47&#47 Append last prediction.
            alive_seq = torch.cat(
                [alive_seq.index_select(0, select_indices),
                 topk_ids.view(-1, 1)], -1)
            if return_attention:
                current_attn = attn["std"].index_select(1, select_indices)
                if alive_attn is None:
                    alive_attn = current_attn
                else:
                    alive_attn = alive_attn.index_select(1, select_indices)
                    alive_attn = torch.cat([alive_attn, current_attn], 0)

            is_finished = topk_ids.to(&quotcpu&quot).eq(end_token)
            if step + 1 == max_length:
                is_finished.fill_(1)
            top_beam_finished |= is_finished[:, 0].eq(1)

            &#47&#47 Save finished hypotheses.
            if is_finished.any():
                predictions = alive_seq.view(-1, beam_size, alive_seq.size(-1))
                attention = (
                    alive_attn.view(
                        alive_attn.size(0), -1, beam_size, alive_attn.size(-1))
                    if alive_attn is not None else None)
                non_finished_batch = []
                for i in range(is_finished.size(0)):
                    b = batch_offset[i]
                    finished_hyp = is_finished[i].nonzero().view(-1)
                    &#47&#47 Store finished hypotheses for this batch.
                    for j in finished_hyp:
                        hypotheses[b].append((
                            topk_scores[i, j],
                            predictions[i, j, 1:],  &#47&#47 Ignore start_token.
                            attention[:, i, j, :memory_lengths[i]]
                            if attention is not None else None))
                    &#47&#47 End condition is the top beam finished and we can return
                    &#47&#47 n_best hypotheses.
                    if top_beam_finished[i] and len(hypotheses[b]) &gt;= n_best:
                        best_hyp = sorted(
                            hypotheses[b], key=lambda x: x[0], reverse=True)
                        for n, (score, pred, attn) in enumerate(best_hyp):
                            if n &gt;= n_best:
                                break
                            results["scores"][b].append(score)
                            results["predictions"][b].append(pred)
                            results["attention"][b].append(
                                attn if attn is not None else [])
                    else:
                        non_finished_batch.append(i)
                non_finished = torch.tensor(non_finished_batch)
                &#47&#47 If all sentences are translated, no need to go further.
                if len(non_finished) == 0:
                    break
                &#47&#47 Remove finished batches for the next step.
                <a id="change">top_beam_finished = top_beam_finished.index_select(
                    0, non_finished)</a>
                batch_offset = batch_offset.index_select(0, non_finished)
                non_finished = non_finished.to(topk_ids.device)
                topk_log_probs = topk_log_probs.index_select(0, non_finished)
                batch_index = batch_index.index_select(0, non_finished)</code></pre><img src="40149351.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-py/commit/3a71ecffa3a8aff931a0ff865434e11e8ea08ba3#diff-004ff622c411caa26172d7c3ca873f7011649e0573e40798b4ce022426fe6444L346' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-py</div><div id='commit'> Commit Name: 3a71ecffa3a8aff931a0ff865434e11e8ea08ba3</div><div id='time'> Time: 2018-10-22</div><div id='author'> Author: guillaume.klein@systrangroup.com</div><div id='file'> File Name: onmt/translate/translator.py</div><div id='class'> Class Name: Translator</div><div id='method'> Method Name: _fast_translate_batch</div><BR><BR><div id='link'><a href='https://github.com/chainer/chainercv/commit/7e1e3ecee5930c527c7f1c89cdd4719d826095e5#diff-6393cb5fb65b66252d4542b43deb4b84509ef4f4eea4dbab5237832c7b692969L36' target='_blank'>Link</a></div><div id='project'> Project Name: chainer/chainercv</div><div id='commit'> Commit Name: 7e1e3ecee5930c527c7f1c89cdd4719d826095e5</div><div id='time'> Time: 2019-03-05</div><div id='author'> Author: yuyuniitani@gmail.com</div><div id='file'> File Name: chainercv/links/model/mask_rcnn/misc.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: mask_to_segm</div><BR><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-py/commit/3a71ecffa3a8aff931a0ff865434e11e8ea08ba3#diff-004ff622c411caa26172d7c3ca873f7011649e0573e40798b4ce022426fe6444L346' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-py</div><div id='commit'> Commit Name: 3a71ecffa3a8aff931a0ff865434e11e8ea08ba3</div><div id='time'> Time: 2018-10-22</div><div id='author'> Author: guillaume.klein@systrangroup.com</div><div id='file'> File Name: onmt/translate/translator.py</div><div id='class'> Class Name: Translator</div><div id='method'> Method Name: _fast_translate_batch</div><BR><BR><div id='link'><a href='https://github.com/NTMC-Community/MatchZoo/commit/c43310c317f67f13d7a80c1b1cdbc10abf28301b#diff-3c7d03da8ddfec36b4cf0044bedce16a239d149a4f5693411c0b4aba888c0a96L69' target='_blank'>Link</a></div><div id='project'> Project Name: NTMC-Community/MatchZoo</div><div id='commit'> Commit Name: c43310c317f67f13d7a80c1b1cdbc10abf28301b</div><div id='time'> Time: 2017-06-18</div><div id='author'> Author: fanyixing111@hotmail.com</div><div id='file'> File Name: matchzoo/metrics/rank_evaluations.py</div><div id='class'> Class Name: rank_eval</div><div id='method'> Method Name: ndcg</div><BR><BR><div id='link'><a href='https://github.com/tryolabs/luminoth/commit/1de812fb2b372d03763d84d38636ae87799b09d8#diff-1f76e18d573d1b1dcf0545405c7f5a6a29b34a656c29c3b55c964fbdebec125fL45' target='_blank'>Link</a></div><div id='project'> Project Name: tryolabs/luminoth</div><div id='commit'> Commit Name: 1de812fb2b372d03763d84d38636ae87799b09d8</div><div id='time'> Time: 2017-11-10</div><div id='author'> Author: javirey@gmail.com</div><div id='file'> File Name: luminoth/utils/predicting.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_prediction</div><BR>