<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        Return:
            emb (FloatTensor): len x batch x input_size
        
        <a id="change">word = self.word_lut(src_input[:, :, 0])</a>
        emb = word
        if self.feature_dicts:
            features = [feature_lut(src_input[:, :, j+1])
                        for j, feature_lut in enumerate(self.feature_luts)]

            &#47&#47 Apply one MLP layer.
            emb = self.activation(
                self.linear(torch.cat([word] + features, -1)))

        if self.positional_encoding:
            emb = emb + Variable(self.pe[:emb.size(0), :1, :emb.size(2)]
                                 .expand_as(emb))
            <a id="change">emb = self.dropout(emb)</a>
        return emb


class Encoder(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
            emb (FloatTensor): len x batch x sum of feature embedding sizes
        
        feat_inputs = (feat.squeeze(2) for feat in src_input.split(1, dim=2))
        features = [lut(feat) <a id="change">for</a> lut, feat in zip(self.emb_luts, feat_inputs)]
        emb = self.merge(features)
        return emb
</code></pre>