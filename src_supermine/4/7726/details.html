<html><h3>a6652b0c1997bb47dd502bf674e0b3b9b2d09d23,examples/reinforcement_learning/tutorial_cartpole_ac.py,Actor,learn,#Actor#Any#Any#Any#,119
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def learn(self, s, a, td):
            &#47&#47 _, exp_v = self.sess.run([self.train_op, self.exp_v], {self.s: [s], self.a: [a], self.td_error: td[0]})
        with tf.GradientTape() as tape:
            _logits = <a id="change">self.model([s]).outputs</a>
            &#47&#47 _probs = tf.nn.softmax(_logits)
            _exp_v = tl.rein.cross_entropy_reward_loss(logits=_logits, actions=[a], rewards=td[0])
        grad = tape.gradient(_exp_v, self.model.trainable_weights)
        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))</code></pre><h3>After Change</h3><pre><code class='java'>

    def learn(self, s, a, td):
        with tf.GradientTape() as tape:
            _logits = <a id="change">self.model(np.array([s]))</a>
            &#47&#47&#47&#47 cross-entropy loss weighted by td-error (advantage), 
            &#47&#47 the cross-entropy mearsures the difference of two probability distributions: the predicted logits and sampled action distribution,
            &#47&#47 then weighted by the td-error: small difference of real and predict actions for large td-error (advantage); and vice versa. 
            _exp_v = tl.rein.cross_entropy_reward_loss(logits=_logits, actions=[a], rewards=td[0])  </code></pre><img src="36538639.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/a6652b0c1997bb47dd502bf674e0b3b9b2d09d23#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L87' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: a6652b0c1997bb47dd502bf674e0b3b9b2d09d23</div><div id='time'> Time: 2019-05-16</div><div id='author'> Author: 1402434478@qq.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Actor</div><div id='method'> Method Name: learn</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/a6652b0c1997bb47dd502bf674e0b3b9b2d09d23#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L172' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: a6652b0c1997bb47dd502bf674e0b3b9b2d09d23</div><div id='time'> Time: 2019-05-16</div><div id='author'> Author: 1402434478@qq.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Critic</div><div id='method'> Method Name: learn</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/a6652b0c1997bb47dd502bf674e0b3b9b2d09d23#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L129' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: a6652b0c1997bb47dd502bf674e0b3b9b2d09d23</div><div id='time'> Time: 2019-05-16</div><div id='author'> Author: 1402434478@qq.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Actor</div><div id='method'> Method Name: choose_action</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/a6652b0c1997bb47dd502bf674e0b3b9b2d09d23#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L135' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: a6652b0c1997bb47dd502bf674e0b3b9b2d09d23</div><div id='time'> Time: 2019-05-16</div><div id='author'> Author: 1402434478@qq.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Actor</div><div id='method'> Method Name: choose_action_greedy</div><BR>