<html><h3>ff9c1dac887643e464f5f829c7d8b920b0aa8140,rllib/agents/ddpg/ddpg_torch_policy.py,,ddpg_actor_critic_loss,#Any#Any#Any#Any#,30
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                std=policy.config["target_noise"]), -target_noise_clip,
            target_noise_clip)
        policy_tp1_smoothed = torch.clamp(policy_tp1 + clipped_normal_sample,
                                          <a id="change">policy.action_space.low.item(0)</a>,
                                          policy.action_space.high.item(0))
    else:
        &#47&#47 No smoothing, just use deterministic actions.
        policy_tp1_smoothed = policy_tp1

    &#47&#47 Q-net(s) evaluation.
    &#47&#47 prev_update_ops = set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS))
    &#47&#47 Q-values for given actions & observations in given current
    q_t = model.get_q_values(model_out_t, train_batch[SampleBatch.ACTIONS])

    &#47&#47 Q-values for current policy (no noise) in given current state
    q_t_det_policy = model.get_q_values(model_out_t, policy_t)

    actor_loss = -torch.mean(q_t_det_policy)

    if twin_q:
        twin_q_t = model.get_twin_q_values(model_out_t,
                                           train_batch[SampleBatch.ACTIONS])
    &#47&#47 q_batchnorm_update_ops = list(
    &#47&#47     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)

    &#47&#47 Target q-net(s) evaluation.
    q_tp1 = policy.target_model.get_q_values(target_model_out_tp1,
                                             policy_tp1_smoothed)

    if twin_q:
        twin_q_tp1 = policy.target_model.get_twin_q_values(
            target_model_out_tp1, policy_tp1_smoothed)

    q_t_selected = torch.squeeze(q_t, axis=len(q_t.shape) - 1)
    if twin_q:
        twin_q_t_selected = torch.squeeze(twin_q_t, axis=len(q_t.shape) - 1)
        q_tp1 = torch.min(q_tp1, twin_q_tp1)

    q_tp1_best = torch.squeeze(input=q_tp1, axis=len(q_tp1.shape) - 1)
    q_tp1_best_masked = \
        (1.0 - train_batch[SampleBatch.DONES].float()) * \
        q_tp1_best

    &#47&#47 Compute RHS of bellman equation.
    q_t_selected_target = (train_batch[SampleBatch.REWARDS] +
                           gamma**n_step * q_tp1_best_masked).detach()

    &#47&#47 Compute the error (potentially clipped).
    if twin_q:
        td_error = q_t_selected - q_t_selected_target
        twin_td_error = twin_q_t_selected - q_t_selected_target
        td_error = td_error + twin_td_error
        if use_huber:
            errors = huber_loss(td_error, huber_threshold) \
                + huber_loss(twin_td_error, huber_threshold)
        else:
            errors = 0.5 * \
                (torch.pow(td_error, 2.0) + torch.pow(twin_td_error, 2.0))
    else:
        td_error = q_t_selected - q_t_selected_target
        if use_huber:
            errors = huber_loss(td_error, huber_threshold)
        else:
            errors = 0.5 * torch.pow(td_error, 2.0)

    critic_loss = torch.mean(train_batch[PRIO_WEIGHTS] * errors)

    &#47&#47 Add l2-regularization if required.
    if l2_reg is not None:
        for name, var in policy.model.policy_variables(as_dict=True).items():
            if "bias" not in name:
                actor_loss += (l2_reg * l2_loss(var))
        for name, var in policy.model.q_variables(as_dict=True).items():
            if "bias" not in name:
                critic_loss += (l2_reg * l2_loss(var))

    &#47&#47 Model self-supervised losses.
    if policy.config["use_state_preprocessor"]:
        &#47&#47 Expand input_dict in case custom_loss&quot need them.
        input_dict[SampleBatch.ACTIONS] = train_batch[SampleBatch.ACTIONS]
        input_dict[SampleBatch.REWARDS] = train_batch[SampleBatch.REWARDS]
        input_dict[SampleBatch.DONES] = train_batch[SampleBatch.DONES]
        input_dict[SampleBatch.NEXT_OBS] = train_batch[SampleBatch.NEXT_OBS]
        [actor_loss, critic_loss] = model.custom_loss(
            [actor_loss, critic_loss], input_dict)

    &#47&#47 Store values for stats function.
    policy.actor_loss = actor_loss
    policy.critic_loss = critic_loss
    <a id="change">policy.td_error</a> = td_error
    policy.q_t = q_t

    &#47&#47 Return two loss terms (corresponding to the two optimizers, we create).</code></pre><h3>After Change</h3><pre><code class='java'>
        policy_tp1_smoothed = torch.min(
            torch.max(
                policy_tp1 + clipped_normal_sample,
                <a id="change">torch.tensor(
                    policy.action_space.low,
                    dtype=torch.float32,
                    device=policy_tp1.device)</a>),
            torch.tensor(
                policy.action_space.high,
                dtype=torch.float32,
                device=policy_tp1.device))
    else:
        &#47&#47 No smoothing, just use deterministic actions.
        policy_tp1_smoothed = policy_tp1

    &#47&#47 Q-net(s) evaluation.
    &#47&#47 prev_update_ops = set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS))
    &#47&#47 Q-values for given actions & observations in given current
    q_t = model.get_q_values(model_out_t, train_batch[SampleBatch.ACTIONS])

    &#47&#47 Q-values for current policy (no noise) in given current state
    q_t_det_policy = model.get_q_values(model_out_t, policy_t)

    actor_loss = -torch.mean(q_t_det_policy)

    if twin_q:
        twin_q_t = model.get_twin_q_values(model_out_t,
                                           train_batch[SampleBatch.ACTIONS])
    &#47&#47 q_batchnorm_update_ops = list(
    &#47&#47     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)

    &#47&#47 Target q-net(s) evaluation.
    q_tp1 = policy.target_model.get_q_values(target_model_out_tp1,
                                             policy_tp1_smoothed)

    if twin_q:
        twin_q_tp1 = policy.target_model.get_twin_q_values(
            target_model_out_tp1, policy_tp1_smoothed)

    q_t_selected = torch.squeeze(q_t, axis=len(q_t.shape) - 1)
    if twin_q:
        twin_q_t_selected = torch.squeeze(twin_q_t, axis=len(q_t.shape) - 1)
        q_tp1 = torch.min(q_tp1, twin_q_tp1)

    q_tp1_best = torch.squeeze(input=q_tp1, axis=len(q_tp1.shape) - 1)
    q_tp1_best_masked = \
        (1.0 - train_batch[SampleBatch.DONES].float()) * \
        q_tp1_best

    &#47&#47 Compute RHS of bellman equation.
    q_t_selected_target = (train_batch[SampleBatch.REWARDS] +
                           gamma**n_step * q_tp1_best_masked).detach()

    &#47&#47 Compute the error (potentially clipped).
    if twin_q:
        td_error = q_t_selected - q_t_selected_target
        twin_td_error = twin_q_t_selected - q_t_selected_target
        td_error = td_error + twin_td_error
        if use_huber:
            errors = huber_loss(td_error, huber_threshold) \
                + huber_loss(twin_td_error, huber_threshold)
        else:
            errors = 0.5 * \
                (torch.pow(td_error, 2.0) + torch.pow(twin_td_error, 2.0))
    else:
        td_error = q_t_selected - q_t_selected_target
        if use_huber:
            errors = huber_loss(td_error, huber_threshold)
        else:
            errors = 0.5 * torch.pow(td_error, 2.0)

    critic_loss = torch.mean(train_batch[PRIO_WEIGHTS] * errors)

    &#47&#47 Add l2-regularization if required.
    if l2_reg is not None:
        for name, var in policy.model.policy_variables(as_dict=True).items():
            if "bias" not in name:
                actor_loss += (l2_reg * l2_loss(var))
        for name, var in policy.model.q_variables(as_dict=True).items():
            if "bias" not in name:
                critic_loss += (l2_reg * l2_loss(var))

    &#47&#47 Model self-supervised losses.
    if policy.config["use_state_preprocessor"]:
        &#47&#47 Expand input_dict in case custom_loss&quot need them.
        input_dict[SampleBatch.ACTIONS] = train_batch[SampleBatch.ACTIONS]
        input_dict[SampleBatch.REWARDS] = train_batch[SampleBatch.REWARDS]
        input_dict[SampleBatch.DONES] = train_batch[SampleBatch.DONES]
        input_dict[SampleBatch.NEXT_OBS] = train_batch[SampleBatch.NEXT_OBS]
        [actor_loss, critic_loss] = model.custom_loss(
            [actor_loss, critic_loss], input_dict)

    &#47&#47 Store values for stats function.
    policy.actor_loss = actor_loss
    policy.critic_loss = critic_loss
    <a id="change">policy.td_error</a> = td_error
    policy.q_t = q_t

    &#47&#47 Return two loss terms (corresponding to the two optimizers, we create).</code></pre><img src="14866002.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/ff9c1dac887643e464f5f829c7d8b920b0aa8140#diff-790d4764fc76890b3cb22c499e41a93ba00f3e01792b9e897948771c06462f97L58' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: ff9c1dac887643e464f5f829c7d8b920b0aa8140</div><div id='time'> Time: 2020-07-28</div><div id='author'> Author: sven@anyscale.io</div><div id='file'> File Name: rllib/agents/ddpg/ddpg_torch_policy.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: ddpg_actor_critic_loss</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/87a61ad92a9e0129e5c81c242f0ea96d77e6b0af#diff-b81a5e7ea2db0effd174e8d874a80910bd94082a22fff0b20d9cd0ef981f3483L37' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 87a61ad92a9e0129e5c81c242f0ea96d77e6b0af</div><div id='time'> Time: 2020-08-19</div><div id='author'> Author: akshita23bhagia@gmail.com</div><div id='file'> File Name: allennlp/training/metrics/attachment_scores.py</div><div id='class'> Class Name: AttachmentScores</div><div id='method'> Method Name: __call__</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/0a456a7582da2ab4271756d7775bba84a75c8c0d#diff-b85faa577aece208a96407af7070dfc45c38a39ec9c543b64742503981ec196eL36' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 0a456a7582da2ab4271756d7775bba84a75c8c0d</div><div id='time'> Time: 2020-08-17</div><div id='author'> Author: eladsegal@users.noreply.github.com</div><div id='file'> File Name: allennlp/training/metrics/categorical_accuracy.py</div><div id='class'> Class Name: CategoricalAccuracy</div><div id='method'> Method Name: __call__</div><BR>