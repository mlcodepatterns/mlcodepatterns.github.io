<html><h3>f19ace982075ea009af81f5e9f687cc2276f50ea,scripts/bert/fp16_utils.py,,grad_global_norm,#Any#Any#,24
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 reduce
    ctx, dtype = arrays[0].context, &quotfloat32&quot
    norms = [<a id="change">nd.add_n(*g).as_in_context(ctx)</a> for g in norm_groups.values()]
    total_norm = nd.add_n(*norms).sqrt()
    scale = total_norm / max_norm
    &#47&#47 is_finite = 0 if NaN or Inf, 1 otherwise.</code></pre><h3>After Change</h3><pre><code class='java'>
        mx.autograd.backward(ls)

    def step(self, batch_size, max_norm=None):
        Makes o<a id="change">ne step of parameter updat</a>e. Should be called after
        `fp16_optimizer.backward()`, and outside of `record()` scope.

        Parameters
        ----------
        batch_size : int
            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
        max_norm : NDArra<a id="change">y, optional, default is No</a>ne
       <a id="change">     max value for global 2-norm of gradients.
        "</a>""
        self.fp32_trainer.allreduce_grads()
        step_size = batch_size * self._scaler.loss_scale
        if max_norm:</code></pre><img src="8425925.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dmlc/gluon-nlp/commit/f19ace982075ea009af81f5e9f687cc2276f50ea#diff-978189d71446b5d60c49613b692de0fa5d50ae07bd06f3d620526f814c3a677eL64' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/gluon-nlp</div><div id='commit'> Commit Name: f19ace982075ea009af81f5e9f687cc2276f50ea</div><div id='time'> Time: 2020-01-20</div><div id='author'> Author: 50716238+MoisesHer@users.noreply.github.com</div><div id='file'> File Name: scripts/bert/fp16_utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: grad_global_norm</div><BR><BR><div id='link'><a href='https://github.com/d2l-ai/d2l-zh/commit/64b39a47566613766f6fee0c2d3f7f163c097ffb#diff-09d0cbaf891d623ac245ccf84485a687b554217349ca55557c60b33234b969aeL16' target='_blank'>Link</a></div><div id='project'> Project Name: d2l-ai/d2l-zh</div><div id='commit'> Commit Name: 64b39a47566613766f6fee0c2d3f7f163c097ffb</div><div id='time'> Time: 2017-10-13</div><div id='author'> Author: mli@amazon.com</div><div id='file'> File Name: utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: evaluate_accuracy</div><BR><BR><div id='link'><a href='https://github.com/d2l-ai/d2l-zh/commit/64b39a47566613766f6fee0c2d3f7f163c097ffb#diff-09d0cbaf891d623ac245ccf84485a687b554217349ca55557c60b33234b969aeL54' target='_blank'>Link</a></div><div id='project'> Project Name: d2l-ai/d2l-zh</div><div id='commit'> Commit Name: 64b39a47566613766f6fee0c2d3f7f163c097ffb</div><div id='time'> Time: 2017-10-13</div><div id='author'> Author: mli@amazon.com</div><div id='file'> File Name: utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train</div><BR>