<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 reduce
    ctx, dtype = arrays[0].context, &quotfloat32&quot
    norms = [<a id="change">nd.add_n(*g).as_in_context(ctx)</a> for g in norm_groups.values()]
    total_norm = nd.add_n(*norms).sqrt()
    scale = total_norm / max_norm
    &#47&#47 is_finite = 0 if NaN or Inf, 1 otherwise.</code></pre><h3>After Change</h3><pre><code class='java'>
        mx.autograd.backward(ls)

    def step(self, batch_size, max_norm=None):
        Makes o<a id="change">ne step of parameter updat</a>e. Should be called after
        `fp16_optimizer.backward()`, and outside of `record()` scope.

        Parameters
        ----------
        batch_size : int
            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
        max_norm : NDArra<a id="change">y, optional, default is No</a>ne
       <a id="change">     max value for global 2-norm of gradients.
        "</a>""
        self.fp32_trainer.allreduce_grads()
        step_size = batch_size * self._scaler.loss_scale
        if max_norm:</code></pre>