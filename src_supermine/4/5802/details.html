<html><h3>57544b1ff9f97d4da9f64d25c8ea5a3d8d247ffc,rllib/examples/rock_paper_scissors_multiagent.py,,run_heuristic_vs_learned,#Any#Any#Any#,106
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            "policy_mapping_fn": select_policy,
        },
    }
    <a id="change">tune.run(trainer, stop={"timesteps_total": args.stop}, config=config)</a>


def run_with_custom_entropy_loss(args):
    Example of customizing the loss function of an existing policy.</code></pre><h3>After Change</h3><pre><code class='java'>
    trainer_obj = cls(config=config)
    env = trainer_obj.workers.local_worker().env
    for _ in range(args.stop_iters):
        <a id="change">results = trainer_obj.train()</a>
        print(results)
        &#47&#47 Timesteps reached.
        <a id="change">if results["timesteps_total"] &gt; args.stop_timesteps:
            break
        &#47&#47 Reward (difference) reached -&gt; all good, return.
        elif env.player1_score - env.player2_score &gt; args.stop_reward:
            return

    &#47&#47 Reward (difference) not reached: Error if `as_test`.
   </a> if args.as_test:
        raise ValueError(
            "Desired reward difference ({}) not reached! Only got to {}.".
            format(args.stop_reward, env.player1_score - env.player2_score))</code></pre><img src="25049999.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/57544b1ff9f97d4da9f64d25c8ea5a3d8d247ffc#diff-6e6f5ac309686c42bec61e8cf74c48e92895c1abaf1b57a60679f105670ee42dL63' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: 57544b1ff9f97d4da9f64d25c8ea5a3d8d247ffc</div><div id='time'> Time: 2020-05-11</div><div id='author'> Author: sven@anyscale.io</div><div id='file'> File Name: rllib/examples/rock_paper_scissors_multiagent.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: run_heuristic_vs_learned</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/cd90b064de4b8d24e9419b3458c83626db356be7#diff-c5f3f08e1e900c0c22a20aa29110aad58e7ee25c181663ce36196263686a5c67L408' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: cd90b064de4b8d24e9419b3458c83626db356be7</div><div id='time'> Time: 2017-11-14</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/engine/application_driver.py</div><div id='class'> Class Name: ApplicationDriver</div><div id='method'> Method Name: _inference_loop</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/f2073333b710a340403843763ba60eb1e6699916#diff-8dac8a79e475488e587e184ea47f922be854b7a4a25c252dcff227eec19cc6f3L81' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: f2073333b710a340403843763ba60eb1e6699916</div><div id='time'> Time: 2019-04-11</div><div id='author'> Author: rundi_wu@pku.edu.cn</div><div id='file'> File Name: examples/data_process/tutorial_tfrecord2.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>