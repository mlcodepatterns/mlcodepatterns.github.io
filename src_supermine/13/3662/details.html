<html><h3>39a28aba95b0d8eeb56f7a17b7dad140e601b591,agents/clipped_ppo_agent.py,ClippedPPOAgent,train_network,#ClippedPPOAgent#Any#Any#,80
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    actions = np.expand_dims(actions, -1)

                &#47&#47 get old policy probabilities and distribution
                result = self.main_network.target_network.predict(<a id="change">[current_states]</a>)
                old_policy_distribution = result[1:]

                &#47&#47 calculate gradients and apply on both the local policy network and on the global policy network
                fetches = [self.main_network.online_network.output_heads[1].kl_divergence,
                           self.main_network.online_network.output_heads[1].entropy]

                total_return = np.expand_dims(total_return, -1)
                value_targets = gae_based_value_targets if self.tp.agent.estimate_value_using_gae else total_return
                total_loss, policy_losses, unclipped_grads, fetch_result =\
                    self.main_network.online_network.accumulate_gradients(
                        <a id="change">[current_states] + [actions] + old_policy_distribution</a>,
                        [total_return, advantages], additional_fetches=fetches)

                self.value_targets.add_sample(value_targets)</code></pre><h3>After Change</h3><pre><code class='java'>

                total_return = np.expand_dims(total_return, -1)
                value_targets = gae_based_value_targets if self.tp.agent.estimate_value_using_gae else total_return
                <a id="change">inputs = copy.copy(current_states)</a>
                &#47&#47 TODO: why is this output 0 and not output 1?
                inputs[&quotoutput_0_0&quot] = actions
                &#47&#47 TODO: does old_policy_distribution really need to be represented as a list?
                &#47&#47 A: yes it does, in the event of discrete controls, it has just a mean
                &#47&#47 otherwise, it has both a mean and standard deviation
                <a id="change">for input_index, input in enumerate(old_policy_distribution):
                    inputs[&quotoutput_0_{}&quot.format(input_index + 1)] = input
                &#47&#47 print(&quotold_policy_distribution.shape&quot, len(old_policy_distribution))
               </a> total_loss, policy_losses, unclipped_grads, fetch_result =\
                    self.main_network.online_network.accumulate_gradients(
                        inputs, [total_return, advantages], additional_fetches=fetches)
</code></pre><img src="17533631.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/39a28aba95b0d8eeb56f7a17b7dad140e601b591#diff-6efea87eeb6fb3e1cfd7144a5c894e4bb048ee60a1dc819fe243f84456338858L97' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 39a28aba95b0d8eeb56f7a17b7dad140e601b591</div><div id='time'> Time: 2018-02-21</div><div id='author'> Author: zach.dwiel@intel.com</div><div id='file'> File Name: agents/clipped_ppo_agent.py</div><div id='class'> Class Name: ClippedPPOAgent</div><div id='method'> Method Name: train_network</div><BR><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/8248caf35eeb43046f2b28937627d43cbf950c9a#diff-28e8eb4814c67ed8c4557d61846635d3d8cc05fa844d5f810fe3556c232f2cd3L151' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 8248caf35eeb43046f2b28937627d43cbf950c9a</div><div id='time'> Time: 2018-02-21</div><div id='author'> Author: zach.dwiel@intel.com</div><div id='file'> File Name: agents/ppo_agent.py</div><div id='class'> Class Name: PPOAgent</div><div id='method'> Method Name: train_policy_network</div><BR><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/39a28aba95b0d8eeb56f7a17b7dad140e601b591#diff-6efea87eeb6fb3e1cfd7144a5c894e4bb048ee60a1dc819fe243f84456338858L97' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 39a28aba95b0d8eeb56f7a17b7dad140e601b591</div><div id='time'> Time: 2018-02-21</div><div id='author'> Author: zach.dwiel@intel.com</div><div id='file'> File Name: agents/clipped_ppo_agent.py</div><div id='class'> Class Name: ClippedPPOAgent</div><div id='method'> Method Name: train_network</div><BR><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/ee6e0bdc3b91b5fb738e8278898ceb49e7080341#diff-28e8eb4814c67ed8c4557d61846635d3d8cc05fa844d5f810fe3556c232f2cd3L105' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: ee6e0bdc3b91b5fb738e8278898ceb49e7080341</div><div id='time'> Time: 2018-02-21</div><div id='author'> Author: zach.dwiel@intel.com</div><div id='file'> File Name: agents/ppo_agent.py</div><div id='class'> Class Name: PPOAgent</div><div id='method'> Method Name: train_value_network</div><BR>