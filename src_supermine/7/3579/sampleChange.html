<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Concat. prev-action/reward if required.
        if self.model_config["lstm_use_prev_action_reward"]:
            <a id="change">wrapped_out = tf.concat(
                [
                    wrapped_out,
                    tf.reshape(
                        tf.cast(input_dict[SampleBatch.PREV_ACTIONS],
                                tf.float32), [-1, self.action_dim]),
                    tf.reshape(
                        tf.cast(input_dict[SampleBatch.PREV_REWARDS],
                                tf.float32), [-1, 1]),
                ],
                axis=1)</a>

        &#47&#47 Then through our LSTM.
        input_dict["obs_flat"] = wrapped_out
        return super().forward(input_dict, state, seq_lens)</code></pre><h3>After Change</h3><pre><code class='java'>
                    tf.cast(input_dict[SampleBatch.PREV_REWARDS], tf.float32),
                    [-1, 1]))

        <a id="change">if prev_a_r:
            wrapped_out = tf.concat([wrapped_out] + prev_a_r, axis=1)

        &#47&#47 Then through our LSTM.
       </a> input_dict["obs_flat"] = wrapped_out
        return super().forward(input_dict, state, seq_lens)

    @override(RecurrentNetwork)</code></pre>