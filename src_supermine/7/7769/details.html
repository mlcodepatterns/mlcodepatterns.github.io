<html><h3>2de52a8976971da4836727ba9242fedcc7474878,src/sdk/pynni/nni/compression/torch/compressor.py,Pruner,export_model,#Pruner#Any#Any#Any#Any#,213
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            if name == "":
                continue
            masks = self.mask_dict.get(name)
            <a id="change">if masks is not None:
                mask_sum = masks[&quotweight&quot].sum().item()
                mask_num = masks[&quotweight&quot].numel()
                _logger.info(&quotLayer: %s  Sparsity: %.2f&quot, name, 1 - mask_sum / mask_num)
                m.weight.data = m.weight.data.mul(masks[&quotweight&quot])
                if masks.__contains__(&quotbias&quot) and hasattr(m, &quotbias&quot) and m.bias is not None:
                    m.bias.data = m.bias.data.mul(masks[&quotbias&quot])
            else:
                _logger.info(&quotLayer: %s  NOT compressed&quot, name)
       </a> torch.save(self.bound_model.state_dict(), model_path)
        _logger.info(&quotModel state_dict saved to %s&quot, model_path)
        if mask_path is not None:
            torch.save(self.mask_dict, mask_path)</code></pre><h3>After Change</h3><pre><code class='java'>
                mask_num = weight_mask.numel()
                _logger.info(&quotLayer: %s  Sparsity: %.2f&quot, wrapper.name, 1 - mask_sum / mask_num)
                wrapper.module.weight.data = wrapper.module.weight.data.mul(weight_mask)
            <a id="change">if bias_mask is not None:
                wrapper.module.bias.data = wrapper.module.bias.data.mul(bias_mask)
            &#47&#47 save mask to dict
           </a> mask_dict[wrapper.name] = {"weight": weight_mask, "bias": bias_mask}

        torch.save(self.bound_model.state_dict(), model_path)
        _logger.info(&quotModel state_dict saved to %s&quot, model_path)</code></pre><img src="36778725.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/nni/commit/2de52a8976971da4836727ba9242fedcc7474878#diff-865376fc1665360e73ce4a363969f6f10706a34184447b541361f664ee39e407L213' target='_blank'>Link</a></div><div id='project'> Project Name: microsoft/nni</div><div id='commit'> Commit Name: 2de52a8976971da4836727ba9242fedcc7474878</div><div id='time'> Time: 2020-01-16</div><div id='author'> Author: 656569648@qq.com</div><div id='file'> File Name: src/sdk/pynni/nni/compression/torch/compressor.py</div><div id='class'> Class Name: Pruner</div><div id='method'> Method Name: export_model</div><BR><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/0d36e4c025e7c8f208c126709a33730aef1c3600#diff-46c1c56bb9faf180d3779512da9bab8b31568007795c660e730c298d38bfbb48L42' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: 0d36e4c025e7c8f208c126709a33730aef1c3600</div><div id='time'> Time: 2020-09-28</div><div id='author'> Author: ameerh@berkeley.edu</div><div id='file'> File Name: python/ray/autoscaler/_private/resource_demand_scheduler.py</div><div id='class'> Class Name: ResourceDemandScheduler</div><div id='method'> Method Name: get_nodes_to_launch</div><BR><BR><div id='link'><a href='https://github.com/deepmipt/DeepPavlov/commit/2e530e78c78c3f9f899143c437f55618d3038951#diff-e9b0a450f9257d421c72c999d72bb7eaedb00c09aaea8f9fcf33b8e4af25bf7dL116' target='_blank'>Link</a></div><div id='project'> Project Name: deepmipt/DeepPavlov</div><div id='commit'> Commit Name: 2e530e78c78c3f9f899143c437f55618d3038951</div><div id='time'> Time: 2018-02-18</div><div id='author'> Author: arkhipov@yahoo.com</div><div id='file'> File Name: deeppavlov/core/models/tf_model.py</div><div id='class'> Class Name: TFModel</div><div id='method'> Method Name: load</div><BR>