<html><h3>641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656,examples/reinforcement_learning/tutorial_cartpole_ac.py,,,#,148
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

tl.layers.initialize_global_variables(sess)

<a id="change">if OUTPUT_GRAPH:
    tf.summary.FileWriter("logs/", sess.graph)

</a>for i_episode in range(MAX_EPISODE):
    episode_time = time.time()
    s = env.reset()
    t = 0  &#47&#47 number of step in this episode</code></pre><h3>After Change</h3><pre><code class='java'>
        a = actor.choose_action(s)

        s_new, r, done, info = env.step(a)
        <a id="change">s_new = s_new.astype(np.float32)</a>

        if done: r = -20
        &#47&#47 these may helpful in some tasks
        &#47&#47 if abs(s_new[0]) &gt;= env.observation_space.high[0]:
        &#47&#47 &#47&#47  cart moves more than 2.4 units from the center
        &#47&#47     r = -20
        &#47&#47 reward for the distance between cart to the center
        &#47&#47 r -= abs(s_new[0])  * .1

        all_r.append(r)

        td_error = critic.learn(s, r, s_new)  &#47&#47 learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]
        actor.learn(s, a, td_error)  &#47&#47 learn Policy         : true_gradient = grad[logPi(s, a) * td_error]

        s = s_new
        t += 1

        if done or t &gt;= MAX_EP_STEPS:
            ep_rs_sum = sum(all_r)

            if &quotrunning_reward&quot not in globals():
                running_reward = ep_rs_sum
            else:
                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05
            &#47&#47 start rending if running_reward greater than a threshold
            &#47&#47 if running_reward &gt; DISPLAY_REWARD_THRESHOLD: RENDER = True
            print("Episode: %d reward: %f running_reward %f took: %.5f" % \
                (i_episode, ep_rs_sum, running_reward, time.time() - episode_time))

            &#47&#47 Early Stopping for quick check
            if t &gt;= MAX_EP_STEPS:
                print("Early Stopping")
                s = <a id="change">env.reset().astype(np.float32)</a>
                rall = 0
                while True:
                    env.render()
                    &#47&#47 a = actor.choose_action(s)</code></pre><img src="26403122.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L148' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656</div><div id='time'> Time: 2019-02-16</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/bd333dd43d69b26015eb3f201afe1772ba701a41#diff-c3102fa1640eb0e74d5ee3a673169d81527d7b01428fd3a2d4d72769b694359fL95' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: bd333dd43d69b26015eb3f201afe1772ba701a41</div><div id='time'> Time: 2018-05-07</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/contrib/dataset_sampler/sampler_uniform_v2.py</div><div id='class'> Class Name: UniformSampler</div><div id='method'> Method Name: layer_op</div><BR><BR><div id='link'><a href='https://github.com/maciejkula/spotlight/commit/fde2f66676f960782c993f7148927c4a4197ab10#diff-1d074553c8360eeb4748d829e86a653cdba4e5fda01ae13e8095bcf928f8d973L85' target='_blank'>Link</a></div><div id='project'> Project Name: maciejkula/spotlight</div><div id='commit'> Commit Name: fde2f66676f960782c993f7148927c4a4197ab10</div><div id='time'> Time: 2017-06-27</div><div id='author'> Author: maciej.kula@gmail.com</div><div id='file'> File Name: spotlight/factorization/explicit.py</div><div id='class'> Class Name: ExplicitFactorizationModel</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L148' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656</div><div id='time'> Time: 2019-02-16</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>