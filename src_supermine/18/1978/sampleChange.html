<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    
    method = train_config.learning_rate_decay_method

    <a id="change">if not method or method == &quotnone&quot:
        return train_config.initial_learning_rate

   </a> if method not in LEARNING_RATE_DECAY_METHODS:
        raise ValueError(&quotInvalid learning_rate method "{}"&quot.format(method))

    <a id="change">if method == &quotpiecewise_constant&quot:
        learning_rate = tf.train.piecewise_constant(
            global_step, boundaries=[
                tf.cast(train_config.learning_rate_decay, tf.int64), ],
            values=[
                train_config.initial_learning_rate,
                train_config.initial_learning_rate * 0.1
            ], name=&quotlearning_rate_piecewise_constant&quot
        )

    elif method == &quotexponential_decay&quot:
        learning_rate = tf.train.exponential_decay(
            learning_rate=train_config.initial_learning_rate,
            global_step=global_step,
            decay_steps=train_config.learning_rate_decay, decay_rate=0.96,
            staircase=True, name=&quotlearning_rate_with_decay&quot
        )

   </a> tf.summary.scalar(&quotlosses/learning_rate&quot, learning_rate)

    return learning_rate
</code></pre><h3>After Change</h3><pre><code class='java'>
    lr_config = train_config.learning_rate.copy()
    decay_method = lr_config.pop(&quotdecay_method&quot, None)

    <a id="change">if not decay_method or decay_method == &quotnone&quot:
        return lr_config.get(&quotvalue&quot) or lr_config.get(&quotlearning_rate&quot)

   </a> if decay_method not in LEARNING_RATE_DECAY_METHODS:
        raise ValueError(&quotInvalid learning_rate method "{}"&quot.format(
            decay_method
        ))

    if decay_method == &quotpiecewise_constant&quot:
        <a id="change">lr_config[&quotx&quot] = global_step</a>
    else:
        lr_config[&quotglobal_step&quot] = global_step

    &#47&#47 boundaries, when used, must be the same type as global_step (int64).
    if &quotboundaries&quot in lr_config:
        lr_config[&quotboundaries&quot] = [
            tf.cast(b, tf.int64) for b in lr_config[&quotboundaries&quot]
        ]

    <a id="change">decay_function = LEARNING_RATE_DECAY_METHODS[decay_method]</a>
    learning_rate = decay_function(
        **lr_config
    )
</code></pre>