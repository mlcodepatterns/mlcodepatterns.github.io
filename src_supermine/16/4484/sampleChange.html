<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if b_init_args is None:
            b_init_args = {}

        <a id="change">Layer.__init__(self, prev_layer=prev_layer, name=name)</a>
        <a id="change">self.inputs = prev_layer.outputs</a>
        if self.inputs.get_shape().ndims != 2:
            raise Exception("The input dimension must be rank 2, please reshape or flatten it")

        if use_gemm:
            raise Exception("TODO. The current version use tf.matmul for inferencing.")

        n_in = int(self.inputs.get_shape()[-1])
        self.n_units = n_units
        logging.info("BinaryDenseLayer  %s: %d %s" % (<a id="change">self.name</a>, <a id="change">self.n_units</a>, act.__name__))
        with tf.variable_scope(name):
            W = tf.get_variable(name=&quotW&quot, shape=(n_in, n_units), initializer=W_init, dtype=LayersConfig.tf_dtype, **W_init_args)
            &#47&#47 W = tl.act.sign(W)    &#47&#47 dont update ...
            W = quantize(W)
            &#47&#47 W = tf.Variable(W)
            &#47&#47 print(W)
            if b_init is not None:
                try:
                    b = tf.get_variable(name=&quotb&quot, shape=(n_units), initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                except Exception:  &#47&#47 If initializer is a constant, do not specify shape.
                    b = tf.get_variable(name=&quotb&quot, initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W) + b)
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W) + b) &#47&#47 TODO
            else:
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W))
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W)) &#47&#47 TODO

        self.all_layers.append(self.outputs)</code></pre><h3>After Change</h3><pre><code class='java'>
            b_init_args=None,
            name=&quotbinary_dense&quot,
    ):
        <a id="change">super(BinaryDenseLayer, self).__init__(prev_layer=prev_layer, name=name)</a>
        logging.info("BinaryDenseLayer  %s: %d %s" % (name, n_units, act.__name__))

        <a id="change">self.inputs = prev_layer.outputs</a>

        if W_init_args is None:
            W_init_args = {}
        if b_init_args is None:
            b_init_args = {}

        if self.inputs.get_shape().ndims != 2:
            raise Exception("The input dimension must be rank 2, please reshape or flatten it")

        if use_gemm:
            raise Exception("TODO. The current version use tf.matmul for inferencing.")

        n_in = int(self.inputs.get_shape()[-1])
        self.n_units = n_units

        with tf.variable_scope(name):
            W = tf.get_variable(name=&quotW&quot, shape=(n_in, n_units), initializer=W_init, dtype=LayersConfig.tf_dtype, **W_init_args)
            &#47&#47 W = tl.act.sign(W)    &#47&#47 dont update ...
            W = quantize(W)
            &#47&#47 W = tf.Variable(W)
            &#47&#47 print(W)
            if b_init is not None:
                try:
                    b = tf.get_variable(name=&quotb&quot, shape=(n_units), initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                except Exception:  &#47&#47 If initializer is a constant, do not specify shape.
                    b = tf.get_variable(name=&quotb&quot, initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W) + b)
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W) + b) &#47&#47 TODO
            else:
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W))
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W)) &#47&#47 TODO

        self.all_layers.append(self.outputs)</code></pre>