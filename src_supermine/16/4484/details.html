<html><h3>88ff1953e94cfbbcd53f4d659a1099fce8a7344e,tensorlayer/layers/binary.py,BinaryDenseLayer,__init__,#BinaryDenseLayer#Any#Any#Any#Any#Any#Any#Any#Any#Any#,127
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if b_init_args is None:
            b_init_args = {}

        <a id="change">Layer.__init__(self, prev_layer=prev_layer, name=name)</a>
        <a id="change">self.inputs = prev_layer.outputs</a>
        if self.inputs.get_shape().ndims != 2:
            raise Exception("The input dimension must be rank 2, please reshape or flatten it")

        if use_gemm:
            raise Exception("TODO. The current version use tf.matmul for inferencing.")

        n_in = int(self.inputs.get_shape()[-1])
        self.n_units = n_units
        logging.info("BinaryDenseLayer  %s: %d %s" % (<a id="change">self.name</a>, <a id="change">self.n_units</a>, act.__name__))
        with tf.variable_scope(name):
            W = tf.get_variable(name=&quotW&quot, shape=(n_in, n_units), initializer=W_init, dtype=LayersConfig.tf_dtype, **W_init_args)
            &#47&#47 W = tl.act.sign(W)    &#47&#47 dont update ...
            W = quantize(W)
            &#47&#47 W = tf.Variable(W)
            &#47&#47 print(W)
            if b_init is not None:
                try:
                    b = tf.get_variable(name=&quotb&quot, shape=(n_units), initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                except Exception:  &#47&#47 If initializer is a constant, do not specify shape.
                    b = tf.get_variable(name=&quotb&quot, initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W) + b)
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W) + b) &#47&#47 TODO
            else:
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W))
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W)) &#47&#47 TODO

        self.all_layers.append(self.outputs)</code></pre><h3>After Change</h3><pre><code class='java'>
            b_init_args=None,
            name=&quotbinary_dense&quot,
    ):
        <a id="change">super(BinaryDenseLayer, self).__init__(prev_layer=prev_layer, name=name)</a>
        logging.info("BinaryDenseLayer  %s: %d %s" % (name, n_units, act.__name__))

        <a id="change">self.inputs = prev_layer.outputs</a>

        if W_init_args is None:
            W_init_args = {}
        if b_init_args is None:
            b_init_args = {}

        if self.inputs.get_shape().ndims != 2:
            raise Exception("The input dimension must be rank 2, please reshape or flatten it")

        if use_gemm:
            raise Exception("TODO. The current version use tf.matmul for inferencing.")

        n_in = int(self.inputs.get_shape()[-1])
        self.n_units = n_units

        with tf.variable_scope(name):
            W = tf.get_variable(name=&quotW&quot, shape=(n_in, n_units), initializer=W_init, dtype=LayersConfig.tf_dtype, **W_init_args)
            &#47&#47 W = tl.act.sign(W)    &#47&#47 dont update ...
            W = quantize(W)
            &#47&#47 W = tf.Variable(W)
            &#47&#47 print(W)
            if b_init is not None:
                try:
                    b = tf.get_variable(name=&quotb&quot, shape=(n_units), initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                except Exception:  &#47&#47 If initializer is a constant, do not specify shape.
                    b = tf.get_variable(name=&quotb&quot, initializer=b_init, dtype=LayersConfig.tf_dtype, **b_init_args)
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W) + b)
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W) + b) &#47&#47 TODO
            else:
                <a id="change">self.outputs</a> = act(tf.matmul(self.inputs, W))
                &#47&#47 self.outputs = act(xnor_gemm(self.inputs, W)) &#47&#47 TODO

        self.all_layers.append(self.outputs)</code></pre><img src="19790985.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 13</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/88ff1953e94cfbbcd53f4d659a1099fce8a7344e#diff-48fecf03a77ead45997f9eda62705d6d2137f5254188246d84cb7f1415f66d3dL128' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 88ff1953e94cfbbcd53f4d659a1099fce8a7344e</div><div id='time'> Time: 2018-04-13</div><div id='author'> Author: DEKHTIARJonathan@users.noreply.github.com</div><div id='file'> File Name: tensorlayer/layers/binary.py</div><div id='class'> Class Name: BinaryDenseLayer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/88ff1953e94cfbbcd53f4d659a1099fce8a7344e#diff-48fecf03a77ead45997f9eda62705d6d2137f5254188246d84cb7f1415f66d3dL537' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 88ff1953e94cfbbcd53f4d659a1099fce8a7344e</div><div id='time'> Time: 2018-04-13</div><div id='author'> Author: DEKHTIARJonathan@users.noreply.github.com</div><div id='file'> File Name: tensorlayer/layers/binary.py</div><div id='class'> Class Name: DorefaDenseLayer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/88ff1953e94cfbbcd53f4d659a1099fce8a7344e#diff-48fecf03a77ead45997f9eda62705d6d2137f5254188246d84cb7f1415f66d3dL328' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 88ff1953e94cfbbcd53f4d659a1099fce8a7344e</div><div id='time'> Time: 2018-04-13</div><div id='author'> Author: DEKHTIARJonathan@users.noreply.github.com</div><div id='file'> File Name: tensorlayer/layers/binary.py</div><div id='class'> Class Name: TernaryDenseLayer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/88ff1953e94cfbbcd53f4d659a1099fce8a7344e#diff-48fecf03a77ead45997f9eda62705d6d2137f5254188246d84cb7f1415f66d3dL128' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 88ff1953e94cfbbcd53f4d659a1099fce8a7344e</div><div id='time'> Time: 2018-04-13</div><div id='author'> Author: DEKHTIARJonathan@users.noreply.github.com</div><div id='file'> File Name: tensorlayer/layers/binary.py</div><div id='class'> Class Name: BinaryDenseLayer</div><div id='method'> Method Name: __init__</div><BR>