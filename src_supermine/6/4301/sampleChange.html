<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 m_x = tf.nn.relu(-x)
    &#47&#47 x -= alpha * m_x
    x = tf.maximum(x, alpha * x, name=name)
    <a id="change">return x</a>


def swish(x, name=&quotswish&quot):
    Swish function.</code></pre><h3>After Change</h3><pre><code class='java'>
    if not (0 &lt; alpha &lt;= 1):
        raise ValueError("`alpha` value must be in [0, 1]`")

    <a id="change">with tf.name_scope(name, "leaky_relu") as name_scope:
        x = tf.convert_to_tensor(x, name="features")
        return tf.maximum(x, alpha * x, name=name_scope)


</a>def leaky_relu6(x, alpha=0.2, name="leaky_relu6"):
    :func:`leaky_relu6` can be used through its shortcut: :func:`tl.act.lrelu6`.

    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:</code></pre>