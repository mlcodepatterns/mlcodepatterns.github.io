<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                avail_gpu = GPUtil.getAvailable(order=&quotmemory&quot, limit=min(num_all_gpu, self.num_worker))
                num_avail_gpu = len(avail_gpu)
                print(num_avail_gpu)
                if 0 &lt; <a id="change">num_avail_gpu &lt; self.num_worker:
                    self.logger.warning(&quotonly %d out of %d GPU(s) is available/free, but "-num_worker=%d"&quot %
                                        (num_avail_gpu, num_all_gpu, self.num_worker))
                    self.logger.warning(&quotmultiple workers will be allocated to one GPU, &quot
                                        &quotmay not scale well and may raise out-of-memory&quot)
                    device_map = (avail_gpu * self.num_worker)[: self.num_worker]
                    run_on_gpu = True
                elif num_avail_gpu == 0:
                    self.logger.warning(&quotno GPU resource available, fall back to CPU&quot)
            except FileNotFoundError:
                self.logger.warning(&quotnvidia-smi is missing, often means no gpu on this machine. &quot
                                    &quotfall back to cpu!&quot)

        self.logger.info(&quotdevice map: \n\t\t%s&quot % &quot\n\t\t&quot.join(
            &quotworker %2d -&gt; %s&quot % (w_id, (&quotgpu %2d&quot % g_id) if g_id &gt;= 0 else &quotcpu&quot) for w_id, g_id in
            enumerate(device_map)))

        &#47&#47 start the backend processes
        for idx, device_id in enumerate(device_map):
            process = BertWorker(idx, self.args, addr_backend, addr_sink, device_id, self.graph_path)
            self.processes.append(process)
            process.start()

        num_req = defaultdict(int)
        while True:
            try:
                request = frontend.recv_multipart()
                client, msg, req_id, msg_len = request
                if msg == ServerCommand.terminate:
                    break
                elif msg == ServerCommand.show_config:
                    num_req[&quotconfig&quot] += 1
                    self.logger.info(&quotnew config request\treq id: %d\tclient: %s&quot % (int(req_id), client))
                    status_runtime = {&quotclient&quot: client.decode(&quotascii&quot),
                                      &quotnum_process&quot: len(self.processes),
                                      &quotventilator -&gt; worker&quot: addr_backend,
                                      &quotworker -&gt; sink&quot: addr_sink,
                                      &quotventilator &lt;-&gt; sink&quot: addr_front2sink,
                                      &quotserver_current_time&quot: str(datetime.now()),
                                      &quotnum_config_request&quot: num_req[&quotconfig&quot],
                                      &quotnum_data_request&quot: num_req[&quotdata&quot],
                                      &quotrun_on_gpu&quot: run_on_gpu}

                    sink.send_multipart([client, msg, jsonapi.dumps({**status_runtime,
                                                                     **self.status_args,
                                                                     **self.status_static}), req_id])
                else:
                    num_req[&quotdata&quot] += 1
                    self.logger.info(&quotnew encode request\treq id: %d\tsize: %d\tclient: %s&quot %
                                     (int(req_id), int(msg_len), client))
                    &#47&#47 register a new job at sink
                    sink.send_multipart([client, ServerCommand.new_job, msg_len, req_id])

                    job_id = client + b&quot&#47&#47&quot + req_id
                    if int(msg_len) &gt; self.max_batch_size:
                        seqs = jsonapi.loads(msg)
                        &#47&#47 partition the large batch into small batches
                        s_idx = 0
                        while s_idx &lt; int(msg_len):
                            tmp = seqs[s_idx: (s_idx + self.max_batch_size)]
                            if tmp:
     </a>                           partial_job_id = job_id + b&quot@%d&quot % s_idx
                                backend.send_multipart([partial_job_id, jsonapi.dumps(tmp)])
                            s_idx += len(tmp)
                    else:</code></pre><h3>After Change</h3><pre><code class='java'>
                avail_gpu = GPUtil.getAvailable(order=&quotmemory&quot, limit=min(num_all_gpu, self.num_worker))
                num_avail_gpu = len(avail_gpu)
                if 0 &lt; num_avail_gpu &lt;= self.num_worker:
                    <a id="change">if num_avail_gpu &lt; self.num_worker:
                        self.logger.warning(&quotonly %d out of %d GPU(s) is available/free, but "-num_worker=%d"&quot %
                                            (num_avail_gpu, num_all_gpu, self.num_worker))
                        self.logger.warning(&quotmultiple workers will be allocated to one GPU, &quot
                                            &quotmay not scale well and may raise out-of-memory&quot)
                   </a> device_map = (avail_gpu * self.num_worker)[: self.num_worker]
                    run_on_gpu = True
                elif num_avail_gpu == 0:
                    self.logger.warning(&quotno GPU resource available, fall back to CPU&quot)</code></pre>