<html><h3>b61833671493bb9fd2acfe48c5a30aaaafd4b571,server/bert_serving/server/__init__.py,BertServer,_run,#BertServer#Any#Any#Any#Any#,84
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                avail_gpu = GPUtil.getAvailable(order=&quotmemory&quot, limit=min(num_all_gpu, self.num_worker))
                num_avail_gpu = len(avail_gpu)
                print(num_avail_gpu)
                if 0 &lt; <a id="change">num_avail_gpu &lt; self.num_worker:
                    self.logger.warning(&quotonly %d out of %d GPU(s) is available/free, but "-num_worker=%d"&quot %
                                        (num_avail_gpu, num_all_gpu, self.num_worker))
                    self.logger.warning(&quotmultiple workers will be allocated to one GPU, &quot
                                        &quotmay not scale well and may raise out-of-memory&quot)
                    device_map = (avail_gpu * self.num_worker)[: self.num_worker]
                    run_on_gpu = True
                elif num_avail_gpu == 0:
                    self.logger.warning(&quotno GPU resource available, fall back to CPU&quot)
            except FileNotFoundError:
                self.logger.warning(&quotnvidia-smi is missing, often means no gpu on this machine. &quot
                                    &quotfall back to cpu!&quot)

        self.logger.info(&quotdevice map: \n\t\t%s&quot % &quot\n\t\t&quot.join(
            &quotworker %2d -&gt; %s&quot % (w_id, (&quotgpu %2d&quot % g_id) if g_id &gt;= 0 else &quotcpu&quot) for w_id, g_id in
            enumerate(device_map)))

        &#47&#47 start the backend processes
        for idx, device_id in enumerate(device_map):
            process = BertWorker(idx, self.args, addr_backend, addr_sink, device_id, self.graph_path)
            self.processes.append(process)
            process.start()

        num_req = defaultdict(int)
        while True:
            try:
                request = frontend.recv_multipart()
                client, msg, req_id, msg_len = request
                if msg == ServerCommand.terminate:
                    break
                elif msg == ServerCommand.show_config:
                    num_req[&quotconfig&quot] += 1
                    self.logger.info(&quotnew config request\treq id: %d\tclient: %s&quot % (int(req_id), client))
                    status_runtime = {&quotclient&quot: client.decode(&quotascii&quot),
                                      &quotnum_process&quot: len(self.processes),
                                      &quotventilator -&gt; worker&quot: addr_backend,
                                      &quotworker -&gt; sink&quot: addr_sink,
                                      &quotventilator &lt;-&gt; sink&quot: addr_front2sink,
                                      &quotserver_current_time&quot: str(datetime.now()),
                                      &quotnum_config_request&quot: num_req[&quotconfig&quot],
                                      &quotnum_data_request&quot: num_req[&quotdata&quot],
                                      &quotrun_on_gpu&quot: run_on_gpu}

                    sink.send_multipart([client, msg, jsonapi.dumps({**status_runtime,
                                                                     **self.status_args,
                                                                     **self.status_static}), req_id])
                else:
                    num_req[&quotdata&quot] += 1
                    self.logger.info(&quotnew encode request\treq id: %d\tsize: %d\tclient: %s&quot %
                                     (int(req_id), int(msg_len), client))
                    &#47&#47 register a new job at sink
                    sink.send_multipart([client, ServerCommand.new_job, msg_len, req_id])

                    job_id = client + b&quot&#47&#47&quot + req_id
                    if int(msg_len) &gt; self.max_batch_size:
                        seqs = jsonapi.loads(msg)
                        &#47&#47 partition the large batch into small batches
                        s_idx = 0
                        while s_idx &lt; int(msg_len):
                            tmp = seqs[s_idx: (s_idx + self.max_batch_size)]
                            if tmp:
     </a>                           partial_job_id = job_id + b&quot@%d&quot % s_idx
                                backend.send_multipart([partial_job_id, jsonapi.dumps(tmp)])
                            s_idx += len(tmp)
                    else:</code></pre><h3>After Change</h3><pre><code class='java'>
                avail_gpu = GPUtil.getAvailable(order=&quotmemory&quot, limit=min(num_all_gpu, self.num_worker))
                num_avail_gpu = len(avail_gpu)
                if 0 &lt; num_avail_gpu &lt;= self.num_worker:
                    <a id="change">if num_avail_gpu &lt; self.num_worker:
                        self.logger.warning(&quotonly %d out of %d GPU(s) is available/free, but "-num_worker=%d"&quot %
                                            (num_avail_gpu, num_all_gpu, self.num_worker))
                        self.logger.warning(&quotmultiple workers will be allocated to one GPU, &quot
                                            &quotmay not scale well and may raise out-of-memory&quot)
                   </a> device_map = (avail_gpu * self.num_worker)[: self.num_worker]
                    run_on_gpu = True
                elif num_avail_gpu == 0:
                    self.logger.warning(&quotno GPU resource available, fall back to CPU&quot)</code></pre><img src="18701676.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/hanxiao/bert-as-service/commit/b61833671493bb9fd2acfe48c5a30aaaafd4b571#diff-44ace16cb718aadd24b1c32a628113844418f650ac5fcc606d7a88c3fdb393f1L84' target='_blank'>Link</a></div><div id='project'> Project Name: hanxiao/bert-as-service</div><div id='commit'> Commit Name: b61833671493bb9fd2acfe48c5a30aaaafd4b571</div><div id='time'> Time: 2018-12-16</div><div id='author'> Author: hanhxiao@tencent.com</div><div id='file'> File Name: server/bert_serving/server/__init__.py</div><div id='class'> Class Name: BertServer</div><div id='method'> Method Name: _run</div><BR><BR><div id='link'><a href='https://github.com/catalyst-team/catalyst/commit/b27d8def595b8974ad255792d969a043233c09df#diff-67049ba255b90db57d90329433be84bde7c444be22db1a06db9510814a1f0152L10' target='_blank'>Link</a></div><div id='project'> Project Name: catalyst-team/catalyst</div><div id='commit'> Commit Name: b27d8def595b8974ad255792d969a043233c09df</div><div id='time'> Time: 2019-10-30</div><div id='author'> Author: tez.romach@gmail.com</div><div id='file'> File Name: catalyst/utils/compression.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/catalyst-team/catalyst/commit/b27d8def595b8974ad255792d969a043233c09df#diff-1d277f74224da298c309f629ac29d966fa703fde3c92e2aefc919998a045b6f5L7' target='_blank'>Link</a></div><div id='project'> Project Name: catalyst-team/catalyst</div><div id='commit'> Commit Name: b27d8def595b8974ad255792d969a043233c09df</div><div id='time'> Time: 2019-10-30</div><div id='author'> Author: tez.romach@gmail.com</div><div id='file'> File Name: catalyst/utils/serialization.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>