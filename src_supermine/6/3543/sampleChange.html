<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47TODO(zhiting)
        att_params = hparams[&quotattention&quot]
        attention_class = hparams[&quotattention&quot][&quotclass&quot] &#47&#47LuongAttention
        attention_kwargs = <a id="change">hparams[&quotattention&quot][&quotparams&quot]</a>
        attention_kwargs[&quotnum_units&quot] = n_hidden
        attention_kwargs[&quotmemory_sequence_length&quot] = attention_values_length
        attention_kwargs[&quotmemory&quot] = attention_keys
        attention_modules = [&quottxtgen.custom&quot, &quottensorflow.contrib.seq2seq&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
        attn_modules = [&quottxtgen.custom&quot, &quottensorflow.contrib.seq2seq&quot]
        &#47&#47 Use variable_scope to ensure all trainable variables created in
        &#47&#47 the attention mechanism  are collected
        <a id="change">with tf.variable_scope(self.variable_scope):
            attention_mechanism = get_instance(
                attn_hparams["type"], attn_kwargs, attn_modules)

       </a> atten_cell_kwargs = {
            "attention_layer_size": attn_hparams["attention_layer_size"],
            "alignment_history": attn_hparams["alignment_history"],
            "output_attention": attn_hparams["output_attention"],</code></pre>