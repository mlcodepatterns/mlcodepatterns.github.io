<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        x = array.reshape((-1,)).astype(&quotfloat32&quot, copy=False)
        return nd.dot(x, x)

    <a id="change">norm_arrays = [_norm(arr) for arr in arrays]</a>

    &#47&#47 group norm arrays by ctx
    def group_by_ctx(arr_list):
        groups = collections.defaultdict(list)</code></pre><h3>After Change</h3><pre><code class='java'>
        batch_size : int
            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
        m<a id="change">ax_norm : NDArray, optional, default is None
            max value for global 2-norm of gradients.
        
        self.fp32_trainer.allreduce_grads()
        step_size = batch_size * self._scaler.loss_scale
       </a> if max_norm:
            _, ratio, is_finite = nlp.utils.grad_global_norm(self.fp32_trainer._params,
                                                             max_norm * self._scaler.loss_scale)
            step_size = ratio * step_size</code></pre>