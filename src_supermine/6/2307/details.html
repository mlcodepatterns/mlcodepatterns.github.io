<html><h3>35b2c4917344f338eda67c78673cf4064b3b4265,examples/reinforcement_learning/tutorial_DQN.py,,,#,95
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                rAll += r
                s = s1
                &#47&#47&#47&#47 Reduce chance of random action if an episode is done.
                <a id="change">if d ==True:
                    e = 1. / ((i / 50) + 10)  &#47&#47 reduce e, GLIE: Greey in the limit with infinite Exploration
                    break

            &#47&#47&#47&#47 Note that, the rewards here with random action
           </a> running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01
            &#47&#47 print("Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs " % \
            &#47&#47     (i, num_episodes, rAll, running_reward, time.time() - episode_time))
            print(&quotEpisode: {}/{}  | Episode Reward: {:.4f} | Running Average Reward: {:.4f}  | Running Time: {:.4f}&quot\</code></pre><h3>After Change</h3><pre><code class='java'>

    t0 = time.time()
    if args.train:
        <a id="change">all_episode_reward = []</a>
        for i in range(num_episodes):
            &#47&#47&#47&#47 Reset environment and get first new observation
            &#47&#47 episode_time = time.time()
            s = env.reset()  &#47&#47 observation is state, integer 0 ~ 15
            rAll = 0
            if render: env.render()
            for j in range(99):  &#47&#47 step index, maximum step is 99
                &#47&#47&#47&#47 Choose an action by greedily (with e chance of random action) from the Q-network
                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()
                a = np.argmax(allQ, 1)

                &#47&#47&#47&#47 e-Greedy Exploration !!! sample random action
                if np.random.rand(1) &lt; e:
                    a[0] = env.action_space.sample()
                &#47&#47&#47&#47 Get new state and reward from environment
                s1, r, d, _ = env.step(a[0])
                if render: env.render()
                &#47&#47&#47&#47 Obtain the Q&quot values by feeding the new state through our network
                Q1 = qnetwork(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).numpy()

                &#47&#47&#47&#47 Obtain maxQ&quot and set our target value for chosen action.
                maxQ1 = np.max(Q1)  &#47&#47 in Q-Learning, policy is greedy, so we use "max" to select the next action.
                targetQ = allQ
                targetQ[0, a[0]] = r + lambd * maxQ1
                &#47&#47&#47&#47 Train network using target and predicted Q values
                &#47&#47 it is not real target Q value, it is just an estimation,
                &#47&#47 but check the Q-Learning update formula:
                &#47&#47    Q&quot(s,a) &lt;- Q(s,a) + alpha(r + lambd * maxQ(s&quot,a&quot) - Q(s, a))
                &#47&#47 minimizing |r + lambd * maxQ(s&quot,a&quot) - Q(s, a)|^2 equals to force Q&quot(s,a) â‰ˆ Q(s,a)
                with tf.GradientTape() as tape:
                    _qvalues = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32))
                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=False)
                grad = tape.gradient(_loss, train_weights)
                optimizer.apply_gradients(zip(grad, train_weights))

                rAll += r
                s = s1
                &#47&#47&#47&#47 Reduce chance of random action if an episode is done.
                if d == True:
                    e = 1. / ((i / 50) + 10)  &#47&#47 reduce e, GLIE: Greey in the limit with infinite Exploration
                    break

            &#47&#47&#47&#47 Note that, the rewards here with random action
            running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01
            &#47&#47 print("Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs " % \
            &#47&#47     (i, num_episodes, rAll, running_reward, time.time() - episode_time))
            print(&quotTraining  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}&quot \
                  .format(i, num_episodes, rAll, time.time() - t0))

            if i == 0:
                <a id="change">all_episode_reward.append(rAll)</a>
            else:
                all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)
</code></pre><img src="12834448.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/35b2c4917344f338eda67c78673cf4064b3b4265#diff-1d0340ad31e3cb69502a14426c672687eb15d10a85d20b5e050706a8e4f24b0fL102' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 35b2c4917344f338eda67c78673cf4064b3b4265</div><div id='time'> Time: 2020-02-07</div><div id='author'> Author: 34995488+Tokarev-TT-33@users.noreply.github.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_DQN.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/bd333dd43d69b26015eb3f201afe1772ba701a41#diff-c3102fa1640eb0e74d5ee3a673169d81527d7b01428fd3a2d4d72769b694359fL95' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: bd333dd43d69b26015eb3f201afe1772ba701a41</div><div id='time'> Time: 2018-05-07</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/contrib/dataset_sampler/sampler_uniform_v2.py</div><div id='class'> Class Name: UniformSampler</div><div id='method'> Method Name: layer_op</div><BR><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/ac24d1db30976e3fc038051bcf2b46d32be416cb#diff-4c21c8e3d7e46fb52de91390a47bf6ae19b29227f0c9d9bffa1c843340d80fe8L241' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: ac24d1db30976e3fc038051bcf2b46d32be416cb</div><div id='time'> Time: 2020-12-12</div><div id='author'> Author: maxfitton@anyscale.com</div><div id='file'> File Name: dashboard/datacenter.py</div><div id='class'> Class Name: DataOrganizer</div><div id='method'> Method Name: _get_actor</div><BR>