<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                rAll += r
                s = s1
                &#47&#47&#47&#47 Reduce chance of random action if an episode is done.
                <a id="change">if d ==True:
                    e = 1. / ((i / 50) + 10)  &#47&#47 reduce e, GLIE: Greey in the limit with infinite Exploration
                    break

            &#47&#47&#47&#47 Note that, the rewards here with random action
           </a> running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01
            &#47&#47 print("Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs " % \
            &#47&#47     (i, num_episodes, rAll, running_reward, time.time() - episode_time))
            print(&quotEpisode: {}/{}  | Episode Reward: {:.4f} | Running Average Reward: {:.4f}  | Running Time: {:.4f}&quot\</code></pre><h3>After Change</h3><pre><code class='java'>

    t0 = time.time()
    if args.train:
        <a id="change">all_episode_reward = []</a>
        for i in range(num_episodes):
            &#47&#47&#47&#47 Reset environment and get first new observation
            &#47&#47 episode_time = time.time()
            s = env.reset()  &#47&#47 observation is state, integer 0 ~ 15
            rAll = 0
            if render: env.render()
            for j in range(99):  &#47&#47 step index, maximum step is 99
                &#47&#47&#47&#47 Choose an action by greedily (with e chance of random action) from the Q-network
                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()
                a = np.argmax(allQ, 1)

                &#47&#47&#47&#47 e-Greedy Exploration !!! sample random action
                if np.random.rand(1) &lt; e:
                    a[0] = env.action_space.sample()
                &#47&#47&#47&#47 Get new state and reward from environment
                s1, r, d, _ = env.step(a[0])
                if render: env.render()
                &#47&#47&#47&#47 Obtain the Q&quot values by feeding the new state through our network
                Q1 = qnetwork(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).numpy()

                &#47&#47&#47&#47 Obtain maxQ&quot and set our target value for chosen action.
                maxQ1 = np.max(Q1)  &#47&#47 in Q-Learning, policy is greedy, so we use "max" to select the next action.
                targetQ = allQ
                targetQ[0, a[0]] = r + lambd * maxQ1
                &#47&#47&#47&#47 Train network using target and predicted Q values
                &#47&#47 it is not real target Q value, it is just an estimation,
                &#47&#47 but check the Q-Learning update formula:
                &#47&#47    Q&quot(s,a) &lt;- Q(s,a) + alpha(r + lambd * maxQ(s&quot,a&quot) - Q(s, a))
                &#47&#47 minimizing |r + lambd * maxQ(s&quot,a&quot) - Q(s, a)|^2 equals to force Q&quot(s,a) â‰ˆ Q(s,a)
                with tf.GradientTape() as tape:
                    _qvalues = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32))
                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=False)
                grad = tape.gradient(_loss, train_weights)
                optimizer.apply_gradients(zip(grad, train_weights))

                rAll += r
                s = s1
                &#47&#47&#47&#47 Reduce chance of random action if an episode is done.
                if d == True:
                    e = 1. / ((i / 50) + 10)  &#47&#47 reduce e, GLIE: Greey in the limit with infinite Exploration
                    break

            &#47&#47&#47&#47 Note that, the rewards here with random action
            running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01
            &#47&#47 print("Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs " % \
            &#47&#47     (i, num_episodes, rAll, running_reward, time.time() - episode_time))
            print(&quotTraining  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}&quot \
                  .format(i, num_episodes, rAll, time.time() - t0))

            if i == 0:
                <a id="change">all_episode_reward.append(rAll)</a>
            else:
                all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)
</code></pre>