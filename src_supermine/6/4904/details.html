<html><h3>4421754f9886233e90563eb8088348bb36024095,niftynet/layer/loss_segmentation.py,LossFunction,layer_op,#LossFunction#Any#Any#Any#,46
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        pred_b = tf.nn.softmax(
                            tf.cast(pred_b, dtype=tf.float32))
                    ground_truth_b = ground_truth[b_ind]
                    weight_b = <a id="change">None if weight_map is None else weight_map[b_ind]</a>

                    loss_params = {
                        &quotprediction&quot: pred_b,
                        &quotground_truth&quot: ground_truth_b,</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 performs softmax if required
                    if self._softmax:
                        pred_b = tf.cast(pred_b, dtype=tf.float32)
                        <a id="change">pred_b = tf.nn.softmax(pred_b)</a>

                    &#47&#47 reshape pred, ground_truth, weight_map to the same
                    &#47&#47 size: (n_voxels, num_classes)
                    &#47&#47 if the ground_truth has only one channel, the shape
                    &#47&#47 becomes: (n_voxels,)
                    spatial_shape = pred_b.get_shape().as_list()[:-1]
                    ref_shape = <a id="change">spatial_shape + [-1]</a>
                    ground_truth_b = tf.reshape(ground_truth[b_ind], ref_shape)
                    if ground_truth_b.get_shape().as_list()[-1] == 1:
                        ground_truth_b = tf.squeeze(ground_truth_b, axis=-1)
                    if weight_map is not None:
                        <a id="change">weight_b = tf.reshape(weight_map[b_ind], ref_shape)</a>
                        if weight_b.get_shape().as_list()[-1] == 1:
                            weight_b = tf.squeeze(weight_b, axis=-1)
                    else:
                        weight_b = None</code></pre><img src="21408548.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/4421754f9886233e90563eb8088348bb36024095#diff-023d37a8c76fa73084ec27927d67d215e2eba9f0bc6e99a32ee9d11a4d647fb2L69' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: 4421754f9886233e90563eb8088348bb36024095</div><div id='time'> Time: 2018-01-12</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/layer/loss_segmentation.py</div><div id='class'> Class Name: LossFunction</div><div id='method'> Method Name: layer_op</div><BR><BR><div id='link'><a href='https://github.com/dmlc/gluon-nlp/commit/06ecac8d4a0132657e8cd8ef9eb3cc5a48d3859f#diff-923bf4022efcda59b2e139db69fceea9b297efa3618eb0ccf629389c78883304L452' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/gluon-nlp</div><div id='commit'> Commit Name: 06ecac8d4a0132657e8cd8ef9eb3cc5a48d3859f</div><div id='time'> Time: 2019-10-24</div><div id='author'> Author: emilmont@gmail.com</div><div id='file'> File Name: src/gluonnlp/model/attention_cell.py</div><div id='class'> Class Name: DotProductAttentionCell</div><div id='method'> Method Name: _compute_weight</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/3a5ace850931e91c55a692ae7ec716a57e66f4e6#diff-023d37a8c76fa73084ec27927d67d215e2eba9f0bc6e99a32ee9d11a4d647fb2L69' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: 3a5ace850931e91c55a692ae7ec716a57e66f4e6</div><div id='time'> Time: 2018-01-26</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/layer/loss_segmentation.py</div><div id='class'> Class Name: LossFunction</div><div id='method'> Method Name: layer_op</div><BR>