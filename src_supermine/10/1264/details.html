<html><h3>415be78cc0d1275a29d0ceda550d0d7a7a5224ea,python/ray/util/sgd/torch/torch_trainer.py,TorchTrainer,__init__,#,171
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                 "For more information, see "
                 "https://github.com/pytorch/examples/issues/467."))

        <a id="change">if not (callable(model_creator) and callable(optimizer_creator)):
            raise ValueError(
                "Must provide a callable model_creator and optimizer_creator.")

       </a> if num_replicas is not None:
            raise DeprecationWarning(
                "num_replicas is deprecated. Use num_workers instead.")

        if batch_size is not None:
            raise DeprecationWarning(
                "batch_size is deprecated. Use config={&quotbatch_size&quot: N} "
                "specify a batch size for each worker or "
                "config={ray.util.sgd.utils.BATCH_SIZE: N} to specify a "
                "batch size to be used across all workers.")

        if data_loader_args:
            raise ValueError(
                "data_loader_args is deprecated. You can return a "
                "torch.utils.data.DataLoader in data_creator. Ray will "
                "automatically set a DistributedSampler if a DataLoader is "
                "returned and num_workers &gt; 1.")

        self.model_creator = model_creator
        self.optimizer_creator = optimizer_creator
        self.loss_creator = loss_creator
        self.data_creator = data_creator
        self.scheduler_creator = scheduler_creator
        self.training_operator_cls = training_operator_cls

        if not training_operator_cls and not loss_creator:
            <a id="change">raise ValueError("If a loss_creator is not provided, you must "
                             "provide a custom training operator.")</a>

        self.initialization_hook = initialization_hook
        self.config = {} if config is None else config
        if use_gpu == "auto":</code></pre><h3>After Change</h3><pre><code class='java'>
            serialize_data_creation=None,
            data_loader_args=None,
    ):
        <a id="change">if (model_creator or data_creator or optimizer_creator
                or scheduler_creator or loss_creator):
            raise DeprecationWarning(
                "Creator functions are deprecated. You should create a "
                "custom TrainingOperator, override setup, and register all "
                "training state there. See TrainingOperator for more info. "
                "If you would still like to use creator functions, you can "
                "do CustomOperator = TrainingOperator.from_creators("
                "model_creator, ...) and pass in CustomOperator into "
                "TorchTrainer.")

       </a> if num_workers &gt; 1 and not dist.is_available():
            raise ValueError(
                ("Distributed PyTorch is not supported on macOS. "
                 "To run without distributed PyTorch, set &quotnum_workers=1&quot. "
                 "For more information, see "
                 "https://github.com/pytorch/examples/issues/467."))

        if num_replicas is not None:
            raise DeprecationWarning(
                "num_replicas is deprecated. Use num_workers instead.")

        if batch_size is not None:
            raise DeprecationWarning(
                "batch_size is deprecated. Use config={&quotbatch_size&quot: N} "
                "specify a batch size for each worker or "
                "config={ray.util.sgd.utils.BATCH_SIZE: N} to specify a "
                "batch size to be used across all workers.")

        if serialize_data_creation is True:
            if log_once("serialize_data_creation"):
                <a id="change">logging.warning(
                    "serialize_data_creation is deprecated and will be "
                    "ignored. If you require serialized data loading you "
                    "should implement this in TrainingOperator.setup. "
                    "You may find FileLock useful here.")</a>

        if data_loader_args:
            raise DeprecationWarning(
                "data_loader_args is deprecated. You can return a "</code></pre><img src="7215150.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/415be78cc0d1275a29d0ceda550d0d7a7a5224ea#diff-ceddd7a3906823cadf3cfc67027bada51381473a998b0670382cfc86b779ce9aL160' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: 415be78cc0d1275a29d0ceda550d0d7a7a5224ea</div><div id='time'> Time: 2020-09-08</div><div id='author'> Author: amogkam@users.noreply.github.com</div><div id='file'> File Name: python/ray/util/sgd/torch/torch_trainer.py</div><div id='class'> Class Name: TorchTrainer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc#diff-f67ab59de16332a2ea8df0fb7032ea69a0912c3bb52538649b4ed0c9fc21d9d2L471' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: 860eb6f13a0e570b95bd251eb53105473850cbdc</div><div id='time'> Time: 2020-05-24</div><div id='author'> Author: ed.nmi.oakes@gmail.com</div><div id='file'> File Name: python/ray/actor.py</div><div id='class'> Class Name: ActorClass</div><div id='method'> Method Name: _remote</div><BR><BR><div id='link'><a href='https://github.com/uber/ludwig/commit/3e2f276459f976054b5c2ab8c55be994170345da#diff-5fc51a020800a8a1683887d964e202fc78acd413966aa9b3e2c01ed81b51190cL173' target='_blank'>Link</a></div><div id='project'> Project Name: uber/ludwig</div><div id='commit'> Commit Name: 3e2f276459f976054b5c2ab8c55be994170345da</div><div id='time'> Time: 2020-08-27</div><div id='author'> Author: carlo.grisetti@dsgroup.it</div><div id='file'> File Name: ludwig/utils/defaults.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: merge_with_defaults</div><BR><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/f3fdb5c5db42324e53af7909d15922a71cbf1112#diff-e0b7af68b3a968065328aee95e6583d49f83cd30522c1f8b580f36ce41cc58f1L10' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: f3fdb5c5db42324e53af7909d15922a71cbf1112</div><div id='time'> Time: 2020-07-26</div><div id='author'> Author: rliaw@berkeley.edu</div><div id='file'> File Name: python/ray/tune/session.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_session</div><BR>