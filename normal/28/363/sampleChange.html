<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        keep_input = keep_input_sub or implementation_bwd == -1 or implementation_fwd == -1
                        &#47&#47 print(bwd, coupling, keep_input, implementation_fwd, implementation_bwd)
                        &#47&#47 test with zero padded convolution
                        <a id="change">X</a> = <a id="change">Variable(torch.from_numpy(data.copy()))</a>
                        Ytarget = Variable(torch.from_numpy(target_data.copy()))
                        Xshape = X.shape
                        Gm2 = copy.deepcopy(Gm)
                        rb = revop.ReversibleBlock(Gm2, coupling=coupling, implementation_fwd=implementation_fwd,
                                                   implementation_bwd=implementation_bwd, keep_input=keep_input)
                        rb.train()
                        rb.zero_grad()

                        optim = torch.optim.RMSprop(rb.parameters())
                        optim.zero_grad()
                        if not bwd:
                            Xin = X.clone()
                            Y = rb(Xin)
                            Yrev = Y.clone()
                            <a id="change">Xinv</a> = rb.inverse(Yrev)
                        else:
                            <a id="change">Xin</a> = X.clone()
                            <a id="change">Y</a> = rb.inverse(Xin)
                            <a id="change">Yrev</a> = Y.clone()
                            Xinv = rb(Yrev)
                        <a id="change">loss</a> = torch.nn.MSELoss()(Y, Ytarget)

                        &#47&#47 has input been retained/discarded after forward (and backward) passes?
                        if keep_input:</code></pre><h3>After Change</h3><pre><code class='java'>
                        keep_input = keep_input_sub or implementation_bwd == -1 or implementation_fwd == -1
                        &#47&#47 print(bwd, coupling, keep_input, implementation_fwd, implementation_bwd)
                        &#47&#47 test with zero padded convolution
                        <a id="change">X</a> = <a id="change">torch.from_numpy(data.copy())</a>
                        Ytarget = torch.from_numpy(target_data.copy())
                        Xshape = X.shape
                        Gm2 = copy.deepcopy(Gm)
                        rb = revop.ReversibleBlock(Gm2, coupling=coupling, implementation_fwd=implementation_fwd,
                                                   implementation_bwd=implementation_bwd, keep_input=keep_input)
                        rb.train()
                        rb.zero_grad()

                        optim = torch.optim.RMSprop(rb.parameters())
                        optim.zero_grad()
                        if not bwd:
                            Xin = X.clone()
                            Y = rb(Xin)
                            Yrev = Y.clone()
                            <a id="change">Xinv</a> = rb.inverse(Yrev)
                        else:
                            <a id="change">Xin</a> = X.clone()
                            <a id="change">Y</a> = rb.inverse(Xin)
                            <a id="change">Yrev</a> = Y.clone()
                            Xinv = rb(Yrev)
                        <a id="change">loss</a> = torch.nn.MSELoss()(Y, Ytarget)

                        &#47&#47 has input been retained/discarded after forward (and backward) passes?
                        if keep_input:</code></pre>