<html><h3>f1e820cce3323cf2a9553f69918d85a9fcecd4f9,codelabs/spark-nlp/topic_model.py,,,#,48
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47 Since some of our data doesn&quott have a body, we can combine all of the text
&#47&#47 for the titles and bodies so that every row has useful data.

df_train = <a id="change">(
    reddit_data
    &#47&#47 Replace null values with an empty string
    .fillna("") 
    .select(
        &#47&#47 Combine columns
        concat(
            &#47&#47 First column to concatenate. col() is used to specify that we&quotre referencing a column
            col("title"), 
            &#47&#47 Literal character that will be between the concatenated columns.
            lit(" "), 
            &#47&#47 Second column to concatenate.
            col("body")
        &#47&#47 Change the name of the new column    
        ).alias("text") 
    )
)</a>

&#47&#47 Now, we begin assembling our pipeline. Each component here is used to some transformation to the data.
&#47&#47 The Document Assembler takes the raw text data and convert it into a format that can
&#47&#47 be tokenized. It becomes one of spark-nlp native object types, the "Document".
document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")

&#47&#47 The Tokenizer takes data that is of the "Document" type and tokenizes it. 
&#47&#47 While slightly more involved than this, this is effectively taking a string and splitting
&#47&#47 it along ths spaces, so each word is its own string. The data then becomes the 
&#47&#47 spark-nlp native type "Token".
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")

&#47&#47 The Normalizer will group words together based on similar semantic meaning. 
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normalizer")

&#47&#47 The Stemmer takes objects of class "Token" and converts the words into their
&#47&#47 root meaning. For instance, the words "cars", "cars&quot" and "car&quots" would all be replaced
&#47&#47 with the word "car".
stemmer = Stemmer().setInputCols(["normalizer"]).setOutputCol("stem")

&#47&#47 The Finisher signals to spark-nlp allows us to access the data outside of spark-nlp
&#47&#47 components. For instance, we can now feed the data into components from Spark MLlib. 
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")

&#47&#47 Stopwords are common words that generally don&quott add much detail to the meaning
&#47&#47 of a body of text. In English, these are mostly "articles" such as the words "the"
&#47&#47 and "of".
stopword_remover = StopWordsRemover(inputCol="to_spark", outputCol="filtered")

&#47&#47 Here we implement TF-IDF as an input to our LDA model. CountVectorizer (TF) keeps track
&#47&#47 of the vocabulary that&quots being created so we can map our topics back to their
&#47&#47 corresponding words.
&#47&#47 TF (term frequency) creates a matrix that counts how many times each word in the
&#47&#47 vocabulary appears in each body of text. This then gives each word a weight based
&#47&#47 on it&quots frequency.
tf = CountVectorizer(inputCol="filtered", outputCol="raw_features")

&#47&#47 Here we implement the IDF portion. IDF (Inverse document frequency) reduces 
&#47&#47 the weights of commonly-appearing words. 
idf = IDF(inputCol="raw_features", outputCol="features")

&#47&#47 LDA creates a statistical representation of how frequently words appear 
&#47&#47 together in order to create "topics" or groups of commonly appearing words.
lda = LDA(k=10, maxIter=10)

&#47&#47 We add all of the transformers into a Pipeline object. Each transformer
&#47&#47 will execute in the ordered provided to the "stages" parameter
pipeline = Pipeline(
    stages = [
        document_assembler,
        tokenizer,
        normalizer,        
        stemmer,
        finisher,
        stopword_remover,
        tf,
        idf,
        lda
    ]
)

&#47&#47 We fit the data to the model. 
model = pipeline.fit(df_train)

&#47&#47 Now that we have completed a pipeline, we want to output the topics as human-readable.
&#47&#47 To do this, we need to grab the vocabulary generated from our pipeline, grab the topic
&#47&#47 model and do the appropriate mapping.  The output from each individual component lives 
&#47&#47 in the model object. We can access them by referring to them by their position in 
&#47&#47 the pipeline via model.stages[&lt;ind&gt;]

&#47&#47 Let&quots create a reference our vocabulary.
<a id="change">vocab</a> = model.stages[-3].vocabulary

&#47&#47 Next, let&quots grab the topics generated by our LDA model via describeTopics(). Using collect(),
&#47&#47 we load the output into a Python array.
raw_topics = model.stages[-1].describeTopics().collect()

&#47&#47 Lastly, let&quots get the indices of the vocabulary terms from our topics
topic_inds = [ind.termIndices for <a id="change">ind</a> in raw_topics]

&#47&#47 The indices we just grab directly map to the term at position &lt;ind&gt; from our vocabulary. 
&#47&#47 Using the below code, we can generate the mappings from our topic indicies to our vocabulary.</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 for the titles and bodies so that every row has useful data.

df_train = (
    <a id="change">reddit_data</a>
    &#47&#47 Replace null values with an empty string
    .fillna("") 
    .select(
        &#47&#47 Combine columns
        concat(
            &#47&#47 First column to concatenate. col() is used to specify that we&quotre referencing a column
            col("title"), 
            &#47&#47 Literal character that will be between the concatenated columns.
            lit(" "), 
            &#47&#47 Second column to concatenate.
            col("body")
        &#47&#47 Change the name of the new column    
        ).alias("text") 
    )
    &#47&#47 The text has several tags including [REMOVED] or [DELETED] for redacted content. 
    &#47&#47 We&quotll replace these with empty strings.
    .select(
        regexp_replace(col("text"), "\[.*?\]", "")
        .alias("text")
    )
)

&#47&#47 Now, we begin assembling our pipeline. Each component here is used to some transformation to the data.
&#47&#47 The Document Assembler takes the raw text data and convert it into a format that can
&#47&#47 be tokenized. It becomes one of spark-nlp native object types, the "Document".
document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")

&#47&#47 The Tokenizer takes data that is of the "Document" type and tokenizes it. 
&#47&#47 While slightly more involved than this, this is effectively taking a string and splitting
&#47&#47 it along ths spaces, so each word is its own string. The data then becomes the 
&#47&#47 spark-nlp native type "Token".
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")

&#47&#47 The Normalizer will group words together based on similar semantic meaning. 
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normalizer")

&#47&#47 The Stemmer takes objects of class "Token" and converts the words into their
&#47&#47 root meaning. For instance, the words "cars", "cars&quot" and "car&quots" would all be replaced
&#47&#47 with the word "car".
stemmer = Stemmer().setInputCols(["normalizer"]).setOutputCol("stem")

&#47&#47 The Finisher signals to spark-nlp allows us to access the data outside of spark-nlp
&#47&#47 components. For instance, we can now feed the data into components from Spark MLlib. 
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")

&#47&#47 Stopwords are common words that generally don&quott add much detail to the meaning
&#47&#47 of a body of text. In English, these are mostly "articles" such as the words "the"
&#47&#47 and "of".
stopword_remover = StopWordsRemover(inputCol="to_spark", outputCol="filtered")

&#47&#47 Here we implement TF-IDF as an input to our LDA model. CountVectorizer (TF) keeps track
&#47&#47 of the vocabulary that&quots being created so we can map our topics back to their
&#47&#47 corresponding words.
&#47&#47 TF (term frequency) creates a matrix that counts how many times each word in the
&#47&#47 vocabulary appears in each body of text. This then gives each word a weight based
&#47&#47 on it&quots frequency.
tf = CountVectorizer(inputCol="filtered", outputCol="raw_features")

&#47&#47 Here we implement the IDF portion. IDF (Inverse document frequency) reduces 
&#47&#47 the weights of commonly-appearing words. 
idf = IDF(inputCol="raw_features", outputCol="features")

&#47&#47 LDA creates a statistical representation of how frequently words appear 
&#47&#47 together in order to create "topics" or groups of commonly appearing words.
&#47&#47 In this case, we&quotll create 5 topics.
lda = LDA(k=5)

&#47&#47 We add all of the transformers into a Pipeline object. Each transformer
&#47&#47 will execute in the ordered provided to the "stages" parameter
pipeline = Pipeline(
    stages = [
        document_assembler,
        tokenizer,
        normalizer,        
        stemmer,
        finisher,
        stopword_remover,
        tf,
        idf,
        lda
    ]
)

&#47&#47 We fit the data to the model. 
model = pipeline.fit(df_train)

&#47&#47 Now that we have completed a pipeline, we want to output the topics as human-readable.
&#47&#47 To do this, we need to grab the vocabulary generated from our pipeline, grab the topic
&#47&#47 model and do the appropriate mapping.  The output from each individual component lives 
&#47&#47 in the model object. We can access them by referring to them by their position in 
&#47&#47 the pipeline via model.stages[&lt;ind&gt;]

&#47&#47 Let&quots create a reference our vocabulary.
<a id="change">vocab</a> = model.stages[-3].vocabulary

&#47&#47 Next, let&quots grab the topics generated by our LDA model via describeTopics(). Using collect(),
&#47&#47 we load the output into a Python array.
raw_topics = model.stages[-1].describeTopics(maxTermsPerTopic=5).collect()

&#47&#47 Lastly, let&quots get the indices of the vocabulary terms from our topics
topic_inds = [ind.termIndices for <a id="change">ind</a> in raw_topics]

&#47&#47 The indices we just grab directly map to the term at position &lt;ind&gt; from our vocabulary. 
&#47&#47 Using the below code, we can generate the mappings from our topic indicies to our vocabulary.</code></pre><img src="1830427.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/GoogleCloudPlatform/cloud-dataproc/commit/f1e820cce3323cf2a9553f69918d85a9fcecd4f9#diff-1c96d42ecc5653aa111c1c6104fa8d7feedce35286948897129578d65bfc7a6aL107' target='_blank'>Link</a></div><div id='project'> Project Name: GoogleCloudPlatform/cloud-dataproc</div><div id='commit'> Commit Name: f1e820cce3323cf2a9553f69918d85a9fcecd4f9</div><div id='time'> Time: 2020-01-16</div><div id='author'> Author: bmiro@google.com</div><div id='file'> File Name: codelabs/spark-nlp/topic_model.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/scikit-learn-contrib/DESlib/commit/8b0eb05775343c5db867d207f6bf06bdfbb463c9#diff-94a1701a6014217bb8cdeb38df7630507985fd11d214cb28ebe726c4458f4001L261' target='_blank'>Link</a></div><div id='project'> Project Name: scikit-learn-contrib/DESlib</div><div id='commit'> Commit Name: 8b0eb05775343c5db867d207f6bf06bdfbb463c9</div><div id='time'> Time: 2018-04-04</div><div id='author'> Author: luiz.gh@gmail.com</div><div id='file'> File Name: deslib/dcs/base.py</div><div id='class'> Class Name: DCS</div><div id='method'> Method Name: predict_proba_with_ds</div><BR><BR><div id='link'><a href='https://github.com/nilmtk/nilmtk/commit/81467e352aa008ccb077bcbd77f17aa319b3b9b2#diff-c025ccf006371a59e3fafbd5946e392da0e03b48587138441437e8f4feb1b59eL110' target='_blank'>Link</a></div><div id='project'> Project Name: nilmtk/nilmtk</div><div id='commit'> Commit Name: 81467e352aa008ccb077bcbd77f17aa319b3b9b2</div><div id='time'> Time: 2014-11-18</div><div id='author'> Author: jack-list@xlk.org.uk</div><div id='file'> File Name: nilmtk/datastore.py</div><div id='class'> Class Name: HDFDataStore</div><div id='method'> Method Name: load</div><BR>