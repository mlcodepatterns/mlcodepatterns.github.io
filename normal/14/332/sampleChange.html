<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47 Since some of our data doesn&quott have a body, we can combine all of the text
&#47&#47 for the titles and bodies so that every row has useful data.

df_train = <a id="change">(
    reddit_data
    &#47&#47 Replace null values with an empty string
    .fillna("") 
    .select(
        &#47&#47 Combine columns
        concat(
            &#47&#47 First column to concatenate. col() is used to specify that we&quotre referencing a column
            col("title"), 
            &#47&#47 Literal character that will be between the concatenated columns.
            lit(" "), 
            &#47&#47 Second column to concatenate.
            col("body")
        &#47&#47 Change the name of the new column    
        ).alias("text") 
    )
)</a>

&#47&#47 Now, we begin assembling our pipeline. Each component here is used to some transformation to the data.
&#47&#47 The Document Assembler takes the raw text data and convert it into a format that can
&#47&#47 be tokenized. It becomes one of spark-nlp native object types, the "Document".
document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")

&#47&#47 The Tokenizer takes data that is of the "Document" type and tokenizes it. 
&#47&#47 While slightly more involved than this, this is effectively taking a string and splitting
&#47&#47 it along ths spaces, so each word is its own string. The data then becomes the 
&#47&#47 spark-nlp native type "Token".
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")

&#47&#47 The Normalizer will group words together based on similar semantic meaning. 
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normalizer")

&#47&#47 The Stemmer takes objects of class "Token" and converts the words into their
&#47&#47 root meaning. For instance, the words "cars", "cars&quot" and "car&quots" would all be replaced
&#47&#47 with the word "car".
stemmer = Stemmer().setInputCols(["normalizer"]).setOutputCol("stem")

&#47&#47 The Finisher signals to spark-nlp allows us to access the data outside of spark-nlp
&#47&#47 components. For instance, we can now feed the data into components from Spark MLlib. 
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")

&#47&#47 Stopwords are common words that generally don&quott add much detail to the meaning
&#47&#47 of a body of text. In English, these are mostly "articles" such as the words "the"
&#47&#47 and "of".
stopword_remover = StopWordsRemover(inputCol="to_spark", outputCol="filtered")

&#47&#47 Here we implement TF-IDF as an input to our LDA model. CountVectorizer (TF) keeps track
&#47&#47 of the vocabulary that&quots being created so we can map our topics back to their
&#47&#47 corresponding words.
&#47&#47 TF (term frequency) creates a matrix that counts how many times each word in the
&#47&#47 vocabulary appears in each body of text. This then gives each word a weight based
&#47&#47 on it&quots frequency.
tf = CountVectorizer(inputCol="filtered", outputCol="raw_features")

&#47&#47 Here we implement the IDF portion. IDF (Inverse document frequency) reduces 
&#47&#47 the weights of commonly-appearing words. 
idf = IDF(inputCol="raw_features", outputCol="features")

&#47&#47 LDA creates a statistical representation of how frequently words appear 
&#47&#47 together in order to create "topics" or groups of commonly appearing words.
lda = LDA(k=10, maxIter=10)

&#47&#47 We add all of the transformers into a Pipeline object. Each transformer
&#47&#47 will execute in the ordered provided to the "stages" parameter
pipeline = Pipeline(
    stages = [
        document_assembler,
        tokenizer,
        normalizer,        
        stemmer,
        finisher,
        stopword_remover,
        tf,
        idf,
        lda
    ]
)

&#47&#47 We fit the data to the model. 
model = pipeline.fit(df_train)

&#47&#47 Now that we have completed a pipeline, we want to output the topics as human-readable.
&#47&#47 To do this, we need to grab the vocabulary generated from our pipeline, grab the topic
&#47&#47 model and do the appropriate mapping.  The output from each individual component lives 
&#47&#47 in the model object. We can access them by referring to them by their position in 
&#47&#47 the pipeline via model.stages[&lt;ind&gt;]

&#47&#47 Let&quots create a reference our vocabulary.
<a id="change">vocab</a> = model.stages[-3].vocabulary

&#47&#47 Next, let&quots grab the topics generated by our LDA model via describeTopics(). Using collect(),
&#47&#47 we load the output into a Python array.
raw_topics = model.stages[-1].describeTopics().collect()

&#47&#47 Lastly, let&quots get the indices of the vocabulary terms from our topics
topic_inds = [ind.termIndices for <a id="change">ind</a> in raw_topics]

&#47&#47 The indices we just grab directly map to the term at position &lt;ind&gt; from our vocabulary. 
&#47&#47 Using the below code, we can generate the mappings from our topic indicies to our vocabulary.</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 for the titles and bodies so that every row has useful data.

df_train = (
    <a id="change">reddit_data</a>
    &#47&#47 Replace null values with an empty string
    .fillna("") 
    .select(
        &#47&#47 Combine columns
        concat(
            &#47&#47 First column to concatenate. col() is used to specify that we&quotre referencing a column
            col("title"), 
            &#47&#47 Literal character that will be between the concatenated columns.
            lit(" "), 
            &#47&#47 Second column to concatenate.
            col("body")
        &#47&#47 Change the name of the new column    
        ).alias("text") 
    )
    &#47&#47 The text has several tags including [REMOVED] or [DELETED] for redacted content. 
    &#47&#47 We&quotll replace these with empty strings.
    .select(
        regexp_replace(col("text"), "\[.*?\]", "")
        .alias("text")
    )
)

&#47&#47 Now, we begin assembling our pipeline. Each component here is used to some transformation to the data.
&#47&#47 The Document Assembler takes the raw text data and convert it into a format that can
&#47&#47 be tokenized. It becomes one of spark-nlp native object types, the "Document".
document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")

&#47&#47 The Tokenizer takes data that is of the "Document" type and tokenizes it. 
&#47&#47 While slightly more involved than this, this is effectively taking a string and splitting
&#47&#47 it along ths spaces, so each word is its own string. The data then becomes the 
&#47&#47 spark-nlp native type "Token".
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")

&#47&#47 The Normalizer will group words together based on similar semantic meaning. 
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normalizer")

&#47&#47 The Stemmer takes objects of class "Token" and converts the words into their
&#47&#47 root meaning. For instance, the words "cars", "cars&quot" and "car&quots" would all be replaced
&#47&#47 with the word "car".
stemmer = Stemmer().setInputCols(["normalizer"]).setOutputCol("stem")

&#47&#47 The Finisher signals to spark-nlp allows us to access the data outside of spark-nlp
&#47&#47 components. For instance, we can now feed the data into components from Spark MLlib. 
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")

&#47&#47 Stopwords are common words that generally don&quott add much detail to the meaning
&#47&#47 of a body of text. In English, these are mostly "articles" such as the words "the"
&#47&#47 and "of".
stopword_remover = StopWordsRemover(inputCol="to_spark", outputCol="filtered")

&#47&#47 Here we implement TF-IDF as an input to our LDA model. CountVectorizer (TF) keeps track
&#47&#47 of the vocabulary that&quots being created so we can map our topics back to their
&#47&#47 corresponding words.
&#47&#47 TF (term frequency) creates a matrix that counts how many times each word in the
&#47&#47 vocabulary appears in each body of text. This then gives each word a weight based
&#47&#47 on it&quots frequency.
tf = CountVectorizer(inputCol="filtered", outputCol="raw_features")

&#47&#47 Here we implement the IDF portion. IDF (Inverse document frequency) reduces 
&#47&#47 the weights of commonly-appearing words. 
idf = IDF(inputCol="raw_features", outputCol="features")

&#47&#47 LDA creates a statistical representation of how frequently words appear 
&#47&#47 together in order to create "topics" or groups of commonly appearing words.
&#47&#47 In this case, we&quotll create 5 topics.
lda = LDA(k=5)

&#47&#47 We add all of the transformers into a Pipeline object. Each transformer
&#47&#47 will execute in the ordered provided to the "stages" parameter
pipeline = Pipeline(
    stages = [
        document_assembler,
        tokenizer,
        normalizer,        
        stemmer,
        finisher,
        stopword_remover,
        tf,
        idf,
        lda
    ]
)

&#47&#47 We fit the data to the model. 
model = pipeline.fit(df_train)

&#47&#47 Now that we have completed a pipeline, we want to output the topics as human-readable.
&#47&#47 To do this, we need to grab the vocabulary generated from our pipeline, grab the topic
&#47&#47 model and do the appropriate mapping.  The output from each individual component lives 
&#47&#47 in the model object. We can access them by referring to them by their position in 
&#47&#47 the pipeline via model.stages[&lt;ind&gt;]

&#47&#47 Let&quots create a reference our vocabulary.
<a id="change">vocab</a> = model.stages[-3].vocabulary

&#47&#47 Next, let&quots grab the topics generated by our LDA model via describeTopics(). Using collect(),
&#47&#47 we load the output into a Python array.
raw_topics = model.stages[-1].describeTopics(maxTermsPerTopic=5).collect()

&#47&#47 Lastly, let&quots get the indices of the vocabulary terms from our topics
topic_inds = [ind.termIndices for <a id="change">ind</a> in raw_topics]

&#47&#47 The indices we just grab directly map to the term at position &lt;ind&gt; from our vocabulary. 
&#47&#47 Using the below code, we can generate the mappings from our topic indicies to our vocabulary.</code></pre>