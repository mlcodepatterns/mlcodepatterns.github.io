<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    assert model.local_conditioning_enabled()
    assert not model.has_speaker_embedding()

    <a id="change">x</a> = Variable(<a id="change">torch.from_numpy(x).contiguous()</a>)
    x = <a id="change">x.cuda() if use_cuda else x</a>

    <a id="change">c</a> = Variable(<a id="change">torch.from_numpy(c).contiguous()</a>)
    c = <a id="change">c.cuda() if use_cuda else c</a>
    print(x.size(), c.size())

    model.eval()

    <a id="change">y_offline</a> = model(x, c=c, softmax=True)

    &#47&#47 Incremental forward with forced teaching
    <a id="change">y_online</a> = model.incremental_forward(
        test_inputs=x, c=c, T=None, tqdm=tqdm, softmax=True, quantize=False)

    &#47&#47 (1 x C x T)
    <a id="change">c</a> = (y_offline - y_online).abs()
    print(c.mean(), c.max())

    try:</code></pre><h3>After Change</h3><pre><code class='java'>
    assert model.local_conditioning_enabled()
    assert not model.has_speaker_embedding()

    <a id="change">x</a> = <a id="change">torch</a>.from_numpy(x).contiguous().to(device)

    <a id="change">c</a> = <a id="change">torch</a>.from_numpy(c).contiguous().to(device)
    print(x.size(), c.size())

    model.eval()

    <a id="change">y_offline</a> = model(x, c=c, softmax=True)

    &#47&#47 Incremental forward with forced teaching
    <a id="change">y_online</a> = model.incremental_forward(
        test_inputs=x, c=c, T=None, tqdm=tqdm, softmax=True, quantize=False)

    &#47&#47 (1 x C x T)
    <a id="change">c</a> = (y_offline - y_online).abs()
    print(c.mean(), c.max())

    try:</code></pre>