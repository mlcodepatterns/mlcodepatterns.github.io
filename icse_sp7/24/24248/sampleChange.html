<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                x = torch.cat((x, action), dim=1)
            activation_func = getattr(F, activation)
            fc_func = self.layers[i]
            x = <a id="change">fc_func(x) if activation == "linear" else activation_func(fc_func(x))</a>
        return x


def fan_in_init(weight_tensor, bias_tensor) -&gt; None:</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 Actions skip input layer
            elif i == 1:
                x = self.batch_norm_ops[i](x)
                <a id="change">x</a> = torch.cat((x, action), dim=1)

            x = <a id="change">self.layers[i](x)</a>
            <a id="change">if activation == "linear":
                continue
            elif activation == "tanh":
                activation_func = torch.tanh
            else:
                activation_func = getattr(F, activation)
           </a> <a id="change">x = activation_func(x)</a>
        return x


def fan_in_init(weight_tensor, bias_tensor) -&gt; None:</code></pre>