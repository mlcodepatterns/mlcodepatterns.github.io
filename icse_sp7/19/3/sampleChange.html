<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                training_batch = training_batch.as_parametric_sarsa_training_batch()

        learning_input = training_batch.training_input
        <a id="change">self.minibatch</a> += 1

        reward = learning_input.reward
        not_done_mask = learning_input.not_terminal

        discount_tensor = torch.full_like(reward, self.gamma)
        if self.use_seq_num_diff_as_time_diff:
            assert self.multi_steps is None
            discount_tensor = torch.pow(self.gamma, learning_input.time_diff.float())
        if self.multi_steps is not None:
            discount_tensor = torch.pow(self.gamma, learning_input.step.float())

        if self.maxq_learning:
            all_next_q_values, all_next_q_values_target = self.get_detached_q_values(
                learning_input.tiled_next_state, learning_input.possible_next_actions
            )
            &#47&#47 Compute max a&quot Q(s&quot, a&quot) over all possible actions using target network
            next_q_values, _ = self.get_max_q_values_with_target(
                all_next_q_values.q_value,
                all_next_q_values_target.q_value,
                learning_input.possible_next_actions_mask.float(),
            )
        else:
            &#47&#47 SARSA (Use the target network)
            _, next_q_values = self.get_detached_q_values(
                learning_input.next_state, learning_input.next_action
            )
            next_q_values = next_q_values.q_value

        filtered_max_q_vals = next_q_values * not_done_mask.float()

        <a id="change">if self.minibatch &lt; self.reward_burnin:
            target_q_values = reward
        else:
            target_q_values = reward + (discount_tensor * filtered_max_q_vals)

        &#47&#47 Get Q-value of action taken
       </a> current_state_action = rlt.StateAction(
            state=learning_input.state, action=learning_input.action
        )
        q_values = self.q_network(current_state_action).q_value
        self.all_action_scores = q_values.detach()

        value_loss = self.q_network_loss(q_values, target_q_values)
        self.loss = value_loss.detach()

        self.q_network_optimizer.zero_grad()
        value_loss.backward()
        self.q_network_optimizer.step()

        &#47&#47 TODO: Maybe soft_update should belong to the target network
        <a id="change">if self.minibatch &lt; self.reward_burnin:
            &#47&#47 Reward burnin: force target network
            self._soft_update(self.q_network, self.q_network_target, 1.0)
        else:
            &#47&#47 Use the soft update rule to update target network
            self._soft_update(self.q_network, self.q_network_target, self.tau)

        &#47&#47 get reward estimates
       </a> reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)
        self.reward_network_optimizer.zero_grad()
        reward_loss.backward()</code></pre><h3>After Change</h3><pre><code class='java'>
        self.q_network_optimizer.step()

        &#47&#47 Use the soft update rule to update target network
        <a id="change">self._soft_update(self.q_network, self.q_network_target, self.tau)</a>

        &#47&#47 get reward estimates
        reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)</code></pre>