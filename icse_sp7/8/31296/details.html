<html><h3>247203f29b7e841204c76d922c1ea5b2680c3663,reagent/models/seq2slate.py,DecoderPyTorch,forward,#DecoderPyTorch#Any#Any#Any#Any#,213
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 (2) True -&gt; item should be ignored in attention
        tgt_mask = tgt_tgt_mask[0, :, :] == 0
        memory_mask = tgt_src_mask[0, :, :] == 0
        output = <a id="change">super</a>(DecoderPyTorch, self).forward(tgt, memory, tgt_mask, memory_mask)
        <a id="change">return output.transpose(0, 1)</a>


class MultiHeadedAttention(nn.Module):
    def __init__(self, num_heads, dim_model):</code></pre><h3>After Change</h3><pre><code class='java'>
        tgt_embed = tgt_embed.transpose(0, 1)
        memory = memory.transpose(0, 1)

        <a id="change">output = tgt_embed</a>

        for mod in self.layers:
            output = mod(
                output,
                memory,
                tgt_mask=tgt_tgt_mask,
                memory_mask=tgt_src_mask,
            )

        batch_size, tgt_seq_len, _ = <a id="change">output.shape</a>
        probs_for_placeholders = torch.zeros(
            batch_size, tgt_seq_len, 2, device=<a id="change">output.device</a>
        )
        <a id="change">probs = torch.cat((probs_for_placeholders, output), dim=2)</a>
        <a id="change">return probs</a>


class MultiHeadedAttention(nn.Module):
    def __init__(self, num_heads, dim_model):</code></pre><img src="153730856.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/247203f29b7e841204c76d922c1ea5b2680c3663#diff-483c86d3177f3b02a8e85bd594c456417a28c668d57dff1ded48e1a6ff2826f1L213' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 247203f29b7e841204c76d922c1ea5b2680c3663</div><div id='time'> Time: 2020-12-08</div><div id='author'> Author: czxttkl@fb.com</div><div id='file'> File Name: reagent/models/seq2slate.py</div><div id='class'> Class Name: DecoderPyTorch</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-tf/commit/30efaaa572d798212c926e5b2edbf2b0fe7fa2f1#diff-7fe9d1e9c3060d9aaea313016eef33efc511e1a0245d2ad16ab0ae2b185aaa0eL117' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-tf</div><div id='commit'> Commit Name: 30efaaa572d798212c926e5b2edbf2b0fe7fa2f1</div><div id='time'> Time: 2019-07-15</div><div id='author'> Author: guillaume.klein@systrangroup.com</div><div id='file'> File Name: opennmt/decoders/rnn_decoder.py</div><div id='class'> Class Name: AttentionalRNNDecoder</div><div id='method'> Method Name: _get_initial_state</div><BR><BR><div id='link'><a href='https://github.com/polyaxon/polyaxon/commit/50bd806d8696d082fb5388c38c4e1375857d16b9#diff-25f17bd780872ebcda007849d39310b0814bca453f700882d36c18e41934af2fL70' target='_blank'>Link</a></div><div id='project'> Project Name: polyaxon/polyaxon</div><div id='commit'> Commit Name: 50bd806d8696d082fb5388c38c4e1375857d16b9</div><div id='time'> Time: 2018-11-15</div><div id='author'> Author: mouradmourafiq@gmail.com</div><div id='file'> File Name: polyaxon/api/endpoint/base.py</div><div id='class'> Class Name: BaseEndpoint</div><div id='method'> Method Name: dispatch</div><BR>