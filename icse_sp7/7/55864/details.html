<html><h3>875a33806acea37f602d0ad20fb77cd42432bbb6,scripts/tokenize/tokenize_pad.py,,,#,6
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
rcParams["debug"]=True

wl = Wordlist("pad_data_qlc.qlc")
<a id="change">wl.tokenize(&quotpad_orthography_profile&quot)</a>
wl.output("qlc", filename="tokenized-pad_data")
</code></pre><h3>After Change</h3><pre><code class='java'>
rc(debug=True)

&#47&#47 make sure the pad data is in this directory
<a id="change">infile = codecs.open("pad_data_qlc.qlc", "r", "utf-8")</a>
header = infile.readline() &#47&#47 skip pad header

<a id="change">t = Tokenizer("pad_orthography_profile")</a>

print()
print("ID"+"\t"+"ORIGINAL"+"\t"+"RULES")
<a id="change">for line in infile:
    line = line.strip()
    tokens = line.split("\t")
    id = tokens[0]
    counterpart = tokens[2]
    grapheme_clusters =t.grapheme_clusters(counterpart)
    rules = t.rules(grapheme_clusters)
    print(id+"\t"+counterpart+"\t"+rules)


</a>
&#47&#47 this tokenize does not work because of the way the orthography rules are currently written, i.e. 
&#47&#47 they expect space delimited tokens; the wordlist.tokenize() function first apples the rules
&#47&#47 and the the Unicode grapheme cluster tokenization</code></pre><img src="258228196.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lingpy/lingpy/commit/875a33806acea37f602d0ad20fb77cd42432bbb6#diff-1c73bc26dcce2401b2fa9da126c3b0acc85e523ec1635c04b5d59e90e78baef0L11' target='_blank'>Link</a></div><div id='project'> Project Name: lingpy/lingpy</div><div id='commit'> Commit Name: 875a33806acea37f602d0ad20fb77cd42432bbb6</div><div id='time'> Time: 2013-11-08</div><div id='author'> Author: bambooforest@gmail.com</div><div id='file'> File Name: scripts/tokenize/tokenize_pad.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/kermitt2/delft/commit/b445c177a4543617c843f24943bb00e69ab24b36#diff-a2da2313b7efb023ecc294820197614353a9ab5741f72d63b272eca4140f8b95L16' target='_blank'>Link</a></div><div id='project'> Project Name: kermitt2/delft</div><div id='commit'> Commit Name: b445c177a4543617c843f24943bb00e69ab24b36</div><div id='time'> Time: 2018-05-02</div><div id='author'> Author: patrice.lopez@science-miner.com</div><div id='file'> File Name: sequenceLabelling/tokenizer.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: tokenizeAndFilter</div><BR><BR><div id='link'><a href='https://github.com/WZBSocialScienceCenter/tmtoolkit/commit/d1196006be574c16473df6efed448f9fa308a680#diff-51378749b59c3a776a9dbdcd863747e10ffe4991a82e8215a12c939d11a1c7bcL399' target='_blank'>Link</a></div><div id='project'> Project Name: WZBSocialScienceCenter/tmtoolkit</div><div id='commit'> Commit Name: d1196006be574c16473df6efed448f9fa308a680</div><div id='time'> Time: 2019-03-06</div><div id='author'> Author: markus.konrad@wzb.eu</div><div id='file'> File Name: tests/test_preprocess.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: test_tmpreproc_en_transform_tokens_lambda</div><BR>