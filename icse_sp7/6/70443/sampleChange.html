<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47 Create random Tensors for weights, and wrap them in Variables.
&#47&#47 Setting requires_grad=True indicates that we want to compute gradients with
&#47&#47 respect to these Variables during the backward pass.
w1 = Variable(<a id="change">torch.randn(D_in, H).type(dtype)</a>, requires_grad=True)
w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)

learning_rate = 1e-6</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 Create random Tensors to hold input and outputs.
&#47&#47 Setting requires_grad=False indicates that we do not need to compute gradients
&#47&#47 with respect to these Tensors during the backward pass.
x = <a id="change">torch.randn(N, D_in, device=device, dtype=dtype)</a>
y = <a id="change">torch.randn(N, D_out, device=device, dtype=dtype)</a>

&#47&#47 Create random Tensors for weights.
&#47&#47 Setting requires_grad=True indicates that we want to compute gradients with
&#47&#47 respect to these Tensors during the backward pass.
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    &#47&#47 Forward pass: compute predicted y using operations on Tensors; these
    &#47&#47 are exactly the same operations we used to compute the forward pass using
    &#47&#47 Tensors, but we do not need to keep references to intermediate values since
    &#47&#47 we are not implementing the backward pass by hand.
    y_pred = x.mm(w1).clamp(min=0).mm(w2)

    &#47&#47 Compute and print loss using operations on Tensors.
    &#47&#47 Now loss is a Tensor of shape (1,)
    &#47&#47 loss.item() gets the a scalar value held in the loss.
    loss = (y_pred - y).pow(2).sum()
    print(t, <a id="change">loss.item()</a>)

    &#47&#47 Use autograd to compute the backward pass. This call will compute the
    &#47&#47 gradient of loss with respect to all Tensors with requires_grad=True.</code></pre>