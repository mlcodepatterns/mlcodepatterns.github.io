<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    The code is borrowed from: `here &lt;https://en.wikipedia.org/wiki/Cross_entropy&gt;`_.
    
    with tf.name_scope("cross_entropy_loss"):
        <a id="change">net_output_tf = output</a>
        target_tf = target
        <a id="change">cross_entropy = tf.add(tf.mul(tf.log(net_output_tf, name=None),target_tf),
                             tf.mul(tf.log(1 - net_output_tf), (1 - target_tf)))</a>
        <a id="change">return -1 * tf.reduce_mean(tf.reduce_sum(cross_entropy, 1), name=&quotcross_entropy_mean&quot)</a>

def mean_squared_error(output, target):
    Return the cost function of Mean-squre-error of two distributions.
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 cross_entropy = tf.add(tf.mul(tf.log(net_output_tf, name=None),target_tf),
        &#47&#47                      tf.mul(tf.log(1 - net_output_tf), (1 - target_tf)))
        &#47&#47 return -1 * tf.reduce_mean(tf.reduce_sum(cross_entropy, 1), name=&quotcross_entropy_mean&quot)
        <a id="change">return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output, target))</a>

def mean_squared_error(output, target):
    Return the cost function of Mean-squre-error of two distributions.
</code></pre>