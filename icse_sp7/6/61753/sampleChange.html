<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47

x = torch.randn((2, 2))
y = torch.randn(<a id="change">(2, 2)</a>)
z = x + y  &#47&#47 These are Tensor types, and backprop would not be possible

var_x = autograd.Variable(x, requires_grad=True)
var_y = <a id="change">autograd.Variable(y, requires_grad=True)</a>
&#47&#47 var_z contains enough information to compute gradients, as we saw above
var_z = var_x + var_y
print(var_z.grad_fn)

<a id="change">var_z_data = var_z.data</a>  &#47&#47 Get the wrapped Tensor object out of var_z...
&#47&#47 Re-wrap the tensor in a new variable
new_var_z = autograd.Variable(var_z_data)
</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 with requires_grad=True by wrapping the code block in
&#47&#47 ``with torch.no_grad():``
print(x.requires_grad)
<a id="change">print((x ** 2).requires_grad)</a>

with torch.no_grad():
	print((x ** 2).requires_grad)
</code></pre>