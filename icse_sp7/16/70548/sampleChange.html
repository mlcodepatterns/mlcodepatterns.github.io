<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            env = OpenAIGym(&quotCartPole-v0&quot)

            &#47&#47 Create a Trust Region Policy Optimization agent
            agent = PPOAgent(config=<a id="change">Configuration(
                log_level=&quotinfo&quot,
                batch_size=4096,

                gae_lambda=0.97,
                learning_rate=0.001,
                entropy_penalty=0.01,
                epochs=5,
                optimizer_batch_size=512,
                loss_clipping=0.2,
                states=env.states,
                actions=env.actions,
                network=layered_network_builder([
                    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),
                    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
                ])
            )</a>)
            runner = Runner(agent=agent, environment=env)

            def episode_finished(r):</code></pre><h3>After Change</h3><pre><code class='java'>
        for _ in xrange(3):
            &#47&#47 Create an OpenAIgym environment
            env = OpenAIGym(&quotCartPole-v0&quot)
            <a id="change">config = Configuration(
                batch_size=4096,
                &#47&#47 Agent
                preprocessing=None,
                exploration=None,
                reward_preprocessing=None,
                &#47&#47 BatchAgent
                keep_last_timestep=True,  &#47&#47 not documented!
                &#47&#47 PPOAgent
                step_optimizer=dict(
                    type=&quotadam&quot,
                    learning_rate=1e-3
                ),
                optimization_steps=10,
                &#47&#47 Model
                scope=&quotppo&quot,
                discount=0.99,
                &#47&#47 DistributionModel
                distributions=None,  &#47&#47 not documented!!!
                entropy_regularization=0.01,
                &#47&#47 PGModel
                baseline_mode=None,
                baseline=None,
                baseline_optimizer=None,
                gae_lambda=None,
                normalize_rewards=False,
                &#47&#47 PGLRModel
                likelihood_ratio_clipping=0.2,
                &#47&#47 Logging
                log_level=&quotinfo&quot,
                &#47&#47 TensorFlow Summaries
                summary_logdir=None,
                summary_labels=[&quottotal-loss&quot],
                summary_frequency=1,
                &#47&#47 Distributed
                distributed=False,
                device=None
    </a>        )

            network_spec = [
                dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),</code></pre>