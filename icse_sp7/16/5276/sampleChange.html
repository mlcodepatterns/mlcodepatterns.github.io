<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    if attack_success:
                        logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                        if l2dist &lt; best_l2dist:
                            <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot % (l2dist, best_l2dist))</a>
                            best_l2dist = l2dist
                            best_adv_image = adv_image
                    
                    &#47&#47 compute gradient:
                    logger.debug(&quotCompute loss gradient&quot)
                    perturbation_tanh = -self._loss_gradient(z, target, image, adv_image, adv_image_tanh, 
                                                             c, clip_min, clip_max)
                    
                    &#47&#47 perform line search to optimize perturbation                     
                    &#47&#47 first, halve the learning rate until perturbation actually decreases the loss:                      
                    prev_loss = loss
                    best_loss = loss
                    best_lr = 0
                    
                    halving = 0
                    while loss &gt;= prev_loss and halving &lt; self.max_halving:
                        logger.debug(&quotApply gradient with learning rate %f (halving=%i)&quot % (lr, halving))
                        new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                        new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                        _, l2dist, loss = self._loss(image, new_adv_image, target, c) 
                        logger.debug(&quotNew Total Loss: %f&quot, loss)
                        logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                        logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)      
                        if loss &lt; best_loss:
                            best_loss = loss
                            best_lr = lr
                        lr /= 2
                        halving += 1                        
                    lr *= 2
                    
                    &#47&#47 if no halving was actually required, double the learning rate as long as this
                    &#47&#47 decreases the loss:
                    if halving == 1 and loss &lt;= prev_loss:
                        doubling = 0
                        while loss &lt;= prev_loss and doubling &lt; self.max_doubling:  
                            prev_loss = loss
                            lr *= 2     
                            logger.debug(&quotApply gradient with learning rate %f (doubling=%i)&quot % (lr, doubling))
                            doubling += 1
                            new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                            new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                            _, l2dist, loss = self._loss(image, new_adv_image, target, c)                            
                            logger.debug(&quotNew Total Loss: %f&quot, loss)
                            logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                            logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)     
                            if loss &lt; best_loss:
                                best_loss = loss
                                best_lr = lr            
                        lr /= 2
                    
                    if best_lr &gt;0:
                        logger.debug(&quotFinally apply gradient with learning rate %f&quot, best_lr)
                        &#47&#47 apply the optimal learning rate that was found and update the loss:
                        adv_image_tanh = adv_image_tanh + best_lr * perturbation_tanh
                        adv_image = self._tanh_to_original(adv_image_tanh, clip_min, clip_max)
                        
                    z, l2dist, loss = self._loss(image, adv_image, target, c)                    
                    attack_success = (loss - l2dist &lt;= 0)
                    overall_attack_success = overall_attack_success or attack_success
                
                &#47&#47 Update depending on attack success:
                if attack_success:
                    logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                    if l2dist &lt; best_l2dist:
                        <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot % (l2dist, best_l2dist))</a>
                        best_l2dist = l2dist
                        best_adv_image = adv_image
                
                if overall_attack_success:</code></pre><h3>After Change</h3><pre><code class='java'>
                    if attack_success:
                        logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                        if l2dist &lt; best_l2dist:
                            <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot, l2dist, best_l2dist)</a>
                            best_l2dist = l2dist
                            best_adv_image = adv_image
                    
                    &#47&#47 compute gradient:
                    logger.debug(&quotCompute loss gradient&quot)
                    perturbation_tanh = -self._loss_gradient(z, target, image, adv_image, adv_image_tanh, 
                                                             c, clip_min, clip_max)
                    
                    &#47&#47 perform line search to optimize perturbation                     
                    &#47&#47 first, halve the learning rate until perturbation actually decreases the loss:                      
                    prev_loss = loss
                    best_loss = loss
                    best_lr = 0
                    
                    halving = 0
                    while loss &gt;= prev_loss and halving &lt; self.max_halving:
                        logger.debug(&quotApply gradient with learning rate %f (halving=%i)&quot, lr, halving)
                        new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                        new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                        _, l2dist, loss = self._loss(image, new_adv_image, target, c) 
                        logger.debug(&quotNew Total Loss: %f&quot, loss)
                        logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                        logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)      
                        if loss &lt; best_loss:
                            best_loss = loss
                            best_lr = lr
                        lr /= 2
                        halving += 1                        
                    lr *= 2
                    
                    &#47&#47 if no halving was actually required, double the learning rate as long as this
                    &#47&#47 decreases the loss:
                    if halving == 1 and loss &lt;= prev_loss:
                        doubling = 0
                        while loss &lt;= prev_loss and doubling &lt; self.max_doubling:  
                            prev_loss = loss
                            lr *= 2     
                            logger.debug(&quotApply gradient with learning rate %f (doubling=%i)&quot, lr, doubling)
                            doubling += 1
                            new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                            new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                            _, l2dist, loss = self._loss(image, new_adv_image, target, c)                            
                            logger.debug(&quotNew Total Loss: %f&quot, loss)
                            logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                            logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)     
                            if loss &lt; best_loss:
                                best_loss = loss
                                best_lr = lr            
                        lr /= 2
                    
                    if best_lr &gt;0:
                        logger.debug(&quotFinally apply gradient with learning rate %f&quot, best_lr)
                        &#47&#47 apply the optimal learning rate that was found and update the loss:
                        adv_image_tanh = adv_image_tanh + best_lr * perturbation_tanh
                        adv_image = self._tanh_to_original(adv_image_tanh, clip_min, clip_max)
                        
                    z, l2dist, loss = self._loss(image, adv_image, target, c)                    
                    attack_success = (loss - l2dist &lt;= 0)
                    overall_attack_success = overall_attack_success or attack_success
                
                &#47&#47 Update depending on attack success:
                if attack_success:
                    logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                    if l2dist &lt; best_l2dist:
                        <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot, l2dist, best_l2dist)</a>
                        best_l2dist = l2dist
                        best_adv_image = adv_image
                
                if overall_attack_success:</code></pre>