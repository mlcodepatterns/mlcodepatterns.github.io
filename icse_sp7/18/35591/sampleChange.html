<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    test_loss = 0
    correct = 0

    <a id="change">for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        s_output, t_output = model(data, data, target)
        test_loss += F.nll_loss(F.log_softmax(s_output, dim = 1), target, size_average=False).data[0] &#47&#47 sum up batch loss
        pred = s_output.data.max(1)[1] &#47&#47 get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

   </a> test_loss /= len(test_loader.dataset)
    print(args.test_dir, &quot\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&quot.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre><h3>After Change</h3><pre><code class='java'>
    model.eval()
    test_loss = 0
    correct = 0
    <a id="change">with torch.no_grad():
        for data, target in test_loader:
            if args.cuda:
                data, target = data.cuda(), target.cuda()
            s_output, t_output = model(data, data, target)
            test_loss += F.nll_loss(F.log_softmax(s_output, dim = 1), target, reduction=&quotsum&quot).item()&#47&#47 sum up batch loss
            pred = s_output.data.max(1)[1] &#47&#47 get the index of the max log-probability
            correct += pred.eq(target.data.view_as(pred)).cpu().sum()

        test_loss /= len(test_loader.dataset)
        print(args.test_dir, &quot\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&quot.format(
            test_loss, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))
   </a> return correct


if __name__ == &quot__main__&quot:</code></pre>