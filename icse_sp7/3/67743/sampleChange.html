<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def clip_grad_norm(self, max_norm):
        Clips gradient norm.
        if max_norm &gt; 0:
            <a id="change">return torch.nn.utils.clip_grad_norm_(self.params, max_norm)</a>
        else:
            return torch.sqrt(sum(p.grad.data.norm()**2 for p in self.params if p.grad is not None))

    def step(self, closure=None):</code></pre><h3>After Change</h3><pre><code class='java'>

    def clip_grad_norm(self, max_norm):
        Clips gradient norm.
        <a id="change">return utils.clip_grad_norm_(self.params, max_norm)</a>

    def step(self, closure=None):
        Performs a single optimization step.
        self.optimizer.step(closure)</code></pre>