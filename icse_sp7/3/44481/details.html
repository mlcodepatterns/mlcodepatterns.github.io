<html><h3>f81e27a5cbe72ec3bf22f12d5a9ea60008bb8f1c,allennlp/modules/seq2seq_encoders/stacked_self_attention.py,StackedSelfAttentionEncoder,forward,#StackedSelfAttentionEncoder#Any#Any#,117
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                &#47&#47 layers, so we exclude it here.
                feedforward_output += cached_input
            &#47&#47 shape (batch_size, sequence_length, hidden_dim)
            attention_output = attention(<a id="change">layer_norm(feedforward_output)</a>, mask)
            output = self.dropout(attention_output) + feedforward_output
        return self._output_layer_norm(output)
</code></pre><h3>After Change</h3><pre><code class='java'>
            if feedforward_output.size() == cached_input.size():
                &#47&#47 First layer might have the wrong size for highway
                &#47&#47 layers, so we exclude it here.
                feedforward_output = feedforward_layer_norm(<a id="change">feedforward_output + cached_input</a>)
            &#47&#47 shape (batch_size, sequence_length, hidden_dim)
            attention_output = attention(feedforward_output, mask)
            output = <a id="change">layer_norm(self.dropout(attention_output) + feedforward_output)</a>

        return output

    @classmethod</code></pre><img src="208449352.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/f81e27a5cbe72ec3bf22f12d5a9ea60008bb8f1c#diff-f9f86df5970efdc2657c1233aac89bf599d22eba9a2e9360d5d402d67972c2c2L117' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: f81e27a5cbe72ec3bf22f12d5a9ea60008bb8f1c</div><div id='time'> Time: 2018-03-01</div><div id='author'> Author: markn@allenai.org</div><div id='file'> File Name: allennlp/modules/seq2seq_encoders/stacked_self_attention.py</div><div id='class'> Class Name: StackedSelfAttentionEncoder</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/59dca319d547dcb785cb7f6ef8e5230e09d5dc90#diff-ee67199b7f654721dcfb70d10dcc4610b4469583ef15826950f134b161149881L96' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 59dca319d547dcb785cb7f6ef8e5230e09d5dc90</div><div id='time'> Time: 2018-08-09</div><div id='author'> Author: vnoroozi@nvidia.com</div><div id='file'> File Name: open_seq2seq/parts/convs2s/ffn_wn_layer.py</div><div id='class'> Class Name: FeedFowardNetworkNormalized</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/59dca319d547dcb785cb7f6ef8e5230e09d5dc90#diff-cb73c026325d9499674477e99d2cabcf2326f12e5b235b0082f4682520ca4f88L119' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 59dca319d547dcb785cb7f6ef8e5230e09d5dc90</div><div id='time'> Time: 2018-08-09</div><div id='author'> Author: vnoroozi@nvidia.com</div><div id='file'> File Name: open_seq2seq/parts/convs2s/conv_wn_layer.py</div><div id='class'> Class Name: Conv1DNetworkNormalized</div><div id='method'> Method Name: call</div><BR>