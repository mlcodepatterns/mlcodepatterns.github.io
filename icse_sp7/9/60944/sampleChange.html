<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                   self.regularisation_coefficient_placeholder: self.regularisation_coefficient,
                                   }))
                + &quot, with unregularized cost &quot
                + str(<a id="change">self.tf_session.run(
                        self.loss_func,
                        feed_dict={self.input_placeholder: params,
                                   self.output_placeholder: [[c] for c in costs],
                                   self.regularisation_coefficient_placeholder: 0,
                                   })</a>))

    def cross_validation_loss(self, params, costs):
        &quot&quot&quot</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 - train for train_epochs epochs
        &#47&#47 - if the new loss is greater than the threshold then we haven&quott improved much, so stop
        &#47&#47 - else start from the top
        <a id="change">while True:
            threshold = 0.8 * self._loss(params, costs)[0]
            for i in range(self.train_epochs):
                &#47&#47 Split the data into random batches, and train on each batch
                indices = np.random.permutation(len(params))
                for j in range(math.ceil(len(params) / self.batch_size)):
                    batch_indices = indices[j * self.batch_size : (j + 1) * self.batch_size]
                    batch_input = [params[index] for index in batch_indices]
                    batch_output = [[costs[index]] for index in batch_indices]
                    self.tf_session.run(self.train_step,
                                        feed_dict={self.input_placeholder: batch_input,
                                                   self.output_placeholder: batch_output,
                                                   self.regularisation_coefficient_placeholder: self.regularisation_coefficient,
                                                   self.keep_prob_placeholder: self.keep_prob,
                                                   })
            (l, ul) = self._loss(params, costs)
            self.log.debug(&quotFit neural network with total training cost &quot + str(l)
                    + &quot, with unregularized cost &quot + str(ul))
            if l &gt; threshold:
                break
            self.log.debug(&quotCost decreased by a lot, train again&quot)

   </a> def cross_validation_loss(self, params, costs):
        &quot&quot&quot
        Returns the loss of the network on a cross validation set.
</code></pre>