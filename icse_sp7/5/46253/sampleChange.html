<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            if hidden_norms.data.max() &gt; max_norm:
                logger.info(f&quotclipping {hidden_norms.max()} to {max_norm}&quot)
                norm = hidden[hidden_norms &gt; max_norm].norm(dim=-1)
                <a id="change">norm = norm.unsqueeze(-1)</a>
                detached_norm = torch.autograd.Variable(norm.data,
                                                        requires_grad=False)
                hidden[hidden_norms &gt; max_norm] *= max_norm/detached_norm
</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 This workaround for PyTorch v0.3.1 does everything in numpy,
                &#47&#47 because the PyTorch slicing and slice assignment is too
                &#47&#47 flaky.
                hidden_norms = <a id="change">hidden_norms.data.cpu().numpy()</a>

                clipped_num += 1
                if hidden_norms.max() &gt; max_clipped_norm:
                    max_clipped_norm = hidden_norms.max()

                clip_select = hidden_norms &gt; max_norm
                clip_norms = hidden_norms[clip_select]

                mask = np.ones(hidden.size())
                <a id="change">normalizer = max_norm/clip_norms</a>
                normalizer = normalizer[:, np.newaxis]

                mask[clip_select] = normalizer
                hidden *= torch.autograd.Variable(</code></pre>