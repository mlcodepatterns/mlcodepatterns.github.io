<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                   &quotdistributed_replicated&quot, &quotindependent&quot],
                  &quot&quot&quotThe method for managing variables: parameter_server,
                  replicated, distributed_replicated, independent&quot&quot&quot)
flags.DEFINE_enum(&quottf_local_parameter_device&quot, &quotgpu&quot, <a id="change">[&quotcpu&quot, &quotgpu&quot]</a>,
                  &quot&quot&quotDevice to use as parameter server: cpu or gpu. For
                  distributed training, it can affect where caching of
                  variables happens.&quot&quot&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
                  &quot&quot&quotDevice to use as parameter server: cpu or gpu. For
                  distributed training, it can affect where caching of
                  variables happens.&quot&quot&quot)
<a id="change">flags.DEFINE_enum(&quottf_device&quot, GPU, [CPU, GPU],
                  &quotDevice to use for computation: cpu or gpu&quot)</a>
flags.DEFINE_enum(&quottf_data_format&quot, &quotNCHW&quot, [&quotNCHW&quot, &quotNHWC&quot], &quot&quot&quotData layout to
                  use: NHWC (TF native) or NCHW (cuDNN native).&quot&quot&quot)
flags.DEFINE_boolean(&quottf_use_nccl&quot, True,
                     &quotWhether to use nccl all-reduce primitives where possible&quot)</code></pre>