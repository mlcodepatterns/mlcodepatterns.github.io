<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if constant.LIMIT_MEMORY:
            config = tf.ConfigProto()
            config.gpu_options.allow_growth = True
            <a id="change">sess = tf.Session(config=config)</a>
            <a id="change">init = tf.global_variables_initializer()</a>
            <a id="change">sess.run(init)</a>
            backend.set_session(sess)
        x_test = x_test.astype(&quotfloat32&quot) / 255
        model = self.load_searcher().load_best_model().produce_model()
        return self.y_encoder.inverse_transform(model.predict(x_test, ))</code></pre><h3>After Change</h3><pre><code class='java'>
        model.eval()

        outputs = []
        <a id="change">with torch.no_grad():
            for index, inputs in enumerate(test_loader):
                outputs.append(model(inputs).numpy())
       </a> output = reduce(lambda x, y: np.concatenate((x, y)), outputs)
        return self.y_encoder.inverse_transform(output)

    def evaluate(self, x_test, y_test):</code></pre>