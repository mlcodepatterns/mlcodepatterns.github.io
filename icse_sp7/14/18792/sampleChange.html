<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            with tf.control_dependencies([optimizer.minimize(loss)]):
                return i + 1

        <a id="change">loop = tf.while_loop(lambda i: i &lt; self.ITERATIONS * self.EPOCHS, loop_body, (0,))</a>

        &#47&#47 return model parameters after training
        loop = tf.Print(loop, [], message="Training complete")
        with tf.control_dependencies([loop]):</code></pre><h3>After Change</h3><pre><code class='java'>
        params = [Wconv1, bconv1, Wconv2, bconv2, Wfc1, bfc1, Wfc2, bfc2]

        &#47&#47 optimizer and data pipeline
        optimizer = tf.train.AdamOptimizer(learning_rate=<a id="change">self.LEARNING_RATE</a>)

        &#47&#47 training loop
        def loop_body(i: tf.Tensor, max_iter: tf.Tensor, nb_epochs: tf.Tensor, avg_loss: tf.Tensor) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:

            &#47&#47 get next batch
            x, y = training_data.get_next()

            &#47&#47 model construction
            x = tf.reshape(x, [-1, self.IN_DIM, self.IN_DIM, 1])
            layer1 = pooling(tf.nn.relu(conv2d(x, Wconv1, self.STRIDE) + bconv1))
            layer2 = pooling(tf.nn.relu(conv2d(layer1, Wconv2, self.STRIDE) + bconv2))
            layer2 = tf.reshape(layer2, [-1, self.HIDDEN_FC1])
            layer3 = tf.nn.relu(tf.matmul(layer2, Wfc1) + bfc1)
            logits = tf.matmul(layer3, Wfc2) + bfc2

            loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=y))

            is_end_epoch = tf.equal(i % max_iter, 0)

            def true_fn() -&gt; tf.Tensor:
                return loss

            def false_fn() -&gt; tf.Tensor:
                return (tf.cast(i - 1, tf.float32) * avg_loss + loss) / tf.cast(i, tf.float32)

            with tf.control_dependencies([optimizer.minimize(loss)]):
                return i + 1, max_iter, nb_epochs, tf.cond(is_end_epoch, true_fn, false_fn)

        <a id="change">loop, _, _, _ = tf.while_loop(self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.])</a>

        &#47&#47 return model parameters after training
        loop = tf.Print(loop, [], message="Training complete")
        with tf.control_dependencies([loop]):</code></pre>