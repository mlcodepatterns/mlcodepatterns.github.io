<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        entry = {&quotimage_id&quot: data[&quotinfos&quot][k][&quotid&quot], &quotcaption&quot: sent}
                        n_predictions.append(entry)
            &#47&#47 case 2 sample_max =0 temperature xx
            elif <a id="change">sample_n_method == &quotsampl</a>e&quot:
                tmp_eval_kwargs.update({&quotsample_max&quot: 0, &quotbeam_size&quot: 1}) &#47&#47 randomness from sample
                with torch.no_grad():
                    <a id="change">_seq, _sampleLogprobs = model(fc_feats, att_feats, att_masks, opt=tmp_eval_kwargs, mode=&quotsample&quot)</a>
                _sents = utils.decode_sequence(loader.get_vocab(), _seq)
                <a id="change">for k, sent in enumerate(_sents):
                    entry = {&quotimage_id&quot: data[&quotinfos&quot][k // sample_n][&quotid&quot], &quotcaption&quot: sent}
                    n_predictions.append(entry)
            &#47&#47 case 3 gumbel max
           </a> elif <a id="change">sample_n_method</a> == &quotgumbel&quot:
                tmp_eval_kwargs.update({&quotsample_max&quot: 2, &quotbeam_size&quot: 1}) &#47&#47 randomness from sample
                with torch.no_grad():
                    _seq, _sampleLogprobs = model(fc_feats, att_feats, att_masks, opt=tmp_eval_kwargs, mode=&quotsample&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
                        entry = {&quotimage_id&quot: data[&quotinfos&quot][k][&quotid&quot], &quotcaption&quot: sent}
                        n_predictions.append(entry)
            &#47&#47 case 2 sample_max =0 temperature xx / gumbel / topk sampling
            elif <a id="change">sample_n_method</a> == &quotsample&quot or \
                 sample_n_method == &quotgumbel&quot or \
                 sample_n_method.startswith(&quottop&quot):
                <a id="change">if sample_n_method == &quotsample&quot:
                    tmp_sample_max = 0
                elif sample_n_method == &quotgumbel&quot:
                    tmp_sample_max = 2
                elif sample_n_method.startswith(&quottop&quot):
                    tmp_sample_max = -int(sample_n_method[3:])
               </a> tmp_eval_kwargs.update({&quotsample_max&quot: tmp_sample_max, &quotbeam_size&quot: 1}) &#47&#47 randomness from sample
                with torch.no_grad():
                    _seq, _sampleLogprobs = model(fc_feats, att_feats, att_masks, opt=tmp_eval_kwargs, mode=&quotsample&quot)
                _sents = utils.decode_sequence(loader.get_vocab(), _seq)</code></pre>