<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                            source=&quotAe&quot, target=&quotAi&quot)

        w = -self.inh * (torch.ones(self.n_neurons, self.n_neurons) - torch.diag(torch.ones(self.n_neurons)))
        self.add_connection(Connection(source=<a id="change">self.layers[&quotAi&quot]</a>, target=<a id="change">self.layers[&quotAe&quot]</a>, w=w, wmin=-self.inh, wmax=0),
                            source=&quotAi&quot, target=&quotAe&quot)

</code></pre><h3>After Change</h3><pre><code class='java'>
    &lt;https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full&gt;`_.
    

    def __init__(<a id="change">self</a>, n_inpt: int, n_neurons: int = 100, exc: float = 22.5, inh: float = 17.5, dt: float = 1.0,
                 nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2), wmin: float = 0.0, wmax: float = 1.0,
                 norm: float = 78.4, theta_plus: float = 0.05, tc_theta_decay: float = 1e7) -&gt; None:
        &#47&#47 language=rst
        
        Constructor for class ``DiehlAndCook2015``.

        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.
        :param n_neurons: Number of excitatory, inhibitory neurons.
        :param exc: Strength of synapse weights from excitatory to inhibitory layer.
        :param inh: Strength of synapse weights from inhibitory to excitatory layer.
        :param dt: Simulation time step.
        :param nu: Single or pair of learning rates for pre- and post-synaptic events, respectively.
        :param wmin: Minimum allowed weight on input to excitatory synapses.
        :param wmax: Maximum allowed weight on input to excitatory synapses.
        :param norm: Input to excitatory layer connection weights normalization constant.
        :param theta_plus: On-spike increment of ``DiehlAndCookNodes`` membrane threshold potential.
        :param tc_theta_decay: Time constant of ``DiehlAndCookNodes`` threshold potential decay.
        
        super().__init__(dt=dt)

        self.n_inpt = n_inpt
        self.n_neurons = n_neurons
        self.exc = exc
        self.inh = inh
        self.dt = dt

        &#47&#47 Layers
        <a id="change">input_layer = Input(n=self.n_inpt, traces=True, tc_trace=20.0)</a>
        exc_layer = DiehlAndCookNodes(
            n=self.n_neurons, traces=True, rest=-65.0, reset=-60.0, thresh=-52.0, refrac=5,
            tc_decay=100.0, tc_trace=20.0, theta_plus=theta_plus, tc_theta_decay=tc_theta_decay
        )
        inh_layer = LIFNodes(
            n=self.n_neurons, traces=False, rest=-60.0, reset=-45.0,
            thresh=-40.0, tc_decay=10.0, refrac=2, tc_trace=20.0
        )

        &#47&#47 Connections
        w = 0.3 * torch.rand(self.n_inpt, self.n_neurons)
        <a id="change">input_exc_conn = Connection(
            source=input_layer, target=exc_layer, w=w, update_rule=PostPre,
            nu=nu, wmin=wmin, wmax=wmax, norm=norm
        )</a>
        w = self.exc * torch.diag(torch.ones(self.n_neurons))
        exc_inh_conn = Connection(source=exc_layer, target=inh_layer, w=w, wmin=0, wmax=self.exc)
        w = -self.inh * (torch.ones(self.n_neurons, self.n_neurons) - torch.diag(torch.ones(self.n_neurons)))
        inh_exc_conn = Connection(source=inh_layer, target=exc_layer, w=w, wmin=-self.inh, wmax=0)

        &#47&#47 Add to network
        self.add_layer(input_layer, name=&quotX&quot)
        self.add_layer(exc_layer, name=&quotAe&quot)
        self.add_layer(inh_layer, name=&quotAi&quot)
        <a id="change">self.add_connection(input_exc_conn, source=&quotX&quot, target=&quotAe&quot)</a>
        self.add_connection(exc_inh_conn, source=&quotAe&quot, target=&quotAi&quot)
        self.add_connection(inh_exc_conn, source=&quotAi&quot, target=&quotAe&quot)

</code></pre>