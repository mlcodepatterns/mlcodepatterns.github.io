<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        :return: `None`
        
        &#47&#47 Check if train and output_ph available
        if <a id="change">self._train</a> is None or self._output_ph is None:
            raise ValueError("Need the training objective and the output placeholder to train the model.")

        num_batch = int(np.ceil(len(inputs) / batch_size))
        ind = np.arange(len(inputs))

        &#47&#47 Start training
        for _ in range(nb_epochs):
            &#47&#47 Shuffle the examples
            random.shuffle(ind)

            &#47&#47 Train for one epoch
            for m in range(num_batch):
                if m &lt; num_batch - 1:
                    i_batch = inputs[ind[m * batch_size:(m + 1) * batch_size]]
                    o_batch = outputs[ind[m * batch_size:(m + 1) * batch_size]]
                else:
                    i_batch = inputs[ind[m*batch_size:]]
                    o_batch = outputs[ind[m * batch_size:]]

                &#47&#47 Run train step
                if self._learning is None:
                    <a id="change">self</a>._sess.run(self._train, feed_dict={self._input_ph: i_batch, <a id="change">self._output_ph</a>: o_batch})
                else:
                    <a id="change">self._sess.run(self._train, feed_dict={self._input_ph: i_batch, self._output_ph: o_batch,
                                                           self._learning: True})</a>

    def class_gradient(self, inputs, logits=False):
        
        Compute per-class derivatives w.r.t. `input`.</code></pre><h3>After Change</h3><pre><code class='java'>

        return preds

    def fit(<a id="change">self</a>, inputs, outputs, batch_size=128, nb_epochs=10):
        
        Fit the classifier on the training set `(inputs, outputs)`.

        :param inputs: Training data.
        :type inputs: `np.ndarray`
        :param outputs: Labels.
        :type outputs: `np.ndarray`
        :param batch_size: Size of batches.
        :type batch_size: `int`
        :param nb_epochs: Number of epochs to use for trainings.
        :type nb_epochs: `int`
        :return: `None`
        
        &#47&#47 Set train phase
        self._model.train(True)

        &#47&#47 Apply defences
        inputs, outputs = self._apply_defences_fit(inputs, outputs)

        num_batch = int(np.ceil(len(inputs) / batch_size))
        ind = np.arange(len(inputs))

        &#47&#47 Start training
        for _ in range(nb_epochs):
            &#47&#47 Shuffle the examples
            random.shuffle(ind)

            &#47&#47 Train for one epoch
            for m in range(num_batch):
                if m &lt; num_batch - 1:
                    i_batch = inputs[ind[m * batch_size:(m + 1) * batch_size]]
                    o_batch = outputs[ind[m * batch_size:(m + 1) * batch_size]]
                else:
                    i_batch = inputs[ind[m*batch_size:]]
                    o_batch = outputs[ind[m * batch_size:]]

                &#47&#47 Zero the parameter gradients
                <a id="change">self._optimizer.zero_grad()</a>

                &#47&#47 Actual training
                m_batch = self._model(i_batch)
                loss = self._loss(m_batch, o_batch)
                loss.backward()
                <a id="change">self</a>._optimizer.step()

    def class_gradient(self, inputs, logits=False):
        </code></pre>