<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        with tf.variable_scope(name, initializer=initializer) as vs:
            &#47&#47 Creats the cell function
            &#47&#47 cell_instance_fn=lambda: cell_fn(num_units=n_hidden, **cell_init_args) &#47&#47 HanSheng
            <a id="change">self.fw_cell</a> = <a id="change">cell_fn(num_units=n_hidden, **cell_init_args)</a>
            <a id="change">self.bw_cell</a> = <a id="change">cell_fn(num_units=n_hidden, **cell_init_args)</a>

            &#47&#47 Apply dropout
            if dropout:
                if type(dropout) in [tuple, list]:
                    in_keep_prob = dropout[0]
                    out_keep_prob = dropout[1]
                elif isinstance(dropout, float):
                    in_keep_prob, out_keep_prob = dropout, dropout
                else:
                    raise Exception("Invalid dropout type (must be a 2-D tuple of "
                                    "float)")
                try:
                    DropoutWrapper_fn = tf.contrib.rnn.DropoutWrapper
                except:
                    DropoutWrapper_fn = tf.nn.rnn_cell.DropoutWrapper

                    &#47&#47 cell_instance_fn1=cell_instance_fn            &#47&#47 HanSheng
                    &#47&#47 cell_instance_fn=lambda: DropoutWrapper_fn(
                    &#47&#47                     cell_instance_fn1(),
                    &#47&#47                     input_keep_prob=in_keep_prob,
                    &#47&#47                     output_keep_prob=out_keep_prob)

                <a id="change">self.fw_cell = DropoutWrapper_fn(
                    self.fw_cell,
                    input_keep_prob=in_keep_prob,
                    output_keep_prob=out_keep_prob)</a>
                <a id="change">self.bw_cell = DropoutWrapper_fn(
                    self.bw_cell,
                    input_keep_prob=in_keep_prob,
                    output_keep_prob=out_keep_prob)</a>
            &#47&#47 Apply multiple layers
            if n_layer &gt; 1:
                try:
                    MultiRNNCell_fn = tf.contrib.rnn.MultiRNNCell
                except:
                    MultiRNNCell_fn = tf.nn.rnn_cell.MultiRNNCell

                &#47&#47 cell_instance_fn2=cell_instance_fn            &#47&#47 HanSheng
                &#47&#47 cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)])
                self.fw_cell = MultiRNNCell_fn(<a id="change">[self.fw_cell] * n_layer</a>)
                self.bw_cell = MultiRNNCell_fn(<a id="change">[self.bw_cell] * n_layer</a>)
            &#47&#47 self.fw_cell=cell_instance_fn()
            &#47&#47 self.bw_cell=cell_instance_fn()
            &#47&#47 Initial state of RNN</code></pre><h3>After Change</h3><pre><code class='java'>

                &#47&#47 cell_instance_fn2=cell_instance_fn            &#47&#47 HanSheng
                &#47&#47 cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)])
                self.fw_cell = <a id="change">MultiRNNCell_fn([cell_creator() for _ in range(n_layer)])</a>
                self.bw_cell = <a id="change">MultiRNNCell_fn([cell_creator() for _ in range(n_layer)])</a>
            &#47&#47 self.fw_cell=cell_instance_fn()
            &#47&#47 self.bw_cell=cell_instance_fn()
            &#47&#47 Initial state of RNN
            if fw_initial_state is None:</code></pre>