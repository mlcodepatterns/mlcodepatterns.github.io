<html><h3>7ba65487591fb4795d6384540bd4f580e820ff61,keras_gat/graph_attention_layer.py,GraphAttention,call,#GraphAttention#,78
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 Compute feature combinations
            repeated = K.reshape(K.tile(linear_transf_X, [1, N]), (N * N, self.F_))  &#47&#47 (N^2 x F&quot)
            tiled = K.tile(linear_transf_X, <a id="change">[N, 1]</a>)  &#47&#47 (N^2 x F&quot)
            combinations = K.concatenate([repeated, tiled])  &#47&#47 (N^2 x 2F&quot)
            <a id="change">combination_slices = K.reshape(combinations, (N, -1, 2 * self.F_))</a>  &#47&#47 (N x N x 2F&quot)

            &#47&#47 Attention head
            dense = K.squeeze(K.dot(combination_slices, attention_kernel), -1)  &#47&#47 a(Wh_i, Wh_j) in the paper (N x N x 1)</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 Compute feature combinations
            &#47&#47 Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]
            attn_for_self = <a id="change">K.dot(
                linear_transf_X, attention_kernel[0])</a>  &#47&#47 (N x 1), [a_1]^T [Wh_i]
            attn_for_neighs = K.dot(
                linear_transf_X, attention_kernel[1])  &#47&#47 (N x 1), [a_2]^T [Wh_j]

            &#47&#47 Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]
            <a id="change">dense = attn_for_self + K.transpose(attn_for_neighs)</a>  &#47&#47 (N x N) via broadcasting

            &#47&#47 add nonlinearty
            dense = LeakyReLU(alpha=0.2)(dense)</code></pre><img src="7717041.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/danielegrattarola/keras-gat/commit/7ba65487591fb4795d6384540bd4f580e820ff61#diff-a86aaaef47a1f935787908180be8993624c9842c988eb4d1c9053df8c50f476cL78' target='_blank'>Link</a></div><div id='project'> Project Name: danielegrattarola/keras-gat</div><div id='commit'> Commit Name: 7ba65487591fb4795d6384540bd4f580e820ff61</div><div id='time'> Time: 2018-03-14</div><div id='author'> Author: mwright@berkeley.edu</div><div id='file'> File Name: keras_gat/graph_attention_layer.py</div><div id='class'> Class Name: GraphAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/danielegrattarola/keras-gat/commit/9d56361641a64ff73ac630812ecd4964eedbc7aa#diff-b5eead7f2a637c33998de1add9a690c0678e849a3d16791260f0f0a16506c7c3L72' target='_blank'>Link</a></div><div id='project'> Project Name: danielegrattarola/keras-gat</div><div id='commit'> Commit Name: 9d56361641a64ff73ac630812ecd4964eedbc7aa</div><div id='time'> Time: 2017-11-09</div><div id='author'> Author: daniele.grattarola@gmail.com</div><div id='file'> File Name: gat/graph_attention_layer.py</div><div id='class'> Class Name: GraphAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/fabianp/mord/commit/ee143e803c2fa352d1c42c95335773639267f190#diff-2fb9a772e4ecbfd228293ca637c954244c95e295aaa3b1e84fbc8f31bfc777bbL58' target='_blank'>Link</a></div><div id='project'> Project Name: fabianp/mord</div><div id='commit'> Commit Name: ee143e803c2fa352d1c42c95335773639267f190</div><div id='time'> Time: 2015-07-14</div><div id='author'> Author: f@bianp.net</div><div id='file'> File Name: mord/threshold_based.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: grad_margin</div><BR>