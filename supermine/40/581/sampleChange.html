<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

def define_G(input_nc, output_nc, ngf, which_model_netG, norm=&quotbatch&quot, use_dropout=False, init_type=&quotnormal&quot, gpu_ids=[]):
    netG = None
    <a id="change">use_gpu = len(gpu_ids) &gt; 0</a><a id="change">
    n</a>orm_layer = get_norm_layer(norm_type=norm)

    if which_model_netG == &quotresnet_9blocks&quot:
        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)
    elif which_model_netG == &quotresnet_6blocks&quot:
        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)
    elif which_model_netG == &quotunet_128&quot:
        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif which_model_netG == &quotunet_256&quot:
        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    else:
        raise NotImplementedError(&quotGenerator model name [%s] is not recognized&quot % which_model_netG)
    <a id="change">if use_gpu:
        assert(torch.cuda.is_available())
        netG.cuda(gpu_ids[0])
        if len(gpu_ids) &gt; 1:
            netG = torch.nn.DataParallel(netG, gpu_ids)
   </a> <a id="change">init_weights(netG, init_type=init_type)</a>
    <a id="change">return netG</a>


def define_D(input_nc, ndf, which_model_netD,
             n_layers_D=3, norm=&quotbatch&quot, use_sigmoid=False, init_type=&quotnormal&quot, gpu_ids=[]):</code></pre><h3>After Change</h3><pre><code class='java'>


def define_G(input_nc, output_nc, ngf, which_model_netG, norm=&quotbatch&quot, use_dropout=False, init_type=&quotnormal&quot, gpu_ids=[]):
    <a id="change">netG</a> = None
    norm_layer = get_norm_layer(norm_type=norm)

    if which_model_netG == &quotresnet_9blocks&quot:
        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)
    elif which_model_netG == &quotresnet_6blocks&quot:
        <a id="change">netG</a> = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)
    elif which_model_netG == &quotunet_128&quot:
        <a id="change">netG</a> = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    elif which_model_netG == &quotunet_256&quot:
        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)
    else:
        raise NotImplementedError(&quotGenerator model name [%s] is not recognized&quot % which_model_netG)
    <a id="change">return init_net(netG, init_type, gpu_ids)</a>


def define_D(input_nc, ndf, which_model_netD,
             n_layers_D=3, norm=&quotbatch&quot, use_sigmoid=False, init_type=&quotnormal&quot, gpu_ids=[]):</code></pre>