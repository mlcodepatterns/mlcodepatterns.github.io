<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        data_time.update(time.time() - end)

        input_var = torch.autograd.Variable(inputs.cuda())
        <a id="change">target_var = torch.autograd.Variable(target.cuda(async=True)</a>)

        &#47&#47 compute output
        output = model(input_var)
        score_map = output[-1].data.cpu()

        loss = criterion(output[0], target_var)
        for j in range(1, len(output)):
            loss += criterion(output[j], target_var)
        acc = accuracy(score_map, target, idx)

        if debug: &#47&#47 visualize groundtruth and predictions
            gt_batch_img = batch_with_heatmap(inputs, target)
            pred_batch_img = batch_with_heatmap(inputs, score_map)
            if not gt_win or not pred_win:
                ax1 = plt.subplot(121)
                ax1.title.set_text(&quotGroundtruth&quot)
                gt_win = plt.imshow(gt_batch_img)
                ax2 = plt.subplot(122)
                ax2.title.set_text(&quotPrediction&quot)
                pred_win = plt.imshow(pred_batch_img)
            else:
                gt_win.set_data(gt_batch_img)
                pred_win.set_data(pred_batch_img)
            plt.pause(.05)
            plt.draw()

        &#47&#47 measure accuracy and record loss
        losses.update(<a id="change">loss</a>.data[0], inputs.size(0))
        acces.update(acc[0], inputs.size(0))

        &#47&#47 compute gradient and do SGD step</code></pre><h3>After Change</h3><pre><code class='java'>

        if debug: &#47&#47 visualize groundtruth and predictions
            gt_batch_img = batch_with_heatmap(input, target)
            pred_batch_img = batch_with_heatmap(input, <a id="change">output[0]</a>)
            if not gt_win or not pred_win:
                ax1 = plt.subplot(121)
                ax1.title.set_text(&quotGroundtruth&quot)</code></pre>