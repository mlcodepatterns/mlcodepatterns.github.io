<html><h3>1b45e3a304f9d2479a836b82d889f0e19183faf6,src/plugin.py,EmbeddingRegularizer,__init__,#EmbeddingRegularizer#,8
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &quot&quot&quot Perform word embedding regularization training for ASR&quot&quot&quot
    def __init__(self, tokenizer, dec_dim, enable, src, distance, weight, fuse, temperature, 
                 freeze=True, fuse_normalize=False, dropout=0.0):
        <a id="change">super().__init__()</a>
        self.enable = enable
        if enable:
            pretrained_emb = torch.FloatTensor(load_embedding(tokenizer,src))
            &#47&#47pretrained_emb = nn.functional.normalize(pretrained_emb,dim=-1) &#47&#47 ToDo : Check impact on old version</code></pre><h3>After Change</h3><pre><code class='java'>
            vocab_size, emb_dim = pretrained_emb.shape
            self.dim = emb_dim

            <a id="change">if bert is not None:
                if not isinstance(bert, (tuple, list)) or len(bert) != 2:
                    raise ValueError("`bert` should be a tuple/list of config and fine-tuned model "
                                     "such as (\"bert-base-uncased\", \"fine-tuned-model.pth\").")
                self.emb_table = BertEmbeddingPredictor(
                    bert[0], text_encoder, bert[1])
            else:
                self.emb_table = nn.Embedding.from_pretrained(
                    pretrained_emb, freeze=freeze, padding_idx=0)

           </a> self.emb_net = nn.Sequential(nn.Linear(dec_dim, (emb_dim+dec_dim)//2),
                                         nn.ReLU(),
                                         nn.Linear((emb_dim+dec_dim)//2, emb_dim))
            self.weight = weight</code></pre><img src="23164631.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch/commit/1b45e3a304f9d2479a836b82d889f0e19183faf6#diff-834f0679bc61d0427df924d74c20767d5b8fa62227d824361dd81555df6f6725L10' target='_blank'>Link</a></div><div id='project'> Project Name: Alexander-H-Liu/End-to-end-ASR-Pytorch</div><div id='commit'> Commit Name: 1b45e3a304f9d2479a836b82d889f0e19183faf6</div><div id='time'> Time: 2019-10-13</div><div id='author'> Author: windqaq@gmail.com</div><div id='file'> File Name: src/plugin.py</div><div id='class'> Class Name: EmbeddingRegularizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/rtqichen/torchdiffeq/commit/b914816142ae2776f531be1c0b49812a0bfde91f#diff-0356630894916625ff67eb26bd8d244ef62c211371a7e2fd3c1e0c71f190b66fL74' target='_blank'>Link</a></div><div id='project'> Project Name: rtqichen/torchdiffeq</div><div id='commit'> Commit Name: b914816142ae2776f531be1c0b49812a0bfde91f</div><div id='time'> Time: 2020-08-04</div><div id='author'> Author: 33688385+patrick-kidger@users.noreply.github.com</div><div id='file'> File Name: torchdiffeq/_impl/adams.py</div><div id='class'> Class Name: VariableCoefficientAdamsBashforth</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/GPflow/GPflow/commit/f31a914bd38b00affc978d7ef1834e1303af5b4c#diff-7460b2b9b0f96f35d0814cca388a2a99ebcf0d032efcf674cd1b3cdba8448bc2L135' target='_blank'>Link</a></div><div id='project'> Project Name: GPflow/GPflow</div><div id='commit'> Commit Name: f31a914bd38b00affc978d7ef1834e1303af5b4c</div><div id='time'> Time: 2018-09-20</div><div id='author'> Author: art.art.v@gmail.com</div><div id='file'> File Name: gpflow/models/model.py</div><div id='class'> Class Name: GPModel</div><div id='method'> Method Name: __init__</div><BR>