<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                self.l_gp_w = train_opt[&quotgp_weigth&quot]

            &#47&#47 optimizers
            <a id="change">self.optimizers = []</a>  &#47&#47 G and D
            &#47&#47 G
            wd_G = train_opt[&quotweight_decay_G&quot] if train_opt[&quotweight_decay_G&quot] else 0
            optim_params = []
            for k, v in self.netG.named_parameters():  &#47&#47 can optimize for a part of the model
                if v.requires_grad:
                    optim_params.append(v)
                else:
                    print(&quotWARNING: params [%s] will not optimize.&quot % k)
            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt[&quotlr_G&quot], \
                weight_decay=wd_G, betas=(train_opt[&quotbeta1_G&quot], 0.999))
            self.optimizers.append(self.optimizer_G)
            &#47&#47 D
            wd_D = train_opt[&quotweight_decay_D&quot] if train_opt[&quotweight_decay_D&quot] else 0
            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=train_opt[&quotlr_D&quot], \
                weight_decay=wd_D, betas=(train_opt[&quotbeta1_D&quot], 0.999))
            self.optimizers.append(self.optimizer_D)

            &#47&#47 schedulers
            <a id="change">self.schedulers = []</a>
            if train_opt[&quotlr_scheme&quot] == &quotMultiStepLR&quot:
                for optimizer in self.optimizers:
                    self.schedulers.append(lr_scheduler.MultiStepLR(optimizer, \
                        train_opt[&quotlr_steps&quot], train_opt[&quotlr_gamma&quot]))</code></pre><h3>After Change</h3><pre><code class='java'>
                if v.requires_grad:
                    optim_params.append(v)
                else:
                    <a id="change">print(&quotWARNING: params [{:s}] will not optimize.&quot.format(k))</a>
            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt[&quotlr_G&quot], \
                weight_decay=wd_G, betas=(train_opt[&quotbeta1_G&quot], 0.999))
            self.optimizers.append(self.optimizer_G)
            &#47&#47 D</code></pre>