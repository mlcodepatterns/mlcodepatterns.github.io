<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                else:
                    inputs, labels = Variable(inputs), Variable(labels)
                &#47&#47 if we use low precision, input also need to be fp16
                <a id="change">if fp16:
                    inputs = inputs.half()
 
                &#47&#47 zero the parameter gradients
               </a> optimizer.zero_grad()

                &#47&#47 forward
                if phase == &quotval&quot:
                    with torch.no_grad():
                        outputs = model(inputs)
                else:
                    outputs = model(inputs)

                if not opt.PCB:
                    _, preds = torch.max(outputs.data, 1)
                    loss = criterion(outputs, labels)
                else:
                    part = {}
                    sm = nn.Softmax(dim=1)
                    num_part = 6
                    for i in range(num_part):
                        part[i] = outputs[i]

                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])
                    _, preds = torch.max(score.data, 1)

                    loss = criterion(part[0], labels)
                    for i in range(num_part-1):
                        loss += criterion(part[i+1], labels)

                &#47&#47 backward + optimize only if in training phase
                if phase == &quottrain&quot:
                    if fp16: &#47&#47 we use optimier to backward loss
                        <a id="change">optimizer.backward(loss)</a>
                    else:
                        loss.backward()
                    optimizer.step()
</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 backward + optimize only if in training phase
                if phase == &quottrain&quot:
                    if fp16: &#47&#47 we use optimier to backward loss
                        <a id="change">with amp.scale_loss(loss, optimizer) as scaled_loss:
                            scaled_loss.backward()
                   </a> else:
                        loss.backward()
                    optimizer.step()
</code></pre>