<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        if self.refit:
            self.regr_ = [clone(clf) for clf in self.regressors]
            <a id="change">self.meta_regr_</a> = clone(self.meta_regressor)
        else:
            self.regr_ = self.regressors
            <a id="change">self.meta_regr_</a> = self.meta_regressor

        kfold = check_cv(self.cv, y)
        if isinstance(self.cv, int):
            &#47&#47 Override shuffle parameter in case of self generated
            &#47&#47 cross-validation strategy
            kfold.shuffle = self.shuffle

        meta_features = np.zeros((X.shape[0], len(self.regressors)))

        &#47&#47
        &#47&#47 The outer loop iterates over the base-regressors. Each regressor
        &#47&#47 is trained cv times and makes predictions, after which we train
        &#47&#47 the meta-regressor on their combined results.
        &#47&#47
        for i, regr in enumerate(self.regressors):
            &#47&#47
            &#47&#47 In the inner loop, each model is trained cv times on the
            &#47&#47 training-part of this fold of data; and the holdout-part of data
            &#47&#47 is used for predictions. This is repeated cv times, so in
            &#47&#47 the end we have predictions for each data point.
            &#47&#47
            &#47&#47 Advantage of this complex approach is that data points we&quotre
            &#47&#47 predicting have not been trained on by the algorithm, so it&quots
            &#47&#47 less susceptible to overfitting.
            &#47&#47
            for train_idx, holdout_idx in kfold.split(X, y, groups):
                instance = clone(regr)
                instance.fit(X[train_idx], y[train_idx])
                y_pred = instance.predict(X[holdout_idx])
                meta_features[holdout_idx, i] = y_pred

        &#47&#47 save meta-features for training data
        if self.store_train_meta_features:
            self.train_meta_features_ = meta_features

        &#47&#47 Train meta-model on the out-of-fold predictions
        if not self.use_features_in_secondary:
            <a id="change">self.meta_regr_.fit(meta_features, y)</a>
        elif sparse.issparse(X):
            self.meta_regr_.fit(sparse.hstack((X, meta_features)), y)
        else:
            self.meta_regr_.fit(np.hstack((X, meta_features)), y)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47
            for train_idx, holdout_idx in kfold.split(X, y, groups):
                instance = clone(regr)
                <a id="change">if sample_weight is None:
                    instance.fit(X[train_idx], y[train_idx])
                else:
                    instance.fit(X[train_idx], y[train_idx],
                                 sample_weight=sample_weight[train_idx])
               </a> y_pred = instance.predict(X[holdout_idx])
                meta_features[holdout_idx, i] = y_pred

        &#47&#47 save meta-features for training data</code></pre>