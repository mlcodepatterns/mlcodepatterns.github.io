<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            return [i-1, b, outputs]

        cond = lambda i, b, inputs_hat: i &gt; 0
        <a id="change">loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]</a>
        <a id="change">_, self.bias, outputs = tf.while_loop(cond, body, loop_vars)</a>

        
        &#47&#47 Routing algorithm V2. Seems not right. This may duplicate tensors by self.num_routing times.
        for _ in range(self.num_routing):</code></pre><h3>After Change</h3><pre><code class='java'>
        _, self.bias, outputs = tf.while_loop(cond, body, loop_vars)

        &#47&#47 Routing algorithm V2. Use for iteration. V2 and V1 both work without much difference on performance
        <a id="change">for _ in range(self.num_routing):
            c = K.softmax(self.bias)
            c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)
            outputs = K.sum(c_expand * inputs_hat, 1, keepdims=True)
            outputs = squash(outputs)
            self.bias = K.update(self.bias, self.bias + K.sum(inputs_hat * outputs, [0, -2, -1]))

        &#47&#47 Handling with no routing scenario. Prior bias will always be zero.
       </a> if self.num_routing == 0:
            c = K.softmax(self.bias)
            c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)
            outputs = squash(K.sum(c_expand * inputs_hat, 1, keepdims=True))</code></pre>