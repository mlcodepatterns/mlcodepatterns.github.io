<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for _ in range(self.num_routing):
            c = K.softmax(self.bias)
            c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)
            outputs = <a id="change">K.sum(c_expand * inputs_hat, 1, keepdims=True)</a>
            outputs = squash(outputs)
            self.bias = K.update(self.bias, self.bias + K.sum(inputs_hat * outputs, [0, -2, -1]))

        &#47&#47 Handling with no routing scenario. Prior bias will always be zero.
        if self.num_routing == 0:
            c = K.softmax(self.bias)
            <a id="change">c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)</a>
            outputs = squash(K.sum(c_expand * inputs_hat, 1, keepdims=True))

        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])
</code></pre><h3>After Change</h3><pre><code class='java'>
        _, self.bias, outputs = tf.while_loop(cond, body, loop_vars)

        &#47&#47 Routing algorithm V2. Use for iteration. V2 and V1 both work without much difference on performance
        <a id="change">assert self.num_routing &gt; 0, &quotThe num_routing should be &gt; 0.&quot</a>
        for i in range(self.num_routing):
            c = tf.nn.softmax(self.bias, dim=2)  &#47&#47 dim=2 is the num_capsule dimension
            &#47&#47 outputs.shape=[None, 1, num_capsule, 1, dim_vector]
            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))</code></pre>