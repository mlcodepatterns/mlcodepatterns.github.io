<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            tb.log_value("qvals_mean", batches_sum_q0 / batches_count, step=idx)
            batches_count = 0
            <a id="change">batches_sum_total_reward = </a>batches_sum_q0 = 0.0

            tb.log_value("loss", np.mean(losses), step=idx)
            tb.log_value("epsilon", action_selector.epsilon, step=idx)</code></pre><h3>After Change</h3><pre><code class='java'>
    use_target_dqn = run.getboolean("dqn", "target_dqn", fallback=False)
    use_double_dqn = run.getboolean("dqn", "double_dqn", fallback=False)

    <a id="change">if not use_target_dqn and not use_double_dqn:
        preprocessor = experience.QLearningPreprocessor.simple_dqn(model)
    elif use_target_dqn:
        preprocessor = experience.QLearningPreprocessor.target_dqn(model, target_net.target_model)
    elif use_target_dqn and use_double_dqn:
        preprocessor = experience.QLearningPreprocessor.double_dqn(model, target_net.target_model)
    else:
        raise RuntimeError("Wrong combination of target/double DQN parameters")

    &#47&#47 running sums of batch values
    &#47&#47 batches_count = 0
    &#47&#47 batches_sum_q0 = 0.0
    &#47&#47
    &#47&#47 def batch_to_train(batch):
    &#47&#47     
    &#47&#47     Convert batch into training data using bellman&quots equation
    &#47&#47     :param batch: list of tuples with Experience instances
    &#47&#47     :return:
    &#47&#47     
    &#47&#47     v0_data = []
    &#47&#47     vL_data = []
    &#47&#47
    &#47&#47     for exps in batch:
    &#47&#47         v0_data.append(exps[0].state)
    &#47&#47         vL_data.append(exps[-1].state)
    &#47&#47
    &#47&#47     states_t = torch.from_numpy(np.array(v0_data, dtype=np.float32))
    &#47&#47     v0 = Variable(states_t)
    &#47&#47     vL = Variable(torch.from_numpy(np.array(vL_data, dtype=np.float32)))
    &#47&#47     if params.cuda_enabled:
    &#47&#47         v0 = v0.cuda()
    &#47&#47         vL = vL.cuda()
    &#47&#47
    &#47&#47     q0 = model(v0).data
    &#47&#47
    &#47&#47     global batches_count, batches_sum_q0
    &#47&#47     batches_count += 1
    &#47&#47     batches_sum_q0 += q0.mean()
    &#47&#47
    &#47&#47     if use_target_dqn and use_double_dqn:
    &#47&#47         qL = model(vL)
    &#47&#47         actions = qL.data.cpu().max(1)[1].squeeze().numpy()
    &#47&#47         qL = target_model(vL).data.cpu().numpy()
    &#47&#47         total_rewards = qL[range(qL.shape[0]), actions]
    &#47&#47     &#47&#47 only target is in use: use best value from it
    &#47&#47     elif use_target_dqn:
    &#47&#47         q = target_model(vL)
    &#47&#47         total_rewards = q.data.max(1)[0].squeeze().cpu().numpy()
    &#47&#47     else:
    &#47&#47         q = model(vL)
    &#47&#47         total_rewards = q.data.max(1)[0].squeeze().cpu().numpy()
    &#47&#47     for idx, exps in enumerate(batch):
    &#47&#47         &#47&#47 game is done, no final reward
    &#47&#47         if exps[-1].done:
    &#47&#47             total_reward = 0.0
    &#47&#47         else:
    &#47&#47             total_reward = total_rewards[idx]
    &#47&#47         for exp in reversed(exps[:-1]):
    &#47&#47             total_reward = exp.reward + GAMMA * total_reward
    &#47&#47         q0[idx][exps[0].action] = total_reward
    &#47&#47     return states_t, q0

   </a> reward_sma = utils.SMAQueue(run.getint("stop", "mean_games", fallback=100))
    speed_mon = utils.SpeedMonitor(run.getint("learning", "batch_size"))

    try:</code></pre>