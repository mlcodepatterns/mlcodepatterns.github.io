<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    with tf.variable_scope(scope, reuse=reuse):
        &#47&#47 Set the fall back option for num_units
        if num_units is None:
            num_units = <a id="change">queries</a>.get_shape().as_list[-1]

        &#47&#47 Linear projections
        <a id="change">Q</a> = tf.layers.dense(queries, num_units, activation=tf.nn.relu) &#47&#47 (N, T_q, C)
        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) &#47&#47 (N, T_k, C)
        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) &#47&#47 (N, T_k, C)

        &#47&#47 Split and concat
        <a id="change">Q_</a> = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_q, C/h)
        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_k, C/h)
        <a id="change">V_</a> = tf.concat(tf.split(V, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_k, C/h)

        &#47&#47 Multiplication
        <a id="change">outputs</a> = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Scale
        <a id="change">outputs</a> = outputs / (K_.get_shape().as_list()[-1] ** 0.5)

        &#47&#47 Key Masking
        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) &#47&#47 (N, T_k)
        key_masks = tf.tile(key_masks, [num_heads, 1]) &#47&#47 (h*N, T_k)
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) &#47&#47 (h*N, T_q, T_k)

        <a id="change">paddings</a> = tf.ones_like(outputs)*(-2**32+1)
        <a id="change">outputs</a> = tf.where(tf.equal(key_masks, 0), paddings, outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Causality = Future blinding
        if causality:
            <a id="change">diag_vals</a> = tf.ones_like(outputs[0, :, :]) &#47&#47 (T_q, T_k)
            <a id="change">tril</a> = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() &#47&#47 (T_q, T_k)
            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) &#47&#47 (h*N, T_q, T_k)

            paddings = tf.ones_like(masks)*(-2**32+1)
            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Activation
        outputs = tf.nn.softmax(outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Query Masking
        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) &#47&#47 (N, T_q)
        query_masks = tf.tile(query_masks, [num_heads, 1]) &#47&#47 (h*N, T_q)
        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) &#47&#47 (h*N, T_q, T_k)
        outputs *= query_masks &#47&#47 broadcasting. (N, T_q, C)

        &#47&#47 Dropouts
        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=context.is_train())

        &#47&#47 Weighted sum
        outputs = tf.matmul(outputs, V_) &#47&#47 ( h*N, T_q, C/h)

        &#47&#47 Restore shape
        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) &#47&#47 (N, T_q, C)

        &#47&#47 Residual connection
        outputs += queries

        &#47&#47 Normalize
        <a id="change">outputs</a> = normalize(outputs) &#47&#47 (N, T_q, C)

    return outputs
</code></pre><h3>After Change</h3><pre><code class='java'>
    &quot&quot&quot
    with tf.variable_scope(scope):
        if num_units is None:
            num_units = <a id="change">queries</a>.get_shape().as_list()[-1]

        <a id="change">Q</a> = tf.layers.dense(queries, num_units, activation=tf.nn.relu)
        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)
        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)

        <a id="change">Q_</a> = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)
        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)
        <a id="change">V_</a> = tf.concat(tf.split(V, num_heads, axis=2), axis=0)

        <a id="change">outputs</a> = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))

        <a id="change">outputs</a> = outputs / (K_.get_shape().as_list()[-1] ** 0.5)

        &#47&#47not sure why there should be key_masks and query_masks
        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))
        key_masks = tf.tile(key_masks, [num_heads, 1])
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])

        <a id="change">paddings</a> = tf.ones_like(outputs)*(-2**32+1)
        <a id="change">outputs</a> = tf.where(tf.equal(key_masks, 0), paddings, outputs)

        if causality:
            <a id="change">diag_vals</a> = tf.ones_like(outputs[0, :, :])
            <a id="change">tril</a> = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()
            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])

            paddings = tf.ones_like(masks)*(-2**32+1)
            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)

        outputs = tf.nn.softmax(outputs)

        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))
        query_masks = tf.tile(query_masks, [num_heads, 1])
        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])
        outputs *= query_masks

        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=context.is_train())

        &#47&#47 Weighted sum
        outputs = tf.matmul(outputs, V_)
        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 )
        &#47&#47(batch_size, length_query, attention_size)

        &#47&#47residual connection
        if num_units == queries.get_shape().as_list()[-1]:
            outputs += queries

        <a id="change">outputs</a> = normalize(outputs)

    return outputs
</code></pre>