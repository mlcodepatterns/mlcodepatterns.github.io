<html><h3>4287aef6a3a82436b4e3e156b22ede235eb4e6ba,texar/core/layers.py,,multihead_attention,#,949
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    with tf.variable_scope(scope, reuse=reuse):
        &#47&#47 Set the fall back option for num_units
        if num_units is None:
            num_units = <a id="change">queries</a>.get_shape().as_list[-1]

        &#47&#47 Linear projections
        <a id="change">Q</a> = tf.layers.dense(queries, num_units, activation=tf.nn.relu) &#47&#47 (N, T_q, C)
        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) &#47&#47 (N, T_k, C)
        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) &#47&#47 (N, T_k, C)

        &#47&#47 Split and concat
        <a id="change">Q_</a> = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_q, C/h)
        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_k, C/h)
        <a id="change">V_</a> = tf.concat(tf.split(V, num_heads, axis=2), axis=0) &#47&#47 (h*N, T_k, C/h)

        &#47&#47 Multiplication
        <a id="change">outputs</a> = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Scale
        <a id="change">outputs</a> = outputs / (K_.get_shape().as_list()[-1] ** 0.5)

        &#47&#47 Key Masking
        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) &#47&#47 (N, T_k)
        key_masks = tf.tile(key_masks, [num_heads, 1]) &#47&#47 (h*N, T_k)
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) &#47&#47 (h*N, T_q, T_k)

        <a id="change">paddings</a> = tf.ones_like(outputs)*(-2**32+1)
        <a id="change">outputs</a> = tf.where(tf.equal(key_masks, 0), paddings, outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Causality = Future blinding
        if causality:
            <a id="change">diag_vals</a> = tf.ones_like(outputs[0, :, :]) &#47&#47 (T_q, T_k)
            <a id="change">tril</a> = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() &#47&#47 (T_q, T_k)
            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) &#47&#47 (h*N, T_q, T_k)

            paddings = tf.ones_like(masks)*(-2**32+1)
            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Activation
        outputs = tf.nn.softmax(outputs) &#47&#47 (h*N, T_q, T_k)

        &#47&#47 Query Masking
        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) &#47&#47 (N, T_q)
        query_masks = tf.tile(query_masks, [num_heads, 1]) &#47&#47 (h*N, T_q)
        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) &#47&#47 (h*N, T_q, T_k)
        outputs *= query_masks &#47&#47 broadcasting. (N, T_q, C)

        &#47&#47 Dropouts
        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=context.is_train())

        &#47&#47 Weighted sum
        outputs = tf.matmul(outputs, V_) &#47&#47 ( h*N, T_q, C/h)

        &#47&#47 Restore shape
        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) &#47&#47 (N, T_q, C)

        &#47&#47 Residual connection
        outputs += queries

        &#47&#47 Normalize
        <a id="change">outputs</a> = normalize(outputs) &#47&#47 (N, T_q, C)

    return outputs
</code></pre><h3>After Change</h3><pre><code class='java'>
    &quot&quot&quot
    with tf.variable_scope(scope):
        if num_units is None:
            num_units = <a id="change">queries</a>.get_shape().as_list()[-1]

        <a id="change">Q</a> = tf.layers.dense(queries, num_units, activation=tf.nn.relu)
        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)
        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)

        <a id="change">Q_</a> = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)
        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)
        <a id="change">V_</a> = tf.concat(tf.split(V, num_heads, axis=2), axis=0)

        <a id="change">outputs</a> = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))

        <a id="change">outputs</a> = outputs / (K_.get_shape().as_list()[-1] ** 0.5)

        &#47&#47not sure why there should be key_masks and query_masks
        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))
        key_masks = tf.tile(key_masks, [num_heads, 1])
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])

        <a id="change">paddings</a> = tf.ones_like(outputs)*(-2**32+1)
        <a id="change">outputs</a> = tf.where(tf.equal(key_masks, 0), paddings, outputs)

        if causality:
            <a id="change">diag_vals</a> = tf.ones_like(outputs[0, :, :])
            <a id="change">tril</a> = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()
            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])

            paddings = tf.ones_like(masks)*(-2**32+1)
            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)

        outputs = tf.nn.softmax(outputs)

        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))
        query_masks = tf.tile(query_masks, [num_heads, 1])
        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])
        outputs *= query_masks

        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=context.is_train())

        &#47&#47 Weighted sum
        outputs = tf.matmul(outputs, V_)
        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 )
        &#47&#47(batch_size, length_query, attention_size)

        &#47&#47residual connection
        if num_units == queries.get_shape().as_list()[-1]:
            outputs += queries

        <a id="change">outputs</a> = normalize(outputs)

    return outputs
</code></pre><img src="136194.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/asyml/texar/commit/4287aef6a3a82436b4e3e156b22ede235eb4e6ba#diff-d67ef9e22f99bc267a53d5d5a857d7a7fd25afeaf78430ee64f5ad3f3b7905ecL972' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: 4287aef6a3a82436b4e3e156b22ede235eb4e6ba</div><div id='time'> Time: 2017-12-11</div><div id='author'> Author: shore@pku.edu.cn</div><div id='file'> File Name: texar/core/layers.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: multihead_attention</div><BR><BR><div id='link'><a href='https://github.com/tflearn/tflearn/commit/ca3ebe149946fbbc56f1e5f275880aa898e706e8#diff-d03e3753a426ce9387fb9ce87f7ea43cfa3620b8a4095966c61e97df2009f4e7L186' target='_blank'>Link</a></div><div id='project'> Project Name: tflearn/tflearn</div><div id='commit'> Commit Name: ca3ebe149946fbbc56f1e5f275880aa898e706e8</div><div id='time'> Time: 2016-10-01</div><div id='author'> Author: simanek@spaceknow.com</div><div id='file'> File Name: tflearn/objectives.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: weak_cross_entropy_2d</div><BR><BR><div id='link'><a href='https://github.com/asyml/texar/commit/c8523b2dc735a1b82ca6170e6ca349defe9f77fc#diff-2c390e268cf05b35eaddc4fff3c05dc4a6279ebdca6fb84a4fd9fb158ebe4897L490' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: c8523b2dc735a1b82ca6170e6ca349defe9f77fc</div><div id='time'> Time: 2017-11-17</div><div id='author'> Author: shore@pku.edu.cn</div><div id='file'> File Name: txtgen/core/layers.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: multihead_attention</div><BR>