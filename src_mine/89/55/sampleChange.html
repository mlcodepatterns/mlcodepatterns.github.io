<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if scaled:
        if query_head_units is None:
            query_shape = npx.shape_array(query)
            <a id="change">scale</a> = np.sqrt(query_shape[<a id="change">-1</a>])
        else:
            scale = math.sqrt(query_head_units)
    else:
        scale = None
    if layout == &quotNKT&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            mask = np.expand_dims(mask, axis=1)
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (B, N, L_query, C_Q) X (B, N, L_mem, C_Q) --&gt; (B, N, L_query, L_mem)
        scores = npx.batch_dot(query, key, transpose_b=True)
        if edge_scores is not None:
            scores = scores + edge_scores
        if scaled:
            scores = scores / scale
        <a id="change">attn_weights</a> = masked_softmax(scores, mask, dtype=dtype, axis=-1)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (B, N, L_mem, C_V) --&gt; (B, L_query, N * C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,bnjc-&gt;binc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights, value).transpose((0, 2, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    elif layout == &quotNTK&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            mask = np.expand_dims(mask, axis=1)
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (B, L_query, N, C_Q) X (B, L_mem, N, C_Q) --&gt; (B, N, L_query, L_mem)
        if use_einsum:
            scores = np.einsum(&quotbinc,bjnc-&gt;bnij&quot, query, key)
        else:
            scores = npx.batch_dot(np.swapaxes(query, 1, 2), np.swapaxes(key, 1, 2),
                                     transpose_b=True)
        if edge_scores is not None:
            scores = scores + edge_scores
        if scaled:
            scores = scores / scale
        <a id="change">attn_weights</a> = masked_softmax(scores, mask, dtype=dtype)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (B, L_mem, N, C_V) --&gt; (B, L_query, N * C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,bjnc-&gt;binc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights,
                                          np.swapaxes(value, 1, 2)).transpose((0, 2, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    elif layout == &quotTNK&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            <a id="change">mask</a> = <a id="change">np.expand_dims(mask, axis=1)</a>
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (L_query, B, N, C_Q) X (L_mem, B, N, C_Q) --&gt; (B, N, L_query, L_mem)
        &#47&#47   This layout structure can be implemented very efficiently because B, N are consecutive
        &#47&#47   to each other. To have a clear picture of what&quots happening, we may consider the
        &#47&#47   (i, j)th element of the output
        &#47&#47       out[i, j, :, :] = query[:, i, j, :] X key[:, i, j, :].T, which is just one GEMM call
        &#47&#47   We can thus implement the whole kernel via a single call of batched GEMM with stride.
        if use_einsum:
            scores = np.einsum(&quotibnc,jbnc-&gt;bnij&quot, query, key)
        else:
            scores = npx.batch_dot(query.transpose((1, 2, 0, 3)),
                                     key.transpose((1, 2, 3, 0)))
        if edge_scores is not None:
            scores = scores + edge_scores
        <a id="change">if scaled:
            scores = scores / scale
       </a> <a id="change">attn_weights</a> = masked_softmax(scores, mask, dtype=dtype)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (L_mem, B, N, C_V) --&gt; (L_query, B, N * C_V)
        &#47&#47 Again, we can implement it via a single call to batched GEMM with stride.

        &#47&#47 Shape (B, N, L_query, C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,jbnc-&gt;ibnc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights,
                                          value.transpose((1, 2, 0, 3))).transpose((2, 0, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    else:
        raise NotImplementedError(&quotlayout="{}" is not supported! &quot
                                  &quotWe only support layout = "NKT", "NTK", and "TNK".&quot</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            scale = math.sqrt(query_head_units)
    else:
        <a id="change">scale</a> = None
    if layout == &quotNKT&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            mask = np.expand_dims(mask, axis=1).astype(np.bool)
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (B, N, L_query, C_Q) X (B, N, L_mem, C_Q) --&gt; (B, N, L_query, L_mem)
        scores = npx.batch_dot(query, key, transpose_b=True)
        if edge_scores is not None:
            scores = scores + edge_scores
        <a id="change">attn_weights</a> = masked_softmax(scores, mask, axis=-1, temperature=scale)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (B, N, L_mem, C_V) --&gt; (B, L_query, N * C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,bnjc-&gt;binc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights, value).transpose((0, 2, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    elif layout == &quotNTK&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            mask = np.expand_dims(mask, axis=1).astype(np.bool)
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (B, L_query, N, C_Q) X (B, L_mem, N, C_Q) --&gt; (B, N, L_query, L_mem)
        if use_einsum:
            scores = np.einsum(&quotbinc,bjnc-&gt;bnij&quot, query, key)
        else:
            scores = npx.batch_dot(np.swapaxes(query, 1, 2), np.swapaxes(key, 1, 2),
                                   transpose_b=True)
        if edge_scores is not None:
            scores = scores + edge_scores
        <a id="change">attn_weights</a> = masked_softmax(scores, mask, axis=<a id="change">-1</a>, temperature=scale)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (B, L_mem, N, C_V) --&gt; (B, L_query, N * C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,bjnc-&gt;binc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights,
                                          np.swapaxes(value, 1, 2)).transpose((0, 2, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    elif layout == &quotTNK&quot:
        &#47&#47 1. Expand the dimension of the mask:
        &#47&#47   (B, L_query, L_mem) --&gt; (B, 1, L_query, L_mem)
        if mask is not None:
            <a id="change">mask</a> = <a id="change">np.expand_dims(mask, axis=1).astype(np.bool)</a>
        &#47&#47 2. Calculate the attention weights
        &#47&#47   Score: (L_query, B, N, C_Q) X (L_mem, B, N, C_Q) --&gt; (B, N, L_query, L_mem)
        &#47&#47   This layout structure can be implemented very efficiently because B, N are consecutive
        &#47&#47   to each other. To have a clear picture of what&quots happening, we may consider the
        &#47&#47   (i, j)th element of the output
        &#47&#47       out[i, j, :, :] = query[:, i, j, :] X key[:, i, j, :].T, which is just one GEMM call
        &#47&#47   We can thus implement the whole kernel via a single call of batched GEMM with stride.
        if use_einsum:
            scores = np.einsum(&quotibnc,jbnc-&gt;bnij&quot, query, key)
        else:
            scores = npx.batch_dot(query.transpose((1, 2, 0, 3)),
                                     key.transpose((1, 2, 3, 0)))
        if edge_scores is not None:
            scores = scores + edge_scores
        <a id="change">attn_weights</a> = masked_softmax(scores, mask, axis=<a id="change">-1</a>, temperature=scale)
        <a id="change">attn_weights</a> = npx.dropout(attn_weights, p=dropout)
        &#47&#47 3. Calculate the context vector
        &#47&#47 (B, N, L_query, L_mem) X (L_mem, B, N, C_V) --&gt; (L_query, B, N * C_V)
        &#47&#47 Again, we can implement it via a single call to batched GEMM with stride.

        &#47&#47 Shape (B, N, L_query, C_V)
        if use_einsum:
            <a id="change">context_vec</a> = np.einsum(&quotbnij,jbnc-&gt;ibnc&quot, attn_weights, value)
        else:
            <a id="change">context_vec</a> = npx.batch_dot(attn_weights,
                                          value.transpose((1, 2, 0, 3))).transpose((2, 0, 1, 3))
        <a id="change">context_vec</a> = npx.reshape(context_vec, (-2, -2, -1))
    else:
        raise NotImplementedError(&quotlayout="{}" is not supported! &quot
                                  &quotWe only support layout = "NKT", "NTK", and "TNK".&quot</code></pre>