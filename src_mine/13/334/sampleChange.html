<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

base_model = Text2Text

<a id="change">base_params</a> = {
  "use_horovod": False,
  "num_gpus": 4,
  "max_steps": 340000,
  "batch_size_per_gpu": 32,
  "save_summaries_steps": 50,
  "print_loss_steps": 48,
  "print_samples_steps": 48,
  "eval_steps": 1000,
  "save_checkpoint_steps": 2001,
  "logdir": "GNMT-Adam-LR0.0008-FP32-4x32-MP-luong10-P8-AAT",
  "optimizer": "Adam",
  "optimizer_params": {},
  &#47&#47 luong10 decay scheme
  "lr_policy": exp_decay,
  "lr_policy_params": {
    "learning_rate": 0.0008,
    "begin_decay_at": 170000,
    "decay_steps": 17000,
    "decay_rate": 0.5,
    "use_staircase_decay": True,
    "min_lr": 0.0000005,
  },
  &#47&#47 "summaries": [&quotlearning_rate&quot, &quotvariables&quot, &quotgradients&quot, &quotlarc_summaries&quot,
  &#47&#47               &quotvariable_norm&quot, &quotgradient_norm&quot, &quotglobal_gradient_norm&quot],
  "max_grad_norm": 32768.0,
  "dtype": tf.float32,
  &#47&#47 "dtype": "mixed",
  &#47&#47 "loss_scaling": "Backoff",
  "encoder": GNMTLikeEncoderWithEmbedding,
  "encoder_params": {
    <a id="change">"initializer"</a>: tf.random_uniform_initializer,
    "initializer_params": {
      "minval": -0.1,
      "maxval": 0.1,</code></pre><h3>After Change</h3><pre><code class='java'>

base_model = Text2Text

<a id="change">base_params</a> = {
  "use_horovod": False,
  "num_gpus": 4,
  "max_steps": 340000,
  "batch_size_per_gpu": 32,
  "save_summaries_steps": 50,
  "print_loss_steps": 48,
  "print_samples_steps": 48,
  "eval_steps": 1000,
  "save_checkpoint_steps": 2001,
  "logdir": "GNMT-Adam-LR0.0008-FP32-4x32-MP-luong10-P8-AAT",
  "optimizer": "Adam",
  "optimizer_params": {},
  &#47&#47 luong10 decay scheme
  "lr_policy": exp_decay,
  "lr_policy_params": {
    "learning_rate": 0.0008,
    "begin_decay_at": 170000,
    "decay_steps": 17000,
    "decay_rate": 0.5,
    "use_staircase_decay": True,
    "min_lr": 0.0000005,
  },
  &#47&#47 "summaries": [&quotlearning_rate&quot, &quotvariables&quot, &quotgradients&quot, &quotlarc_summaries&quot,
  &#47&#47               &quotvariable_norm&quot, &quotgradient_norm&quot, &quotglobal_gradient_norm&quot],
  "max_grad_norm": 32768.0,
  "dtype": tf.float32,
  &#47&#47 "dtype": "mixed",
  &#47&#47 "loss_scaling": "Backoff",
  "encoder": GNMTLikeEncoderWithEmbedding,
  "encoder_params": {
    "initializer": tf.random_uniform_initializer,
    "initializer_params": {
      "minval": -0.1,
      "maxval": 0.1,
    },
    &#47&#47"encoder_cell_type": "lstm",
    &#47&#47"encoder_cell_units": 1024,
    <a id="change">"core_cell"</a>: tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell,
    "core_cell_params": <a id="change">{
      "num_units": 1024,
      &#47&#47 "forget_bias": 1.0,
    }</a>,
    "encoder_layers": 7,
    "encoder_dp_input_keep_prob": 0.8,
    "encoder_dp_output_keep_prob": 1.0,
    "encoder_use_skip_connections": True,
    "src_emb_size": 1024,
  },

  "decoder": RNNDecoderWithAttention,
  "decoder_params": {
    <a id="change">"initializer"</a>: tf.random_uniform_initializer,
    "initializer_params": {
       "minval": -0.1,
       "maxval": 0.1,
     },
    &#47&#47"decoder_cell_type": "lstm",
    &#47&#47"decoder_cell_units": 1024,
    "core_cell": tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell,
    "core_cell_params": <a id="change">{
      "num_units": 1024,
      &#47&#47 "forget_bias": 1.0,
    }</a>,
    "decoder_layers": 8,
    "decoder_dp_input_keep_prob": 0.8,
    "decoder_dp_output_keep_prob": 1.0,</code></pre>