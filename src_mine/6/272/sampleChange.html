<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if isinstance(self.scheme, MiddlewareScheme):
            self.layers_params = copy.copy(self.schemes[self.scheme])
        else:
            self.layers_params = <a id="change">copy.copy(self.scheme)</a>

        &#47&#47 we allow adding batchnorm, dropout or activation functions after each layer.
        &#47&#47 The motivation is to simplify the transition between a network with batchnorm and a network without
        &#47&#47 batchnorm to a single flag (the same applies to activation function and dropout)</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            &#47&#47 if scheme is specified directly, convert to TF layer if it&quots not a callable object
            &#47&#47 NOTE: if layer object is callable, it must return a TF tensor when invoked
            self.layers_params = [convert_layer(l) for l in <a id="change">copy.copy(self.scheme)</a>]

        &#47&#47 we allow adding batchnorm, dropout or activation functions after each layer.
        &#47&#47 The motivation is to simplify the transition between a network with batchnorm and a network without</code></pre>