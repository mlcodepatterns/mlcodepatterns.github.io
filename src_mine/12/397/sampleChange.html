<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        rewards = np.array(rewards)[:trajectory_len]

        values = torch.zeros((states.shape[0] + 1, 1)).to(self._device)
        values[:states.shape[0]] = <a id="change">self.critic(states)</a>
        values = values.cpu().numpy().reshape(-1)[:trajectory_len+1]

        _, logprobs = self.actor(states, logprob=actions)
        logprobs = logprobs.cpu().numpy().reshape(-1)[:trajectory_len]

        <a id="change">deltas</a> = rewards + self.gamma * values[1:] - values[:-1]
        advantages = geometric_cumsum(self.gamma, deltas)[0]
        returns = geometric_cumsum(self.gamma * self.gae_lambda, rewards)[0]

        <a id="change">rollout</a> = {
            "return": returns,
            "value": values[:trajectory_len],
            "advantage": advantages,</code></pre><h3>After Change</h3><pre><code class='java'>
        }

    @torch.no_grad()
    def get_rollout(<a id="change">self</a>, states, actions, rewards, dones):
        trajectory_len = \
            rewards.shape[0] if dones[-1] else rewards.shape[0] - 1

        states = self._to_tensor(states)
        actions = self._to_tensor(actions)
        rewards = np.array(rewards)[:trajectory_len]

        values = torch.zeros((states.shape[0] + 1, self._num_heads)).\
            to(self._device)
        values[:states.shape[0], :] = <a id="change">self</a>.critic(states).squeeze(-1)
        &#47&#47 Each column corresponds to a different gamma
        values = values.cpu().numpy()[:trajectory_len+1, :]
        _, logprobs = self.actor(states, logprob=actions)
        logprobs = logprobs.cpu().numpy().reshape(-1)[:trajectory_len]
        <a id="change">deltas</a> = rewards[:, None] + self._gammas * values[1:] - values[:-1]
        &#47&#47 len x num_heads

        &#47&#47 For each gamma in the list of gammas compute the
        &#47&#47 advantage and returns
        advantages = np.stack([
            geometric_cumsum(gamma, deltas[:, i])[0]
            for i, gamma in enumerate(self._gammas)
        ], axis=1)  &#47&#47 len x num_heads
        returns = np.stack([
            geometric_cumsum(gamma * self.gae_lambda, rewards)[0]
            for gamma in self._gammas
        ], axis=1)  &#47&#47 len x num_heads

        <a id="change">rollout</a> = {
            "return": returns,
            "value": values[:trajectory_len],
            "advantage": advantages,</code></pre>