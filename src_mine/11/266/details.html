<html><h3>566e089f21fd004943e35652d39ed7becbdfe1ad,open_seq2seq/models/text2speech.py,Text2Speech,maybe_print_logs,#Text2Speech#,203
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      save_audio(predicted_final_spectrogram_sample, self.params["logdir"], step)
    elif "mel" in self.get_data_layer().params[&quotoutput_type&quot]:
      predicted_final_spectrogram_sample = inverse_mel(predicted_final_spectrogram_sample)
      <a id="change">save_audio(predicted_final_spectrogram_sample, self.params["logdir"], step)</a>
    
    return {}

  def finalize_evaluation(self, results_per_batch, step):</code></pre><h3>After Change</h3><pre><code class='java'>
      return loss, [decoder_out, final_spectrogram, attention_mask, target_output, final_sequence_lengths]

  def maybe_print_logs(self, input_values, output_values):
    <a id="change">self.train_steps</a> += 1
    dict_to_log = {}
    <a id="change">step</a> = <a id="change">self.train_steps * self.params["print_samples_steps"]</a>
    spec, target, _ = input_values[&quottarget_tensors&quot]
    predicted_decoder_spectrograms = output_values[0]
    predicted_final_spectrograms = output_values[1]
    attention_mask = output_values[2]
    target_output = output_values[3]
    y_sample = spec[0]
    target = target[0]
    &#47&#47 y_length_sample = y_length[0]
    predicted_spectrogram_sample = predicted_decoder_spectrograms[0]
    predicted_final_spectrogram_sample = predicted_final_spectrograms[0]
    attention_mask_sample = attention_mask[0]
    target_output_sample = target_output[0]
    audio_length = output_values[4][0]

    im_summary = plot_spectrogram_w_target(y_sample, predicted_spectrogram_sample,
                     predicted_final_spectrogram_sample,
                     attention_mask_sample,
                     target_output_sample,
                     target,
                     audio_length,
                     self.params["logdir"], step,
                     append="train", 
                     save_to_tensorboard = self.save_to_tensorboard)
    dict_to_log[&quotimage&quot] = im_summary

    predicted_final_spectrogram_sample = predicted_final_spectrogram_sample[:audio_length-1,:]
    if "magnitude" in self.get_data_layer().params[&quotoutput_type&quot]:
      predicted_final_spectrogram_sample = np.exp(predicted_final_spectrogram_sample)
      wav_summary = save_audio(predicted_final_spectrogram_sample, self.params["logdir"], step,
        save_to_tensorboard = self.save_to_tensorboard)
      dict_to_log[&quotaudio&quot] = wav_summary
    elif "mel" in self.get_data_layer().params[&quotoutput_type&quot]:
      predicted_final_spectrogram_sample = inverse_mel(predicted_final_spectrogram_sample)
      <a id="change">wav_summary</a> = <a id="change">save_audio(predicted_final_spectrogram_sample, self.params["logdir"], step,
        save_to_tensorboard = self.save_to_tensorboard)</a>
      <a id="change">dict_to_log[&quotaudio&quot]</a> = <a id="change">wav_summary</a>
    
    if self.save_to_tensorboard:
      return dict_to_log
    else:</code></pre><img src="706152.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/566e089f21fd004943e35652d39ed7becbdfe1ad#diff-fa5a5d968f70899ad0c7f6d00c3467c8f24a7d56f626a78a090a69a37aaa6f13L218' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 566e089f21fd004943e35652d39ed7becbdfe1ad</div><div id='time'> Time: 2018-06-21</div><div id='author'> Author: jasoli@nvidia.com</div><div id='file'> File Name: open_seq2seq/models/text2speech.py</div><div id='class'> Class Name: Text2Speech</div><div id='method'> Method Name: maybe_print_logs</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/566e089f21fd004943e35652d39ed7becbdfe1ad#diff-fa5a5d968f70899ad0c7f6d00c3467c8f24a7d56f626a78a090a69a37aaa6f13L249' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 566e089f21fd004943e35652d39ed7becbdfe1ad</div><div id='time'> Time: 2018-06-21</div><div id='author'> Author: jasoli@nvidia.com</div><div id='file'> File Name: open_seq2seq/models/text2speech.py</div><div id='class'> Class Name: Text2Speech</div><div id='method'> Method Name: finalize_evaluation</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/566e089f21fd004943e35652d39ed7becbdfe1ad#diff-fa5a5d968f70899ad0c7f6d00c3467c8f24a7d56f626a78a090a69a37aaa6f13L218' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 566e089f21fd004943e35652d39ed7becbdfe1ad</div><div id='time'> Time: 2018-06-21</div><div id='author'> Author: jasoli@nvidia.com</div><div id='file'> File Name: open_seq2seq/models/text2speech.py</div><div id='class'> Class Name: Text2Speech</div><div id='method'> Method Name: maybe_print_logs</div><BR>