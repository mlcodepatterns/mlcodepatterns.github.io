<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 user did not specify a meaningful sequence length
    &#47&#47 override the sequence length by the maximum seq length of the current batch
    if max_seq_length is None:
        max_seq_length = min(<a id="change">max(len(ta) + len(tb) for ta, tb in all_tokens)</a> + 3,
                             max_position_embeddings)  &#47&#47 3 account for maximum 3 special symbols
        logger.warning(&quot"max_seq_length" is undefined, &quot
                       &quotand bert config json defines "max_position_embeddings"=%d. &quot
                       &quothence set "max_seq_length"=%d according to the current batch.&quot % (
                           max_position_embeddings, max_seq_length))

    for (tokens_a, tokens_b) in all_tokens:
        if tokens_b:
            &#47&#47 Modifies `tokens_a` and `tokens_b` in place so that the total
            &#47&#47 length is less than the specified length.
            &#47&#47 Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            &#47&#47 Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) &gt; max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)]

        &#47&#47 The convention in BERT is:
        &#47&#47 (a) For sequence pairs:
        &#47&#47  tokens:   [CLS] is this jack &#47&#47&#47&#47son &#47&#47&#47&#47ville ? [SEP] no it is not . [SEP]
        &#47&#47  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
        &#47&#47 (b) For single sequences:
        &#47&#47  tokens:   [CLS] the dog is hairy . [SEP]
        &#47&#47  type_ids: 0     0   0   0  0     0 0
        &#47&#47
        &#47&#47 Where "type_ids" are used to indicate whether this is the first
        &#47&#47 sequence or the second sequence. The embedding vectors for `type=0` and
        &#47&#47 `type=1` were learned during pre-training and are added to the wordpiece
        &#47&#47 embedding vector (and position vector). This is not *strictly* necessary
        &#47&#47 since the [SEP] token unambiguously separates the sequences, but it makes
        &#47&#47 it easier for the model to learn the concept of sequences.
        &#47&#47
        &#47&#47 For classification tasks, the first vector (corresponding to [CLS]) is
        &#47&#47 used as as the "sentence vector". Note that this only makes sense because
        &#47&#47 the entire model is fine-tuned.
        tokens = [&quot[CLS]&quot] + tokens_a + [&quot[SEP]&quot]
        input_type_ids = [0] * len(tokens)
        input_mask = [int(not mask_cls_sep)] + [1] * len(tokens_a) + [int(not mask_cls_sep)]

        if tokens_b:
            tokens += tokens_b + [&quot[SEP]&quot]
            input_type_ids += [1] * (len(tokens_b) + 1)
            input_mask += [1] * len(tokens_b) + [int(not mask_cls_sep)]

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        &#47&#47 Zero-pad up to the sequence length. more pythonic
        <a id="change">pad_len</a> = max_seq_length - len(input_ids)
        <a id="change">input_ids</a> += [0] * pad_len
        input_mask += [0] * pad_len
        input_type_ids += [0] * pad_len

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(input_type_ids) == max_seq_length

        logger.debug(&quottokens: %s&quot % &quot &quot.join([tokenization.printable_text(x) for x in tokens]))
        logger.debug(&quotinput_ids: %s&quot % &quot &quot.join([str(x) for <a id="change">x</a> in input_ids]))
        logger.debug(&quotinput_mask: %s&quot % &quot &quot.join([str(x) for x in input_mask]))
        logger.debug(&quotinput_type_ids: %s&quot % &quot &quot.join([str(x) for x in input_type_ids]))
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 user did not specify a meaningful sequence length
    &#47&#47 override the sequence length by the maximum seq length of the current batch
    if max_seq_length is None:
        max_seq_length = <a id="change">max(len(ta) + len(tb) for ta, tb in all_tokens)</a>
        &#47&#47 add special tokens into account
        &#47&#47 case 1: Account for [CLS], tokens_a [SEP], tokens_b [SEP] -&gt; 3 additional tokens
        &#47&#47 case 2: Account for [CLS], tokens_a [SEP] -&gt; 2 additional tokens
        max_seq_length += 3 if any(len(tb) for _, tb in all_tokens) else 2
        max_seq_length = min(max_seq_length, max_position_embeddings)
        logger.warning(&quot"max_seq_length" is undefined, &quot
                       &quotand bert config json defines "max_position_embeddings"=%d. &quot
                       &quothence set "max_seq_length"=%d according to the current batch.&quot % (
                           max_position_embeddings, max_seq_length))

    for (tokens_a, tokens_b) in all_tokens:
        if tokens_b:
            &#47&#47 Modifies `tokens_a` and `tokens_b` in place so that the total
            &#47&#47 length is less than the specified length.
            &#47&#47 Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            &#47&#47 Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) &gt; max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)]

        &#47&#47 The convention in BERT is:
        &#47&#47 (a) For sequence pairs:
        &#47&#47  tokens:   [CLS] is this jack &#47&#47&#47&#47son &#47&#47&#47&#47ville ? [SEP] no it is not . [SEP]
        &#47&#47  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
        &#47&#47 (b) For single sequences:
        &#47&#47  tokens:   [CLS] the dog is hairy . [SEP]
        &#47&#47  type_ids: 0     0   0   0  0     0 0
        &#47&#47
        &#47&#47 Where "type_ids" are used to indicate whether this is the first
        &#47&#47 sequence or the second sequence. The embedding vectors for `type=0` and
        &#47&#47 `type=1` were learned during pre-training and are added to the wordpiece
        &#47&#47 embedding vector (and position vector). This is not *strictly* necessary
        &#47&#47 since the [SEP] token unambiguously separates the sequences, but it makes
        &#47&#47 it easier for the model to learn the concept of sequences.
        &#47&#47
        &#47&#47 For classification tasks, the first vector (corresponding to [CLS]) is
        &#47&#47 used as as the "sentence vector". Note that this only makes sense because
        &#47&#47 the entire model is fine-tuned.
        tokens = [&quot[CLS]&quot] + tokens_a + [&quot[SEP]&quot]
        input_type_ids = [0] * len(tokens)
        input_mask = [int(not mask_cls_sep)] + [1] * len(tokens_a) + [int(not mask_cls_sep)]

        if tokens_b:
            tokens += tokens_b + [&quot[SEP]&quot]
            input_type_ids += [1] * (len(tokens_b) + 1)
            input_mask += [1] * len(tokens_b) + [int(not mask_cls_sep)]

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        &#47&#47 Zero-pad up to the sequence length. more pythonic
        <a id="change">pad_len</a> = max_seq_length - len(input_ids)
        <a id="change">input_ids</a> += [0] * pad_len
        input_mask += [0] * pad_len
        input_type_ids += [0] * pad_len

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(input_type_ids) == max_seq_length

        logger.debug(&quottokens: %s&quot % &quot &quot.join([tokenization.printable_text(x) for x in tokens]))
        logger.debug(&quotinput_ids: %s&quot % &quot &quot.join([str(x) for <a id="change">x</a> in input_ids]))
        logger.debug(&quotinput_mask: %s&quot % &quot &quot.join([str(x) for x in input_mask]))
        logger.debug(&quotinput_type_ids: %s&quot % &quot &quot.join([str(x) for x in input_type_ids]))
</code></pre>