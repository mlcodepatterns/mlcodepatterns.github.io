<html><h3>752f3796546cd4aef6e5f330634623af1b8e2710,server/bert_serving/server/bert/extract_features.py,,convert_lst_to_features,#,41
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 user did not specify a meaningful sequence length
    &#47&#47 override the sequence length by the maximum seq length of the current batch
    if max_seq_length is None:
        max_seq_length = min(<a id="change">max(len(ta) + len(tb) for ta, tb in all_tokens)</a> + 3,
                             max_position_embeddings)  &#47&#47 3 account for maximum 3 special symbols
        logger.warning(&quot"max_seq_length" is undefined, &quot
                       &quotand bert config json defines "max_position_embeddings"=%d. &quot
                       &quothence set "max_seq_length"=%d according to the current batch.&quot % (
                           max_position_embeddings, max_seq_length))

    for (tokens_a, tokens_b) in all_tokens:
        if tokens_b:
            &#47&#47 Modifies `tokens_a` and `tokens_b` in place so that the total
            &#47&#47 length is less than the specified length.
            &#47&#47 Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            &#47&#47 Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) &gt; max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)]

        &#47&#47 The convention in BERT is:
        &#47&#47 (a) For sequence pairs:
        &#47&#47  tokens:   [CLS] is this jack &#47&#47&#47&#47son &#47&#47&#47&#47ville ? [SEP] no it is not . [SEP]
        &#47&#47  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
        &#47&#47 (b) For single sequences:
        &#47&#47  tokens:   [CLS] the dog is hairy . [SEP]
        &#47&#47  type_ids: 0     0   0   0  0     0 0
        &#47&#47
        &#47&#47 Where "type_ids" are used to indicate whether this is the first
        &#47&#47 sequence or the second sequence. The embedding vectors for `type=0` and
        &#47&#47 `type=1` were learned during pre-training and are added to the wordpiece
        &#47&#47 embedding vector (and position vector). This is not *strictly* necessary
        &#47&#47 since the [SEP] token unambiguously separates the sequences, but it makes
        &#47&#47 it easier for the model to learn the concept of sequences.
        &#47&#47
        &#47&#47 For classification tasks, the first vector (corresponding to [CLS]) is
        &#47&#47 used as as the "sentence vector". Note that this only makes sense because
        &#47&#47 the entire model is fine-tuned.
        tokens = [&quot[CLS]&quot] + tokens_a + [&quot[SEP]&quot]
        input_type_ids = [0] * len(tokens)
        input_mask = [int(not mask_cls_sep)] + [1] * len(tokens_a) + [int(not mask_cls_sep)]

        if tokens_b:
            tokens += tokens_b + [&quot[SEP]&quot]
            input_type_ids += [1] * (len(tokens_b) + 1)
            input_mask += [1] * len(tokens_b) + [int(not mask_cls_sep)]

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        &#47&#47 Zero-pad up to the sequence length. more pythonic
        <a id="change">pad_len</a> = max_seq_length - len(input_ids)
        <a id="change">input_ids</a> += [0] * pad_len
        input_mask += [0] * pad_len
        input_type_ids += [0] * pad_len

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(input_type_ids) == max_seq_length

        logger.debug(&quottokens: %s&quot % &quot &quot.join([tokenization.printable_text(x) for x in tokens]))
        logger.debug(&quotinput_ids: %s&quot % &quot &quot.join([str(x) for <a id="change">x</a> in input_ids]))
        logger.debug(&quotinput_mask: %s&quot % &quot &quot.join([str(x) for x in input_mask]))
        logger.debug(&quotinput_type_ids: %s&quot % &quot &quot.join([str(x) for x in input_type_ids]))
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 user did not specify a meaningful sequence length
    &#47&#47 override the sequence length by the maximum seq length of the current batch
    if max_seq_length is None:
        max_seq_length = <a id="change">max(len(ta) + len(tb) for ta, tb in all_tokens)</a>
        &#47&#47 add special tokens into account
        &#47&#47 case 1: Account for [CLS], tokens_a [SEP], tokens_b [SEP] -&gt; 3 additional tokens
        &#47&#47 case 2: Account for [CLS], tokens_a [SEP] -&gt; 2 additional tokens
        max_seq_length += 3 if any(len(tb) for _, tb in all_tokens) else 2
        max_seq_length = min(max_seq_length, max_position_embeddings)
        logger.warning(&quot"max_seq_length" is undefined, &quot
                       &quotand bert config json defines "max_position_embeddings"=%d. &quot
                       &quothence set "max_seq_length"=%d according to the current batch.&quot % (
                           max_position_embeddings, max_seq_length))

    for (tokens_a, tokens_b) in all_tokens:
        if tokens_b:
            &#47&#47 Modifies `tokens_a` and `tokens_b` in place so that the total
            &#47&#47 length is less than the specified length.
            &#47&#47 Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            &#47&#47 Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) &gt; max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)]

        &#47&#47 The convention in BERT is:
        &#47&#47 (a) For sequence pairs:
        &#47&#47  tokens:   [CLS] is this jack &#47&#47&#47&#47son &#47&#47&#47&#47ville ? [SEP] no it is not . [SEP]
        &#47&#47  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
        &#47&#47 (b) For single sequences:
        &#47&#47  tokens:   [CLS] the dog is hairy . [SEP]
        &#47&#47  type_ids: 0     0   0   0  0     0 0
        &#47&#47
        &#47&#47 Where "type_ids" are used to indicate whether this is the first
        &#47&#47 sequence or the second sequence. The embedding vectors for `type=0` and
        &#47&#47 `type=1` were learned during pre-training and are added to the wordpiece
        &#47&#47 embedding vector (and position vector). This is not *strictly* necessary
        &#47&#47 since the [SEP] token unambiguously separates the sequences, but it makes
        &#47&#47 it easier for the model to learn the concept of sequences.
        &#47&#47
        &#47&#47 For classification tasks, the first vector (corresponding to [CLS]) is
        &#47&#47 used as as the "sentence vector". Note that this only makes sense because
        &#47&#47 the entire model is fine-tuned.
        tokens = [&quot[CLS]&quot] + tokens_a + [&quot[SEP]&quot]
        input_type_ids = [0] * len(tokens)
        input_mask = [int(not mask_cls_sep)] + [1] * len(tokens_a) + [int(not mask_cls_sep)]

        if tokens_b:
            tokens += tokens_b + [&quot[SEP]&quot]
            input_type_ids += [1] * (len(tokens_b) + 1)
            input_mask += [1] * len(tokens_b) + [int(not mask_cls_sep)]

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        &#47&#47 Zero-pad up to the sequence length. more pythonic
        <a id="change">pad_len</a> = max_seq_length - len(input_ids)
        <a id="change">input_ids</a> += [0] * pad_len
        input_mask += [0] * pad_len
        input_type_ids += [0] * pad_len

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(input_type_ids) == max_seq_length

        logger.debug(&quottokens: %s&quot % &quot &quot.join([tokenization.printable_text(x) for x in tokens]))
        logger.debug(&quotinput_ids: %s&quot % &quot &quot.join([str(x) for <a id="change">x</a> in input_ids]))
        logger.debug(&quotinput_mask: %s&quot % &quot &quot.join([str(x) for x in input_mask]))
        logger.debug(&quotinput_type_ids: %s&quot % &quot &quot.join([str(x) for x in input_type_ids]))
</code></pre><img src="1073424.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/hanxiao/bert-as-service/commit/752f3796546cd4aef6e5f330634623af1b8e2710#diff-c38e28907f0c6735ffe31e5c0eb1fab517bcfd322dd48d5dbedf0f737aed4702L49' target='_blank'>Link</a></div><div id='project'> Project Name: hanxiao/bert-as-service</div><div id='commit'> Commit Name: 752f3796546cd4aef6e5f330634623af1b8e2710</div><div id='time'> Time: 2019-02-13</div><div id='author'> Author: hanhxiao@tencent.com</div><div id='file'> File Name: server/bert_serving/server/bert/extract_features.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: convert_lst_to_features</div><BR><BR><div id='link'><a href='https://github.com/chainer/chainercv/commit/b21b8b41eee0776833d39c5bcbf3a299494ffb4f#diff-285fa2a680c67ed3355b20d7926b41e6bf64f8acc7c8023ffca2eecf5ddfc285L355' target='_blank'>Link</a></div><div id='project'> Project Name: chainer/chainercv</div><div id='commit'> Commit Name: b21b8b41eee0776833d39c5bcbf3a299494ffb4f</div><div id='time'> Time: 2018-06-19</div><div id='author'> Author: shingogo@hotmail.co.jp</div><div id='file'> File Name: chainercv/experimental/links/model/fcis/fcis_resnet101.py</div><div id='class'> Class Name: FCISResNet101Head</div><div id='method'> Method Name: _pool</div><BR><BR><div id='link'><a href='https://github.com/chainer/chainercv/commit/8d99e10342647b4c9e63176fc316d51ab6033804#diff-fca8f61493b10ae12eed148b72d7dc229b5423cc07c3a85314b8d87edbe34709L320' target='_blank'>Link</a></div><div id='project'> Project Name: chainer/chainercv</div><div id='commit'> Commit Name: 8d99e10342647b4c9e63176fc316d51ab6033804</div><div id='time'> Time: 2017-05-19</div><div id='author'> Author: yuyuniitani@gmail.com</div><div id='file'> File Name: chainercv/links/model/faster_rcnn/faster_rcnn.py</div><div id='class'> Class Name: FasterRCNNBase</div><div id='method'> Method Name: prepare</div><BR>