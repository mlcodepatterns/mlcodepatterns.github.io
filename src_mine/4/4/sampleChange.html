<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        s1, r, d, _ = env.step(a[0])
        &#47&#47&#47&#47 Obtain the Q&quot values by feeding the new state through our network
            &#47&#47 Q1 = sess.run(y, feed_dict={inputs: [to_one_hot(s1, 16)]})
        Q1 = <a id="change">qnetwork</a>(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).outputs.numpy()

        &#47&#47&#47&#47 Obtain maxQ&quot and set our target value for chosen action.
        maxQ1 = np.max(Q1)  &#47&#47 in Q-Learning, policy is greedy, so we use "max" to select the next action.</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47&#47&#47 Get new state and reward from environment
        s1, r, d, _ = env.step(a[0])
        &#47&#47&#47&#47 Obtain the Q&quot values by feeding the new state through our network
        Q1 = <a id="change">qnetwork</a>(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).numpy()

        &#47&#47&#47&#47 Obtain maxQ&quot and set our target value for chosen action.
        maxQ1 = np.max(Q1)  &#47&#47 in Q-Learning, policy is greedy, so we use "max" to select the next action.</code></pre>