<html><h3>bcdec7cdfadef83ea07a918a973aba4220177eaf,models/AttModel.py,AttModel,_sample,#AttModel#Any#Any#Any#Any#,251
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 stop when all finished
            if t == 0:
                unfinished = <a id="change">it &gt; 0
     </a>       else:
                unfinished = unfinished * (<a id="change">it &gt; 0)
    </a>        <a id="change">it = it * unfinished.type_as(it)</a>
            <a id="change">seq[:,t]</a> = it
            seqLogprobs[:,t] = logprobs
            &#47&#47 quit loop if all sequences have finished
            if unfinished.sum() == 0:</code></pre><h3>After Change</h3><pre><code class='java'>

        trigrams = [] &#47&#47 will be a list of batch_size dictionaries
        
        seq = fc_feats.new_full((batch_size*sample_n, self.seq_length), <a id="change">self.pad_idx</a>, dtype=torch.long)
        seqLogprobs = fc_feats.new_zeros(batch_size*sample_n, self.seq_length, self.vocab_size + 1)
        for t in range(self.seq_length + 1):
            if t == 0: &#47&#47 input &lt;bos&gt;
                <a id="change">it</a> = fc_feats.new_full(<a id="change">[batch_size*sample_n]</a>, <a id="change">self.bos_idx</a>, dtype=torch.long)

            logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state, output_logsoftmax=output_logsoftmax)
            
            if decoding_constraint and t &gt; 0:
                tmp = logprobs.new_zeros(logprobs.size())
                tmp.scatter_(1, seq[:,t-1].data.unsqueeze(1), float(&quot-inf&quot))
                logprobs = logprobs + tmp

            if remove_bad_endings and t &gt; 0:
                tmp = logprobs.new_zeros(logprobs.size())
                prev_bad = np.isin(seq[:,t-1].data.cpu().numpy(), self.bad_endings_ix)
                &#47&#47 Make it impossible to generate bad_endings
                tmp[torch.from_numpy(prev_bad.astype(&quotuint8&quot)), 0] = float(&quot-inf&quot)
                logprobs = logprobs + tmp

            &#47&#47 Mess with trigrams
            &#47&#47 Copy from https://github.com/lukemelas/image-paragraph-captioning
            if block_trigrams and t &gt;= 3:
                &#47&#47 Store trigram generated at last step
                prev_two_batch = seq[:,t-3:t-1]
                for i in range(batch_size): &#47&#47 = seq.size(0)
                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())
                    current  = seq[i][t-1]
                    if t == 3: &#47&#47 initialize
                        trigrams.append({prev_two: [current]}) &#47&#47 {LongTensor: list containing 1 int}
                    elif t &gt; 3:
                        if prev_two in trigrams[i]: &#47&#47 add to list
                            trigrams[i][prev_two].append(current)
                        else: &#47&#47 create list
                            trigrams[i][prev_two] = [current]
                &#47&#47 Block used trigrams at next step
                prev_two_batch = seq[:,t-2:t]
                mask = torch.zeros(logprobs.size(), requires_grad=False).cuda() &#47&#47 batch_size x vocab_size
                for i in range(batch_size):
                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())
                    if prev_two in trigrams[i]:
                        for j in trigrams[i][prev_two]:
                            mask[i,j] += 1
                &#47&#47 Apply mask to log probs
                &#47&#47logprobs = logprobs - (mask * 1e9)
                alpha = 2.0 &#47&#47 = 4
                logprobs = logprobs + (mask * -0.693 * alpha) &#47&#47 ln(1/2) * alpha (alpha -&gt; infty works best)

            &#47&#47 sample the next word
            if t == self.seq_length: &#47&#47 skip if we achieve maximum length
                break
            it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, temperature)

            &#47&#47 stop when all finished
            if t == 0:
                unfinished = <a id="change">it != self.eos_idx</a>
            else:
                <a id="change">it[~unfinished] = self.pad_idx</a> &#47&#47 This allows eos_idx not being overwritten to 0
                logprobs = logprobs * unfinished.unsqueeze(1).float()
                <a id="change">unfinished = unfinished * (it != self.eos_idx)</a>
            <a id="change">seq[:,t]</a> = it
            seqLogprobs[:,t] = logprobs
            &#47&#47 quit loop if all sequences have finished
            if unfinished.sum() == 0:</code></pre><img src="74764404.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 19</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/bcdec7cdfadef83ea07a918a973aba4220177eaf#diff-19db989ab9f9301d21cdc9066529071818266d09691a8afb0dd298eaded0b467L255' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: bcdec7cdfadef83ea07a918a973aba4220177eaf</div><div id='time'> Time: 2020-04-02</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/AttModel.py</div><div id='class'> Class Name: AttModel</div><div id='method'> Method Name: _sample</div><BR><BR><div id='link'><a href='https://github.com/ruotianluo/ImageCaptioning.pytorch/commit/bcdec7cdfadef83ea07a918a973aba4220177eaf#diff-19db989ab9f9301d21cdc9066529071818266d09691a8afb0dd298eaded0b467L351' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/ImageCaptioning.pytorch</div><div id='commit'> Commit Name: bcdec7cdfadef83ea07a918a973aba4220177eaf</div><div id='time'> Time: 2020-04-02</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/AttModel.py</div><div id='class'> Class Name: AttModel</div><div id='method'> Method Name: _diverse_sample</div><BR><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/bcdec7cdfadef83ea07a918a973aba4220177eaf#diff-19db989ab9f9301d21cdc9066529071818266d09691a8afb0dd298eaded0b467L255' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: bcdec7cdfadef83ea07a918a973aba4220177eaf</div><div id='time'> Time: 2020-04-02</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/AttModel.py</div><div id='class'> Class Name: AttModel</div><div id='method'> Method Name: _sample</div><BR>