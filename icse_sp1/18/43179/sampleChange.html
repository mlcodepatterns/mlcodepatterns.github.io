<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    K.set_session(sess)
    with g.as_default():
      n_tasks = 1
      <a id="change">n_feat = 75</a>
      max_depth = 4
      n_pos = 6
      n_neg = 4
      test_batch_size = 10
      support_batch_size = n_pos + n_neg
      n_train_trials = 80
      
      &#47&#47 Load mini log-solubility dataset.
      featurizer = dc.feat.ConvMolFeaturizer()
      tasks = ["outcome"]
      input_file = os.path.join(self.current_dir, "example_classification.csv")
      loader = dc.data.CSVLoader(
          tasks=tasks, smiles_field="smiles", featurizer=featurizer)
      dataset = loader.featurize(input_file)

      classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

      <a id="change">support_model</a> = dc.nn.SequentialSupportGraph(n_feat)
      
      &#47&#47 Add layers
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add(dc.nn.GraphConv(64, activation=&quotrelu&quot))</a>
      &#47&#47 Need to add batch-norm separately to test/support due to differing
      &#47&#47 shapes.
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
      <a id="change">support_model.add(dc.nn.GraphPool())</a>
      <a id="change">support_model.add_test(dc.nn.GraphGather(test_batch_size))</a>
      <a id="change">support_model.add_support(dc.nn.GraphGather(support_batch_size))</a>

      &#47&#47 Apply an attention lstm layer
      <a id="change">support_model.join(dc.nn.AttnLSTMEmbedding(
          test_batch_size, support_batch_size, max_depth))</a>

      with self.test_session() as sess:
        model = dc.models.SupportGraphClassifier(
          sess, support_model, test_batch_size=test_batch_size,</code></pre><h3>After Change</h3><pre><code class='java'>
    g = tf.Graph()
    sess = tf.Session(graph=g)
    n_tasks = 1
    <a id="change">n_feat = 75</a>
    max_depth = 4
    n_pos = 6
    n_neg = 4
    test_batch_size = 10
    support_batch_size = n_pos + n_neg
    n_train_trials = 80
    
    &#47&#47 Load mini log-solubility dataset.
    featurizer = dc.feat.ConvMolFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_classification.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)
    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

    with g.as_default():
      <a id="change">support_model</a> = dc.nn.SequentialSupportGraph(n_feat)
      
      &#47&#47 Add layers
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add(dc.nn.GraphConv(64, activation=&quotrelu&quot))</a>
      &#47&#47 Need to add batch-norm separately to test/support due to differing
      &#47&#47 shapes.
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
      &#47&#47 output will be (n_atoms, 64)
      <a id="change">support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
      <a id="change">support_model.add(dc.nn.GraphPool())</a>
      <a id="change">support_model.add_test(dc.nn.GraphGather(test_batch_size))</a>
      <a id="change">support_model.add_support(dc.nn.GraphGather(support_batch_size))</a>

      &#47&#47 Apply an attention lstm layer
      <a id="change">support_model.join(dc.nn.AttnLSTMEmbedding(
          test_batch_size, support_batch_size, max_depth))</a>

      with self.test_session() as sess:
        model = dc.models.SupportGraphClassifier(
          sess, support_model, test_batch_size=test_batch_size,</code></pre>