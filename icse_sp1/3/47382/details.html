<html><h3>b70e026702c590618552ab857fc6661662ab72f2,allennlp/modules/attention/linear_attention.py,LinearAttention,_forward_internal,#LinearAttention#Any#Any#,66
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -&gt; torch.Tensor:
        &#47&#47 TODO(mattg): Remove the need for this tiling.
        &#47&#47 https://github.com/allenai/allennlp/pull/1235&#47&#47issuecomment-391540133
        tiled_vector = <a id="change">vector.unsqueeze(1).expand(vector.size()[0],
                                                  matrix.size()[1],
                                                  vector.size()[1])</a>

        combined_tensors = util.combine_tensors(self._combination, [tiled_vector, matrix])
        <a id="change">dot_product = torch.matmul(combined_tensors, self._weight_vector)</a>
        return self._activation(dot_product + self._bias)
</code></pre><h3>After Change</h3><pre><code class='java'>
        combined_tensors = util.combine_tensors_and_multiply(self._combination,
                                                             [vector.unsqueeze(1), matrix],
                                                             self._weight_vector)
        return self._activation(<a id="change">combined_tensors.squeeze(1)</a> + self._bias)
</code></pre><img src="220100094.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/b70e026702c590618552ab857fc6661662ab72f2#diff-2dc3e3d965d390be432e3f1bbdff76257b8f546c9fbc19c0814d2b22ccb1daafL67' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: b70e026702c590618552ab857fc6661662ab72f2</div><div id='time'> Time: 2018-08-21</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: allennlp/modules/attention/linear_attention.py</div><div id='class'> Class Name: LinearAttention</div><div id='method'> Method Name: _forward_internal</div><BR><BR><div id='link'><a href='https://github.com/cornellius-gp/gpytorch/commit/60a342edc8b501802135df44869353cc8604d838#diff-b110adb1897838e27a3903e77c9061e1f5378089b0e617fd7e95ea8489b61282L14' target='_blank'>Link</a></div><div id='project'> Project Name: cornellius-gp/gpytorch</div><div id='commit'> Commit Name: 60a342edc8b501802135df44869353cc8604d838</div><div id='time'> Time: 2018-01-11</div><div id='author'> Author: gpleiss@gmail.com</div><div id='file'> File Name: gpytorch/kernels/rbf_kernel.py</div><div id='class'> Class Name: RBFKernel</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/d1f6748120abc8b1b0bfccd2e6ccc4142ba127d0#diff-b5d561d59860b56c5e63000dbce8a4f22d5ef90a4032198b42e0b8d6139ac700L245' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: d1f6748120abc8b1b0bfccd2e6ccc4142ba127d0</div><div id='time'> Time: 2018-08-29</div><div id='author'> Author: lauraruis@Live.nl</div><div id='file'> File Name: allennlp/modules/conditional_random_field.py</div><div id='class'> Class Name: ConditionalRandomField</div><div id='method'> Method Name: _joint_likelihood</div><BR>