<html><h3>aab3902d4a7d55f5a86058854adc36b8a12c873f,catalyst/dl/callbacks/base.py,OptimizerCallback,on_batch_end,#OptimizerCallback#Any#,202
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                self._accumulation_counter = 0
        else:
            model = state.model
            <a id="change">model.zero_grad()</a>
            optimizer = state.get_key(
                key="optimizer", inner_key=self.optimizer_key
            )
            loss = state.get_key(key="loss", inner_key=self.optimizer_key)</code></pre><h3>After Change</h3><pre><code class='java'>
    def on_batch_end(self, state):
        loss = state.get_key(key="loss", inner_key=self.loss_key)
        if isinstance(loss, dict):
            <a id="change">loss = list(loss.values())</a>
        if isinstance(loss, list):
            loss = torch.mean(torch.stack(loss))

        if self.prefix is not None:
            state.metrics.add_batch_value(metrics_dict={
                self.prefix: loss.item(),
            })

        if not state.need_backward:
            return

        self._accumulation_counter += 1
        model = state.model
        optimizer = state.get_key(
            key="optimizer", inner_key=self.optimizer_key
        )

        &#47&#47 This is very hacky check whether we have AMP optimizer and this may
        &#47&#47 change in future.
        &#47&#47 But alternative solution is to have AmpOptimizerCallback.
        &#47&#47 or expose another c&quottor argument.
        if hasattr(optimizer, &quot_amp_stash&quot):
            from apex import amp
            <a id="change">with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
       </a> else:
            loss.backward()

        if (self._accumulation_counter + 1) % self.accumulation_steps == 0:</code></pre><img src="199705662.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/catalyst-team/catalyst/commit/aab3902d4a7d55f5a86058854adc36b8a12c873f#diff-94f0b9669517bfb949ba9c8c8bd57cbd96ea0f16b7482a95010f509c6c577190L197' target='_blank'>Link</a></div><div id='project'> Project Name: catalyst-team/catalyst</div><div id='commit'> Commit Name: aab3902d4a7d55f5a86058854adc36b8a12c873f</div><div id='time'> Time: 2019-05-20</div><div id='author'> Author: ekhvedchenya@gmail.com</div><div id='file'> File Name: catalyst/dl/callbacks/base.py</div><div id='class'> Class Name: OptimizerCallback</div><div id='method'> Method Name: on_batch_end</div><BR><BR><div id='link'><a href='https://github.com/junyanz/BicycleGAN/commit/6f0eec8ef0a147a80f64f25103089feb33553a06#diff-3ca7434929a648f9dcfde66ac2ba5f180772206c4af14964dbabe1a6caac1c27L192' target='_blank'>Link</a></div><div id='project'> Project Name: junyanz/BicycleGAN</div><div id='commit'> Commit Name: 6f0eec8ef0a147a80f64f25103089feb33553a06</div><div id='time'> Time: 2020-06-27</div><div id='author'> Author: junyanzhu89@gmail.com</div><div id='file'> File Name: models/bicycle_gan_model.py</div><div id='class'> Class Name: BiCycleGANModel</div><div id='method'> Method Name: update_G_and_E</div><BR>