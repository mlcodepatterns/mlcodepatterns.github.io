<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        dataset_as_episodes = self.call_memory(&quotget_all_complete_episodes_from_to&quot,
                                               (self.call_memory(&quotget_last_training_set_episode_id&quot) + 1,
                                                self.call_memory(&quotnum_complete_episodes&quot)))
        <a id="change">if len(dataset_as_episodes) == 0:
            raise ValueError(&quottrain_to_eval_ratio is too high causing the evaluation set to be empty. &quot
                             &quotConsider decreasing its value.&quot)

       </a> ips, dm, dr, seq_dr = self.ope_manager.evaluate(
                                  dataset_as_episodes=dataset_as_episodes,
                                  batch_size=self.ap.network_wrappers[&quotmain&quot].batch_size,
                                  discount_factor=self.ap.algorithm.discount,</code></pre><h3>After Change</h3><pre><code class='java'>
        self.agent_logger.create_signal_value(&quotDirect Method Reward&quot, dm)
        self.agent_logger.create_signal_value(&quotDoubly Robust&quot, dr)
        self.agent_logger.create_signal_value(&quotSequential Doubly Robust&quot, seq_dr)
        <a id="change">self.agent_logger.create_signal_value(&quotWeighted Importance Sampling&quot, wis)</a>

    def get_reward_model_loss(self, batch: Batch):
        network_keys = self.ap.network_wrappers[&quotreward_model&quot].input_embedders_parameters.keys()
        current_rewards_prediction_for_all_actions = self.networks[&quotreward_model&quot].online_network.predict(</code></pre>