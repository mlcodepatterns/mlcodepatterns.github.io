<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        self.src_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        self.feed_forward = FFN(d_model, pdrop, d_ff=d_ff, activation_type=activation_type)
        self.ln1 = <a id="change">LayerNorm(d_model)</a>
        self.ln2 = LayerNorm(d_model)
        self.ln3 = LayerNorm(d_model)
        self.dropout = nn.Dropout(pdrop)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, num_heads, d_model, pdrop, scale=True, activation_type=&quotrelu&quot, d_ff=None):
        super(TransformerDecoder, self).__init__()
        self.d_model = d_model
        self.d_ff = <a id="change">d_ff if d_ff is not None else 4 * d_model</a>
        self.self_attn = MultiHeadedAttention(num_heads, self.d_model, pdrop, scale=scale)
        self.src_attn = MultiHeadedAttention(num_heads, self.d_model, pdrop, scale=scale)
        self.ffn = nn.Sequential(pytorch_linear(self.d_model, self.d_ff),
                                 pytorch_activation(activation_type),</code></pre>