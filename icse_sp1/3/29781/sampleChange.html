<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
             for i in range(num_streams)]
  for thread in threads:
    thread.start()
  <a id="change">for thread in threads:
    thread.join()

  &#47&#47 Metadata to attach to samples
 </a> metadata = {&quotnetperf_test_length&quot: FLAGS.netperf_test_length,
              &quotmax_iter&quot: FLAGS.netperf_max_iter or 1}

  parsed_output = [_ParseNetperfOutput(stdout, metadata, benchmark_name)</code></pre><h3>After Change</h3><pre><code class='java'>
  &#47&#47 Run all of the netperf processes and collect their stdout
  &#47&#47 TODO: Record start times of netperf processes on the remote machine

  <a id="change">remote_script_path = &quot%s/run/%s&quot % (vm.GetScratchDir(), REMOTE_SCRIPT)</a>
  remote_cmd = &quot%s --netperf_cmd="%s" --num_streams=%s&quot % \
               (remote_script_path, netperf_cmd, num_streams)
  remote_stdout, _ = <a id="change">vm.RemoteCommand(remote_cmd)</a>

  &#47&#47 Decode stdouts, stderrs, and return codes from remote command&quots stdout
  stdouts, stderrs, return_codes = json.loads(remote_stdout)
</code></pre>