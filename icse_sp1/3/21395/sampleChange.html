<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                self.__graph_ops[&quotoptimizer&quot] = tf.train.AdagradOptimizer(self.__learning_rate).minimize(self.__graph_ops[&quotcost&quot])
                self.__log(&quotUsing Adagrad optimizer&quot)
            elif self.__optimizer == &quotAdadelta&quot:
                self.__graph_ops[&quotoptimizer&quot] = <a id="change">tf.train.AdadeltaOptimizer(self.__learning_rate).minimize(self.__graph_ops[&quotcost&quot])</a>
                self.__log(&quotUsing Adadelta optimizer&quot)
            elif self.__optimizer == &quotSGD&quot:
                self.__graph_ops[&quotoptimizer&quot] = tf.train.GradientDescentOptimizer(self.__learning_rate).minimize(self.__graph_ops[&quotcost&quot])
                self.__log(&quotUsing SGD optimizer&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 Compute gradients, clip them, the apply the clipped gradients
            gradients, variables = zip(*self.__graph_ops[&quotoptimizer&quot].compute_gradients(self.__graph_ops[&quotcost&quot]))
            gradients, global_grad_norm = tf.clip_by_global_norm(gradients, 5.0)  &#47&#47 need to make this 5.0 an adjustable hyperparameter
            self.__graph_ops[&quotoptimizer&quot] = <a id="change">self.__graph_ops[&quotoptimizer&quot].apply_gradients(zip(gradients, variables))</a>

            if self.__problem_type == definitions.ProblemType.CLASSIFICATION:
                class_predictions = tf.argmax(tf.nn.softmax(xx), 1)
                correct_predictions = tf.equal(class_predictions, tf.argmax(y, 1))</code></pre>