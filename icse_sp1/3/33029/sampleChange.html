<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
            next_q_values = self.get_next_action_q_values(next_states, next_actions)

        filtered_next_q_vals = <a id="change">next_q_values.reshape(-1, 1)</a> * not_done_mask

        if self.use_reward_burnin and self.minibatch &lt; self.reward_burnin:
            target_q_values = rewards</code></pre><h3>After Change</h3><pre><code class='java'>
            next_actions = training_samples.next_actions
            next_q_values = self.get_next_action_q_values(next_states, next_actions)

        filtered_next_q_vals = <a id="change">next_q_values * not_done_mask</a>

        if self.use_reward_burnin and self.minibatch &lt; self.reward_burnin:
            target_q_values = rewards
        else:
            target_q_values = rewards + (discount_tensor * filtered_next_q_vals)

        &#47&#47 Get Q-value of action taken
        all_q_values = self.q_network(states)
        self.all_action_scores = deepcopy(all_q_values.detach())
        q_values = torch.sum(all_q_values * actions, 1, keepdim=True)

        logger.info(q_values.shape)
        <a id="change">logger.info(target_q_values.shape)</a>
        logger.info(rewards.shape)
        logger.info(next_q_values.shape)
        loss = self.q_network_loss(q_values, target_q_values)
        self.loss = loss.detach()</code></pre>