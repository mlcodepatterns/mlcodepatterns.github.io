<html><h3>4d68a1e4435dfeb5884093aa91a33e1b34a909cc,ml/rl/training/_dqn_trainer.py,_DQNTrainer,train,#_DQNTrainer#Any#,65
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 get reward estimates
        reward_estimates = self.reward_network(current_state).q_values
        reward_loss = F.mse_loss(reward_estimates, rewards)
        <a id="change">self.reward_network_optimizer.zero_grad()</a>
        reward_loss.backward()
        self.reward_network_optimizer.step()

        self.loss_reporter.report(</code></pre><h3>After Change</h3><pre><code class='java'>
        self.minibatch += 1
        rewards = boosted_rewards
        discount_tensor = torch.full_like(rewards, self.gamma)
        not_done_mask = <a id="change">learning_input.not_terminal.float()</a>

        if self.use_seq_num_diff_as_time_diff:
            &#47&#47 TODO: Implement this in another diff
            logger.warning(
                "_dqn_trainer has not implemented use_seq_num_diff_as_time_diff feature"
            )
            pass

        all_next_q_values, all_next_q_values_target = self.get_detached_q_values(
            learning_input.next_state
        )

        if self.maxq_learning:
            &#47&#47 Compute max a&quot Q(s&quot, a&quot) over all possible actions using target network
            next_q_values, max_q_action_idxs = self.get_max_q_values_with_target(
                all_next_q_values,
                all_next_q_values_target,
                learning_input.possible_next_actions_mask.float(),
            )
        else:
            &#47&#47 SARSA
            next_q_values, max_q_action_idxs = self.get_max_q_values_with_target(
                all_next_q_values, all_next_q_values_target, learning_input.next_action
            )

        filtered_next_q_vals = next_q_values * not_done_mask

        if self.minibatch &lt; self.reward_burnin:
            target_q_values = rewards
        else:
            target_q_values = rewards + (discount_tensor * filtered_next_q_vals)

        &#47&#47 Get Q-value of action taken
        current_state = rlt.StateInput(state=learning_input.state)
        all_q_values = self.q_network(current_state).q_values
        self.all_action_scores = all_q_values.detach()
        q_values = torch.sum(all_q_values * learning_input.action, 1, keepdim=True)

        loss = self.q_network_loss(q_values, target_q_values)
        self.loss = loss.detach()

        self.q_network_optimizer.zero_grad()
        loss.backward()
        if self.gradient_handler:
            self.gradient_handler(self.q_network.parameters())
        self.q_network_optimizer.step()

        if self.minibatch &lt; self.reward_burnin:
            &#47&#47 Reward burnin: force target network
            self._soft_update(self.q_network, self.q_network_target, 1.0)
        else:
            &#47&#47 Use the soft update rule to update target network
            self._soft_update(self.q_network, self.q_network_target, self.tau)

        logged_action_idxs = learning_input.action.argmax(dim=1, keepdim=True)
        <a id="change">reward_loss, model_rewards, model_propensities = self.calculate_cpes(
            training_batch,
            current_state,
            logged_action_idxs,
            max_q_action_idxs,
            discount_tensor,
            not_done_mask,
        )</a>

        self.loss_reporter.report(
            td_loss=self.loss,
            reward_loss=reward_loss,</code></pre><img src="156111022.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/4d68a1e4435dfeb5884093aa91a33e1b34a909cc#diff-913a8f0fd2562d0fbb89ee7491fa2c9c45b085151c0e705ea2baf8e58e0bfd49L65' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 4d68a1e4435dfeb5884093aa91a33e1b34a909cc</div><div id='time'> Time: 2019-02-13</div><div id='author'> Author: kittipat@fb.com</div><div id='file'> File Name: ml/rl/training/_dqn_trainer.py</div><div id='class'> Class Name: _DQNTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/5028ed1b6bedd526dee27ea731284f43e87303f0#diff-1d9c528283eebce84124f45bd2f04e9c1b50dc4d3f63e867776eb89dc55dd95fL271' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 5028ed1b6bedd526dee27ea731284f43e87303f0</div><div id='time'> Time: 2020-03-11</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/trainer.py</div><div id='class'> Class Name: Trainer</div><div id='method'> Method Name: train_step</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/69061e67d62a067c2a8a5c6a440f7b9605c111d6#diff-5644bcaad81ed3a83dea1856c2e1f9c847ba4cbc711ff993f48ca73d77fe0080L36' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 69061e67d62a067c2a8a5c6a440f7b9605c111d6</div><div id='time'> Time: 2020-08-28</div><div id='author'> Author: badri@fb.com</div><div id='file'> File Name: reagent/training/reinforce.py</div><div id='class'> Class Name: Reinforce</div><div id='method'> Method Name: train</div><BR>