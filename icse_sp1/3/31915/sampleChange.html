<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if device == &quotgpu&quot:
            scattering.cuda()
            x_data = <a id="change">x_data.cuda()</a>
        else:
            scattering.cpu()
            x_data = torch.randn(1, M, N, O).float()
            x_data = x_data.cpu()</code></pre><h3>After Change</h3><pre><code class='java'>

scattering = Scattering3D(M, N, O, J, L, sigma_0)

x = <a id="change">torch.randn(batch_size, M, N, O, dtype=torch.float32)</a>

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Run the benchmark
&#47&#47 -----------------
&#47&#47 For each device, we need to convert the Tensor `x` to the appropriate type,
&#47&#47 invoke `times` calls to `scattering.forward` and print the running times.
&#47&#47 Before the timer starts, we add an extra `scattering.forward` call to ensure
&#47&#47 any first-time overhead, such as memory allocation and CUDA kernel
&#47&#47 compilation, is not counted. If the benchmark is running on the GPU, we also
&#47&#47 need to call `torch.cuda.synchronize()` before and after the benchmark to
&#47&#47 make sure that all CUDA kernels have finished executing.

for device in devices:
    fmt_str = &quot==&gt; Testing Float32 with {} backend, on {}, forward&quot
    print(fmt_str.format(backend.NAME, device.upper()))

    if device == &quotgpu&quot:
        <a id="change">x = x.cuda()</a>
    else:
        x = x.cpu()

    scattering.forward(x, method=&quotintegral&quot, integral_powers=integral_powers)</code></pre>