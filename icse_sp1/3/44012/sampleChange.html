<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    - About cross-entropy: `wiki &lt;https://en.wikipedia.org/wiki/Cross_entropy&gt;`_.\n
    - The code is borrowed from: `here &lt;https://en.wikipedia.org/wiki/Cross_entropy&gt;`_.
    
    <a id="change">with tf.name_scope(name):
        &#47&#47 net_output_tf = output
        &#47&#47 target_tf = target
        &#47&#47 cross_entropy = tf.add(tf.mul(tf.log(net_output_tf, name=None),target_tf),
        &#47&#47                      tf.mul(tf.log(1 - net_output_tf), (1 - target_tf)))
        &#47&#47 return -1 * tf.reduce_mean(tf.reduce_sum(cross_entropy, 1), name=&quotcross_entropy_mean&quot)
        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output, target))


</a>def binary_cross_entropy(output, target, epsilon=1e-8, name=&quotbce_loss&quot):
    Computes binary cross entropy given `output`.

    For brevity, let `x = output`, `z = target`.  The binary cross entropy loss is</code></pre><h3>After Change</h3><pre><code class='java'>
        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, targets=target))
    except: &#47&#47 TF 1.0
        assert name is not None, "Please give a unique name to tl.cost.cross_entropy for TF1.0+"
        return <a id="change">tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output, name=name))</a>

def sigmoid_cross_entropy(output, target, name=None):
    It is a sigmoid cross-entropy operation, see ``tf.nn.sigmoid_cross_entropy_with_logits``.
    </code></pre>