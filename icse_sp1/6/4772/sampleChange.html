<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

&#47&#47 Create random Tensors for weights, and wrap them in Variables.
w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)
w2 = <a id="change">Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)</a>

learning_rate = 1e-6
for t in range(500):
    &#47&#47 To apply our Function, we use Function.apply method. We alias this as &quotrelu&quot.</code></pre><h3>After Change</h3><pre><code class='java'>

&#47&#47 Create random Tensors for weights.
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = <a id="change">torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)</a>

learning_rate = 1e-6
for t in range(500):
    &#47&#47 To apply our Function, we use Function.apply method. We alias this as &quotrelu&quot.
    relu = MyReLU.apply

    &#47&#47 Forward pass: compute predicted y using operations; we compute
    &#47&#47 ReLU using our custom autograd operation.
    y_pred = relu(x.mm(w1)).mm(w2)

    &#47&#47 Compute and print loss
    loss = (y_pred - y).pow(2).sum()
    print(t, loss.item())

    &#47&#47 Use autograd to compute the backward pass.
    loss.backward()

    &#47&#47 Update weights using gradient descent
    <a id="change">with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad

        &#47&#47 Manually zero the gradients after updating weights
        w1.grad.zero_()
        w2.grad.zero_()</a>
</code></pre>