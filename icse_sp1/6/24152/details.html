<html><h3>249c21346208b682d128e28fe4480262f8b76cd8,torch/distributed/optim/zero_redundancy_optimizer.py,ZeroRedundancyOptimizer,state_dict,#ZeroRedundancyOptimizer#,393
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            start = end

        return {
            "state": [s["state"] <a id="change">for</a> s in self._all_states],
            "param_groups": param_groups,
            "partition": partition,
            "local_state_dict": False,</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 - go through the per-shard states, which are all indexed locally
        for rank, s in enumerate(self._all_states):
            &#47&#47 -- match the local indexing and the global partition, update the corresponding saved state globally
            <a id="change">for local_pg, global_pg in zip(
                s["param_groups"], self.partition_parameters()[rank]
            ):
                local_index_to_param_id = {
                    i_param: id(global_pg["params"][i])
                    for i, i_param in enumerate(local_pg["params"])
                }

                for local_param_index in local_pg["params"]:
                    &#47&#47 Update the state, if any
                    if local_param_index in s["state"].keys():
                        global_id = self._param_to_index[
                            local_index_to_param_id[local_param_index]
                        ]
                        state_dict["state"][global_id] = s["state"][local_param_index]

        &#47&#47 Make sure that the parameters are sorted in the state, as expected
       </a> state_dict["state"] = dict(sorted(state_dict["state"].items()))
        return state_dict

    @staticmethod</code></pre><img src="127051543.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/249c21346208b682d128e28fe4480262f8b76cd8#diff-30d3614ba0cf90cf509ef5d96b4e120eb8bfc65a7602b8438a3718f7f197ef6cL393' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 249c21346208b682d128e28fe4480262f8b76cd8</div><div id='time'> Time: 2021-02-27</div><div id='author'> Author: benjamin.lefaudeux@gmail.com</div><div id='file'> File Name: torch/distributed/optim/zero_redundancy_optimizer.py</div><div id='class'> Class Name: ZeroRedundancyOptimizer</div><div id='method'> Method Name: state_dict</div><BR><BR><div id='link'><a href='https://github.com/keras-team/keras/commit/555ca942df407b8c1bf1d48383c60fa1bf09cc1d#diff-9e5405f9e10f42526dbf1fefa450e58964aa819e1c57308377c4c3d3f74b3498L21' target='_blank'>Link</a></div><div id='project'> Project Name: keras-team/keras</div><div id='commit'> Commit Name: 555ca942df407b8c1bf1d48383c60fa1bf09cc1d</div><div id='time'> Time: 2019-08-28</div><div id='author'> Author: francois.chollet@gmail.com</div><div id='file'> File Name: keras/utils/layer_utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: count_params</div><BR><BR><div id='link'><a href='https://github.com/keras-team/keras/commit/555ca942df407b8c1bf1d48383c60fa1bf09cc1d#diff-9a910d8066ae4cc4a32c73163e3688b40724b264d5b446e75cbd49beb7abf625L446' target='_blank'>Link</a></div><div id='project'> Project Name: keras-team/keras</div><div id='commit'> Commit Name: 555ca942df407b8c1bf1d48383c60fa1bf09cc1d</div><div id='time'> Time: 2019-08-28</div><div id='author'> Author: francois.chollet@gmail.com</div><div id='file'> File Name: keras/engine/network.py</div><div id='class'> Class Name: Network</div><div id='method'> Method Name: losses</div><BR>