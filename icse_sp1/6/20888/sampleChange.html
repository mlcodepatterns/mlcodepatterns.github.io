<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                op5 = tf.group(*[ tf.assign(w,v) for w,v in zip(restored_vars, tmp_vars)])
                with tf.get_default_graph().control_dependencies([op5]):
                    &#47&#47 Flin = gamma * IF - rho * JF + beta * JtF
                    <a id="change">op7 = tf.group(*[tf.assign_add(gsw, (jg * self._beta)) if jg is not None else tf.no_op() for gsw, jg in zip(gswap, Jgrads)])</a>
                    <a id="change">with tf.get_default_graph().control_dependencies([op7]):
                        flin_grads_and_vars = zip(gswap, var_list)
                        &#47&#47 step 1
                        op8 = self.optimizer.apply_gradients(list(flin_grads_and_vars).copy(), global_step=global_step, name=name)
                        with tf.get_default_graph().control_dependencies([op8]):
                            return tf.no_op()
 </a> def _apply_sparse(self, grad, var):
    raise NotImplementedError("Sparse gradient updates are not supported.")
  def variables(self):
      return super().variables() + self.optimizer.variables()</code></pre><h3>After Change</h3><pre><code class='java'>
                            print("JG NONE", grad)
                            flin += [grad]
                        else:
                            flin += <a id="change">[grad + jg * self._beta]</a>

                    step3 = list(zip(flin, var_list))
                    op6 = self.optimizer.apply_gradients(step3.copy(), global_step=global_step, name=name)
                    with tf.get_default_graph().control_dependencies([op6]):</code></pre>