<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if self.params.summary_verbosity &gt;= 2:
          &#47&#47 Histogram of log values of all non-zero gradients.
          all_grads = <a id="change">[]</a>
          for grad, var in avg_grads:
            all_grads.append(tf.reshape(grad, [-1]))
          grads = tf.abs(<a id="change">tf.concat(all_grads, 0)</a>)
          &#47&#47 exclude grads with zero values.
          indices_for_non_zero_grads = tf.where(tf.not_equal(grads, 0))
          log_grads = tf.reshape(
              tf.log(tf.gather(grads, indices_for_non_zero_grads)), [-1])
          <a id="change">tf</a>.summary.histogram(&quotlog_gradients&quot, log_grads)

        if self.params.summary_verbosity &gt;= 3:
          for grad, var in avg_grads:</code></pre><h3>After Change</h3><pre><code class='java'>
            inc_loss_scale_every_n=self.params.fp16_inc_loss_scale_every_n,
            is_chief=not self.job_name or self.task_index == 0)

        <a id="change">with tf.name_scope(&quotappend_apply_gradient_ops&quot):
          self.variable_mgr.append_apply_gradients_ops(
              gradient_state, opt, clipped_grads, training_ops,
              loss_scale_params)
   </a> train_op = tf.group(*(training_ops + update_ops), name=&quottrain_ops_group&quot)

    with tf.device(self.cpu_device):
      if self.task_index == 0 and self.params.summary_verbosity &gt;= 1:</code></pre>