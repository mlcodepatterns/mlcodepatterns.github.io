<html><h3>e822a02daa1cd1c42f6d2bf309d5996a21979ef1,qucumber/quantum_reconstruction.py,QuantumReconstruction,fit,#QuantumReconstruction#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,114
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                
                
                vector_to_grads(all_grads[0],self.nn_state.rbm_am.parameters())
                vector_to_grads(all_grads[1],<a id="change">self.nn_state.rbm_ph.parameters()</a>)
                &#47&#47 assign all available gradients to the corresponding parameter
                &#47&#47for net in self.nn_state.networks:
                </code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47                             self.nn_state.rbm_am.hidden_bias],lr=lr)
            batch_bases = None
        callbacks.on_train_start(self)
        t0 = <a id="change">time.time()</a>
        batch_num = m.ceil(train_samples.shape[0] / pos_batch_size)
        for ep in progress_bar(range(1,epochs+1), desc="Epochs ",
                               disable=disable_progbar):

            random_permutation      = torch.randperm(train_samples.shape[0])
            shuffled_samples        = train_samples[random_permutation]
            &#47&#47 List of all the batches for positive phase.
            pos_batches = [shuffled_samples[batch_start:(batch_start + pos_batch_size)]
                           for batch_start in range(0, len(train_samples), pos_batch_size)]

            if input_bases is not None:
                shuffled_bases = input_bases[random_permutation]
                pos_batches_bases = [shuffled_bases[batch_start:(batch_start + pos_batch_size)]
                           for batch_start in range(0, len(train_samples), pos_batch_size)]
             
            
            &#47&#47 DATALOADER
            &#47&#47pos_batches = DataLoader(train_samples, batch_size=pos_batch_size,
            &#47&#47                         shuffle=shf)
            &#47&#47multiplier = int((neg_batch_size / pos_batch_size) + 0.5)
            &#47&#47 
            &#47&#47neg_batches = [DataLoader(train_samples, batch_size=neg_batch_size,
            &#47&#47                          shuffle=True)
            &#47&#47               for i in range(multiplier)]
            &#47&#47neg_batches = chain(*neg_batches)
            callbacks.on_epoch_start(self, ep)

            if self.stop_training:  &#47&#47 check for stop_training signal
                break
            
            &#47&#47for batch_num, (pos_batch,neg_batch) in enumerate(zip(pos_batches,
            &#47&#47                                                   neg_batches)):
            for b in range(batch_num):
                callbacks.on_batch_start(self, ep, b)
                &#47&#47if input_bases is not None:
                &#47&#47    pos_batch_bases = input_bases[batch_num*pos_batch_size:(batch_num+1)*;pos_batch_size]

                if input_bases is None:
                    random_permutation      = torch.randperm(train_samples.shape[0])
                    neg_batch        = train_samples[random_permutation][0:neg_batch_size]
                
                &#47&#47neg_batches = [shuffled_samples[batch_start:(batch_start + neg_batch_size)]
                &#47&#47           for batch_start in range(0, len(train_samples), neg_batch_size)]

                &#47&#47if input_bases is not None:
                else:
                    random_permutation      = torch.randperm(z_samples.shape[0])
                    neg_batch = z_samples[random_permutation][0:neg_batch_size]
                    batch_bases = pos_batches_bases[b]
                self.nn_state.set_visible_layer(neg_batch)
                all_grads = self.compute_batch_gradients(k, pos_batches[b],batch_bases)
                optimizer.zero_grad()  &#47&#47 clear any cached gradients
                
                
                for p,net in enumerate(self.nn_state.networks):
                    rbm = getattr(self.nn_state,net)
                    vector_to_grads(all_grads[p],rbm.parameters())
                &#47&#47vector_to_grads(all_grads[0],self.nn_state.rbm_am.parameters())
                &#47&#47vector_to_grads(all_grads[1],self.nn_state.rbm_ph.parameters())
                &#47&#47 assign all available gradients to the corresponding parameter
                &#47&#47for net in self.nn_state.networks:
                
                &#47&#47for net in self.nn_state.networks:
                &#47&#47    rbm = getattr(self.nn_state, net)
                &#47&#47    for param in all_grads[net].keys():
                &#47&#47        getattr(rbm, param).grad = all_grads[net][param]
                optimizer.step()  &#47&#47 tell the optimizer to apply the gradients

                callbacks.on_batch_end(self, ep, b)
            if observer is not None:
                if ((ep % observer.frequency) == 0): 
                    &#47&#47if target_psi is not None:
                    stat = observer.scan(ep,self.nn_state)

            callbacks.on_epoch_end(self, ep)
        &#47&#47F = self.fidelity(target_psi,vis)
        &#47&#47print(F.item())
        &#47&#47callbacks.on_train_end(self)
        <a id="change">t1 = time.time()</a>
        print("\nElapsed time = %.2f" %<a id="change">(t1-t0)</a>) 


def vector_to_grads(vec, parameters):</code></pre><img src="265831960.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PIQuIL/QuCumber/commit/e822a02daa1cd1c42f6d2bf309d5996a21979ef1#diff-ef251de1419577af4a10e0c40393f8f877a610257eae232bc4d877dfb2433882L185' target='_blank'>Link</a></div><div id='project'> Project Name: PIQuIL/QuCumber</div><div id='commit'> Commit Name: e822a02daa1cd1c42f6d2bf309d5996a21979ef1</div><div id='time'> Time: 2018-07-31</div><div id='author'> Author: gtorlai@uwaterloo.ca</div><div id='file'> File Name: qucumber/quantum_reconstruction.py</div><div id='class'> Class Name: QuantumReconstruction</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/PIQuIL/QuCumber/commit/e822a02daa1cd1c42f6d2bf309d5996a21979ef1#diff-ef251de1419577af4a10e0c40393f8f877a610257eae232bc4d877dfb2433882L185' target='_blank'>Link</a></div><div id='project'> Project Name: PIQuIL/QuCumber</div><div id='commit'> Commit Name: e822a02daa1cd1c42f6d2bf309d5996a21979ef1</div><div id='time'> Time: 2018-07-31</div><div id='author'> Author: gtorlai@uwaterloo.ca</div><div id='file'> File Name: qucumber/quantum_reconstruction.py</div><div id='class'> Class Name: QuantumReconstruction</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/KaiyangZhou/deep-person-reid/commit/b88d36cd9c8056e15607a40f5d10a9072ab84b22#diff-e6c0c833279ad43f0c76b2e4799c36ce85c9cbbcd1aeb2f6e0af3401a600792eL97' target='_blank'>Link</a></div><div id='project'> Project Name: KaiyangZhou/deep-person-reid</div><div id='commit'> Commit Name: b88d36cd9c8056e15607a40f5d10a9072ab84b22</div><div id='time'> Time: 2018-07-06</div><div id='author'> Author: k.zhou@qmul.ac.uk</div><div id='file'> File Name: train_vidreid_xent.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: main</div><BR><BR><div id='link'><a href='https://github.com/KaiyangZhou/deep-person-reid/commit/b88d36cd9c8056e15607a40f5d10a9072ab84b22#diff-da7ad75a94e9cad37a1620619fb9b62dc9ca1760acdab2b811be171463c78966L105' target='_blank'>Link</a></div><div id='project'> Project Name: KaiyangZhou/deep-person-reid</div><div id='commit'> Commit Name: b88d36cd9c8056e15607a40f5d10a9072ab84b22</div><div id='time'> Time: 2018-07-06</div><div id='author'> Author: k.zhou@qmul.ac.uk</div><div id='file'> File Name: train_imgreid_xent.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: main</div><BR>