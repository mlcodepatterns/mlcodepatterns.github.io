<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def __init__(self, d_model, num_heads, pdrop, scale=True, activation=&quotrelu&quot, d_ff=None, name=None):
        super(TransformerEncoder, self).__init__(name=name)
        <a id="change">if d_ff is None:
            d_ff = 4 * d_model

       </a> self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale)
        self.src_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale)
        self.dropout = tf.keras.layers.Dropout(pdrop)
        self.ln1 = LayerNorm(name=&quotln_1&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.ffn = FFN(d_model, pdrop, activation_type, d_ff, name=&quotffn&quot)
        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.ln3 = <a id="change">tf.keras.layers.LayerNormalization(epsilon=1e-6)</a>
        self.dropout = tf.keras.layers.Dropout(pdrop)

    def call(self, inputs):
        memory, x, src_mask, tgt_mask = inputs</code></pre>