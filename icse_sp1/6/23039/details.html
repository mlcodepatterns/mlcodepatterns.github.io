<html><h3>d4255c9c4d04cf7f09881b272535cfdc155957a7,agent.py,Agent,learn,#Agent#Any#,51
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Calculate nth next state probabilities
    self.online_net.reset_noise()  &#47&#47 Sample new noise for action selection
    pns = <a id="change">self.online_net(next_states).data</a>  &#47&#47 Probabilities p(s_t+n, ·; θonline)
    dns = self.support.expand_as(pns) * pns  &#47&#47 Distribution d_t+n = (z, p(s_t+n, ·; θonline))
    argmax_indices_ns = dns.sum(2).max(1)[1]  &#47&#47 Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; θonline))]
    self.target_net.reset_noise()  &#47&#47 Sample new target net noise</code></pre><h3>After Change</h3><pre><code class='java'>
    ps = self.online_net(states)  &#47&#47 Probabilities p(s_t, ·; θonline)
    ps_a = ps[range(self.batch_size), actions]  &#47&#47 p(s_t, a_t; θonline)

    <a id="change">with torch.no_grad():
      &#47&#47 Calculate nth next state probabilities
      &#47&#47 TODO: Add back below but prevent inplace operation?
      &#47&#47 self.online_net.reset_noise()  &#47&#47 Sample new noise for action selection
      pns = self.online_net(next_states)  &#47&#47 Probabilities p(s_t+n, ·; θonline)
      dns = self.support.expand_as(pns) * pns  &#47&#47 Distribution d_t+n = (z, p(s_t+n, ·; θonline))
      argmax_indices_ns = dns.sum(2).max(1)[1]  &#47&#47 Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; θonline))]
      self.target_net.reset_noise()  &#47&#47 Sample new target net noise
      pns = self.target_net(next_states)  &#47&#47 Probabilities p(s_t+n, ·; θtarget)
      pns_a = pns[range(self.batch_size), argmax_indices_ns]  &#47&#47 Double-Q probabilities p(s_t+n, argmax_a[(z, p(s_t+n, a; θonline))]; θtarget)

      &#47&#47 Compute Tz (Bellman operator T applied to z)
      Tz = returns.unsqueeze(1) + nonterminals * (self.discount ** self.n) * self.support.unsqueeze(0)  &#47&#47 Tz = R^n + (γ^n)z (accounting for terminal states)
      Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)  &#47&#47 Clamp between supported values
      &#47&#47 Compute L2 projection of Tz onto fixed support z
      b = (Tz - self.Vmin) / self.delta_z  &#47&#47 b = (Tz - Vmin) / Δz
      l, u = b.floor().long(), b.ceil().long()
      &#47&#47 Fix disappearing probability mass when l = b = u (b is int)
      l[(u &gt; 0) * (l == u)] -= 1
      u[(l &lt; (self.atoms - 1)) * (l == u)] += 1

      &#47&#47 Distribute probability of Tz
      m = states.new_zeros(self.batch_size, self.atoms)
      offset = torch.linspace(0, ((self.batch_size - 1) * self.atoms), self.batch_size).unsqueeze(1).expand(self.batch_size, self.atoms).to(actions)
      m.view(-1).index_add_(0, (l + offset).view(-1), (pns_a * (u.float() - b)).view(-1))  &#47&#47 m_l = m_l + p(s_t+n, a*)(u - b)
      m.view(-1).index_add_(0, (u + offset).view(-1), (pns_a * (b - l.float())).view(-1))  &#47&#47 m_u = m_u + p(s_t+n, a*)(b - l)

   </a> ps_a = ps_a.clamp(min=1e-3)  &#47&#47 Clamp for numerical stability in log
    loss = -torch.sum(m * ps_a.log(), 1)  &#47&#47 Cross-entropy loss (minimises DKL(m||p(s_t, a_t)))
    self.online_net.zero_grad()
    (weights * loss).mean().backward()  &#47&#47 Importance weight losses</code></pre><img src="122257075.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/Kaixhin/Rainbow/commit/d4255c9c4d04cf7f09881b272535cfdc155957a7#diff-5f5933f2ca262b18d3bbf61a15941bdcb093728d17eabba09ba2142f6ce9e2deL51' target='_blank'>Link</a></div><div id='project'> Project Name: Kaixhin/Rainbow</div><div id='commit'> Commit Name: d4255c9c4d04cf7f09881b272535cfdc155957a7</div><div id='time'> Time: 2018-04-28</div><div id='author'> Author: design@kaixhin.com</div><div id='file'> File Name: agent.py</div><div id='class'> Class Name: Agent</div><div id='method'> Method Name: learn</div><BR><BR><div id='link'><a href='https://github.com/Kaixhin/Rainbow/commit/d4255c9c4d04cf7f09881b272535cfdc155957a7#diff-5f5933f2ca262b18d3bbf61a15941bdcb093728d17eabba09ba2142f6ce9e2deL100' target='_blank'>Link</a></div><div id='project'> Project Name: Kaixhin/Rainbow</div><div id='commit'> Commit Name: d4255c9c4d04cf7f09881b272535cfdc155957a7</div><div id='time'> Time: 2018-04-28</div><div id='author'> Author: design@kaixhin.com</div><div id='file'> File Name: agent.py</div><div id='class'> Class Name: Agent</div><div id='method'> Method Name: evaluate_q</div><BR><BR><div id='link'><a href='https://github.com/Kaixhin/Rainbow/commit/d4255c9c4d04cf7f09881b272535cfdc155957a7#diff-5f5933f2ca262b18d3bbf61a15941bdcb093728d17eabba09ba2142f6ce9e2deL44' target='_blank'>Link</a></div><div id='project'> Project Name: Kaixhin/Rainbow</div><div id='commit'> Commit Name: d4255c9c4d04cf7f09881b272535cfdc155957a7</div><div id='time'> Time: 2018-04-28</div><div id='author'> Author: design@kaixhin.com</div><div id='file'> File Name: agent.py</div><div id='class'> Class Name: Agent</div><div id='method'> Method Name: act</div><BR>