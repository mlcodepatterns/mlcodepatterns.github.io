<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 less susceptible to overfitting.
            &#47&#47
            for train_idx, holdout_idx in kfold.split(X, y, groups):
                instance = <a id="change">clone(regr)</a>
                if sample_weight is None:
                    instance.fit(X[train_idx], y[train_idx])
                else:
                    instance.fit(X[train_idx], y[train_idx],
                                 sample_weight=sample_weight[train_idx])
                y_pred = instance.predict(X[holdout_idx])
                <a id="change">meta_features[holdout_idx, i] = y_pred</a>

        &#47&#47 save meta-features for training data
        if self.store_train_meta_features:
            self.train_meta_features_ = meta_features</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Advantage of this complex approach is that data points we&quotre
        &#47&#47 predicting have not been trained on by the algorithm, so it&quots
        &#47&#47 less susceptible to overfitting.
        <a id="change">if sample_weight is None:
            fit_params = None
        else:
            fit_params = dict(sample_weight=sample_weight)
       </a> meta_features = np.column_stack([cross_val_predict(
                regr, X, y, groups=groups, cv=kfold,
                n_jobs=self.n_jobs, fit_params=fit_params,
                pre_dispatch=self.pre_dispatch)</code></pre>