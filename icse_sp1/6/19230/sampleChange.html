<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 For the GAT model to match that in the paper, we need to ensure that the graph has self-loops,
        &#47&#47 since the neighbourhood of node i in eq. (4) includes node i itself.
        &#47&#47 Adding self-loops to A via setting the diagonal elements of A to 1.0:
        <a id="change">if kwargs.get("add_self_loops", False):
            &#47&#47 get the number of nodes from inputs[1] directly
            N = K.int_shape(inputs[1])[-1]
            if N is not None:
                &#47&#47 create self-loops
                A = tf.linalg.set_diag(A, K.cast(np.ones((N,)), dtype="float"))
            else:
                raise ValueError(
                    "{}: need to know number of nodes to add self-loops; obtained None instead".format(
                        type(self).__name__
                    )
                )

       </a> outputs = []
        for head in range(self.attn_heads):
            kernel = self.kernels[head]  &#47&#47 W in the paper (F x F&quot)
            attention_kernel = self.attn_kernels[</code></pre><h3>After Change</h3><pre><code class='java'>
            )

        &#47&#47 Remove singleton batch dimension
        A = <a id="change">K.squeeze(A, 0)</a>
        X = K.squeeze(X, 0)
        out_indices = K.squeeze(out_indices, 0)

        &#47&#47 TODO: move this pre-processing to the generator</code></pre>