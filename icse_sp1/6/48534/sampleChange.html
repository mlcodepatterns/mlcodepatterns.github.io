<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
          tf.contrib.data.batch_and_drop_remainder(batch_size))

      dataset = dataset.prefetch(2)  &#47&#47 Prefetch overlaps in-feed with training
      <a id="change">images, labels = dataset.make_one_shot_iterator().get_next()</a>
    else:
      images = tf.random_uniform(
          [batch_size, self.hparams.image_size,
           self.hparams.image_size, 3], minval=-1, maxval=1)</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 than a full batch size. As long as this validation is done with
    &#47&#47 consistent batch size, exactly the same images will be used.
    if not self.is_training:
      <a id="change">dataset = dataset.apply(batching.filter_irregular_batches(batch_size))</a>

    dataset = <a id="change">dataset.map(
        lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),
        num_parallel_calls=8)</a>

    &#47&#47 For XLA, we must used fixed shapes. Because we repeat the source training
    &#47&#47 dataset indefinitely, this is not a dangerous operation.
    &#47&#47
    &#47&#47 When evaluating, prevent accidentally evaluating the same image twice by
    &#47&#47 dropping the final batch if it is less than a full batch size. As long as
    &#47&#47 this validation is done with consistent batch size, exactly the same
    &#47&#47 images will be used.
    def set_shapes(images, labels):
      images.set_shape(images.get_shape().merge_with(
          tf.TensorShape([None, None, None, batch_size])))
      labels.set_shape(labels.get_shape().merge_with(
          tf.TensorShape([batch_size])))
      return images, labels

    if self.is_training:
      <a id="change">dataset = dataset.map(set_shapes)</a>

    dataset = dataset.prefetch(32)  &#47&#47 Prefetch overlaps in-feed with training
    return dataset  &#47&#47 Must return the dataset and not tensors for high perf!
</code></pre>