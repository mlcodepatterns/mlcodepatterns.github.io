<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if len(feature_names):
            print(&quotFeature ranking (top 20 features):&quot)
            indices = np.argsort(self.clf.feature_importances_)[::-1][:20]
            <a id="change">for i in range(len(indices)):
                print(&quot{}. \&quot{}\&quot ({})&quot.format(i + 1, feature_names[indices[i]], self.clf.feature_importances_[indices[i]]))

       </a> y_pred = self.clf.predict(X_test)

        print(&quotNo confidence threshold - {} classified&quot.format(len(y_test)))
        print(metrics.confusion_matrix(y_test, y_pred, labels=class_names))</code></pre><h3>After Change</h3><pre><code class='java'>

        feature_names = self.get_feature_names()
        if len(feature_names):
            <a id="change">explainer = shap.TreeExplainer(self.clf)</a>
            shap_values = explainer.shap_values(X_train)

            print(&quotFeature ranking (top 20 features):&quot)
            &#47&#47 Calculate the values that represent the fraction of the model output variability attributable
            &#47&#47 to each feature across the whole dataset.
            shap_sums = np.abs(shap_values).sum(0)
            rel_shap_sums = shap_sums / shap_sums.sum()
            <a id="change">indices = np.argsort(rel_shap_sums)[::-1][:20]</a>
            <a id="change">for i, index in enumerate(indices):
                print(&quot{}. \&quot{}\&quot ({})&quot.format(i + 1, feature_names[index], rel_shap_sums[index]))

       </a> y_pred = self.clf.predict(X_test)

        print(&quotNo confidence threshold - {} classified&quot.format(len(y_test)))
        print(metrics.confusion_matrix(y_test, y_pred, labels=class_names))</code></pre>