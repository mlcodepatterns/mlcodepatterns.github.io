<html><h3>e9aea97df1dc7878827ac193ba75cbea0b3ee351,ludwig/models/modules/sequence_decoders.py,SequenceGeneratorDecoder,__init__,#SequenceGeneratorDecoder#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,32
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if attention_mechanism:
            if attention_mechanism == &quotbahdanau&quot:
                pass
            elif <a id="change">attention_mechanism</a> == &quotluong&quot:
                self.attention_mechanism = tfa.seq2seq.LuongAttention(
                    state_size,
                    None,  &#47&#47 todo tf2: confirm on need
                    memory_sequence_length=max_sequence_length  &#47&#47 todo tf2: confirm inputs or output seq length
                )
            else:
                <a id="change">raise ValueError(
                    "Attention specificaiton &quot{}&quot is invalid.  Valid values are "
                    "&quotbahdanau&quot or &quotluong&quot.".format(self.attention_mechanism))</a>

            self.decoder_cell = tfa.seq2seq.AttentionWrapper(
                self.decoder_cell,
                self.attention_mechanism</code></pre><h3>After Change</h3><pre><code class='java'>
        self.decoder_embedding = tf.keras.layers.Embedding(
            input_dim=output_vocab_size,
            output_dim=embedding_dims)
        self.dense_layer = <a id="change">tf.keras.layers.Dense(output_vocab_size)</a>
        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)

        &#47&#47 Sampler
        self.sampler = tfa.seq2seq.sampler.TrainingSampler()</code></pre><img src="193713689.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/uber/ludwig/commit/e9aea97df1dc7878827ac193ba75cbea0b3ee351#diff-c9939725f3db666ea558a390c6beae039a2f53db3ecf738c5ca12c5ca1667094L1' target='_blank'>Link</a></div><div id='project'> Project Name: uber/ludwig</div><div id='commit'> Commit Name: e9aea97df1dc7878827ac193ba75cbea0b3ee351</div><div id='time'> Time: 2020-05-05</div><div id='author'> Author: jimthompson5802@gmail.com</div><div id='file'> File Name: ludwig/models/modules/sequence_decoders.py</div><div id='class'> Class Name: SequenceGeneratorDecoder</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/f0cecdb5ecce2d1bc555f01df98f61ebd80e4472#diff-e85b85a07669677217d7c0926f9a3165f6f29feeaadb1137b3adecc2dbea7123L28' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: f0cecdb5ecce2d1bc555f01df98f61ebd80e4472</div><div id='time'> Time: 2019-01-15</div><div id='author'> Author: boris.ginsburg@gmail.com</div><div id='file'> File Name: open_seq2seq/parts/transformer/attention_layer.py</div><div id='class'> Class Name: Attention</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/privacy/commit/751eaead545d45bcc47bff7d82656b08c474b434#diff-a164c9fd8be99bb805fbbb153d7df88c191ba454cad0d1705e82715206f717fdL104' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/privacy</div><div id='commit'> Commit Name: 751eaead545d45bcc47bff7d82656b08c474b434</div><div id='time'> Time: 2019-06-10</div><div id='author'> Author: choquette.christopher@gmail.com</div><div id='file'> File Name: privacy/bolton/model.py</div><div id='class'> Class Name: Bolton</div><div id='method'> Method Name: compile</div><BR>