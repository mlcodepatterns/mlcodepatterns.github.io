<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        <a id="change">std = ((x - mean).pow(2).sum(-1, keepdim=True).div(x.size(-1) - 1) + self.eps).sqrt()</a>
        d = (std + self.eps) + self.b
        return self.a * (x - mean) / d

</code></pre><h3>After Change</h3><pre><code class='java'>
    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the
    sub-sequent `FFN` sub-layer.

    There are 3 uses<a id="change"> of multi-head </a>attention in the Transformer.
    For encoder-decod<a id="change">er layers, the queries come from the </a>previous decoder layer, and the me<a id="change">mory keys come from
    the encoder.  For encod</a>er layers, the K, Q and V all come from the output of the previous layer of the encoder.
    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using
    future values
    </code></pre>