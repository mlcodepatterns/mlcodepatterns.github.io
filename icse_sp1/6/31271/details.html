<html><h3>78a7f7de24c34235d0784a5781f46de34d2336eb,python/eight_mile/pytorch/layers.py,LayerNorm,forward,#LayerNorm#Any#,408
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        <a id="change">std = ((x - mean).pow(2).sum(-1, keepdim=True).div(x.size(-1) - 1) + self.eps).sqrt()</a>
        d = (std + self.eps) + self.b
        return self.a * (x - mean) / d

</code></pre><h3>After Change</h3><pre><code class='java'>
    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the
    sub-sequent `FFN` sub-layer.

    There are 3 uses<a id="change"> of multi-head </a>attention in the Transformer.
    For encoder-decod<a id="change">er layers, the queries come from the </a>previous decoder layer, and the me<a id="change">mory keys come from
    the encoder.  For encod</a>er layers, the K, Q and V all come from the output of the previous layer of the encoder.
    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using
    future values
    </code></pre><img src="153849899.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dpressel/mead-baseline/commit/78a7f7de24c34235d0784a5781f46de34d2336eb#diff-c41fa94148789d4fdd61157e7ff95333b730d8f4e1f417ecfcec8007cafc71caL408' target='_blank'>Link</a></div><div id='project'> Project Name: dpressel/mead-baseline</div><div id='commit'> Commit Name: 78a7f7de24c34235d0784a5781f46de34d2336eb</div><div id='time'> Time: 2019-10-29</div><div id='author'> Author: dpressel@gmail.com</div><div id='file'> File Name: python/eight_mile/pytorch/layers.py</div><div id='class'> Class Name: LayerNorm</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/4e43734dd0b7f1c026069af64151a8f52f41060d#diff-82ea677a2863fbf09ce4b99505c168e4cd69829d4e17e559d6737eb36d94f234L86' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: 4e43734dd0b7f1c026069af64151a8f52f41060d</div><div id='time'> Time: 2019-07-03</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: torch_geometric/nn/conv/gat_conv.py</div><div id='class'> Class Name: GATConv</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/stanfordnlp/stanza/commit/fcea9fee573e854177b4b9af1cfd1b20029ed21e#diff-90d2eeebd9deb2053bf9ddfa87fce659e52d89df2fa817750979c7cfb1b4be15L25' target='_blank'>Link</a></div><div id='project'> Project Name: stanfordnlp/stanza</div><div id='commit'> Commit Name: fcea9fee573e854177b4b9af1cfd1b20029ed21e</div><div id='time'> Time: 2018-10-11</div><div id='author'> Author: qipeng@users.noreply.github.com</div><div id='file'> File Name: models/common/char_model.py</div><div id='class'> Class Name: CharacterModel</div><div id='method'> Method Name: forward</div><BR>