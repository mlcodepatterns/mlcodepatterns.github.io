<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)

&#47&#47 Create random Tensors for weights, and wrap them in Variables.
w1 = Variable(<a id="change">torch.randn(D_in, H).type(dtype)</a>, requires_grad=True)
w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)

learning_rate = 1e-6</code></pre><h3>After Change</h3><pre><code class='java'>

&#47&#47 Create random Tensors to hold input and outputs.
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = <a id="change">torch.randn(N, D_out, device=device, dtype=dtype)</a>

&#47&#47 Create random Tensors for weights.
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = <a id="change">torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)</a>

learning_rate = 1e-6
for t in range(500):
    &#47&#47 To apply our Function, we use Function.apply method. We alias this as &quotrelu&quot.
    relu = MyReLU.apply

    &#47&#47 Forward pass: compute predicted y using operations; we compute
    &#47&#47 ReLU using our custom autograd operation.
    y_pred = relu(x.mm(w1)).mm(w2)

    &#47&#47 Compute and print loss
    loss = (y_pred - y).pow(2).sum()
    print(t, <a id="change">loss.item()</a>)

    &#47&#47 Use autograd to compute the backward pass.
    loss.backward()</code></pre>