<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

	&#47&#47 Fast forward the input to the position where it was left last time.
	for i in range(trainer.update_number):
		<a id="change">next(training_iter)</a>

	while trainer.update_minibatch(
			training_iter,
			args.max_epochs,</code></pre><h3>After Change</h3><pre><code class='java'>
	saved_params = None
	saved_cost = None

	<a id="change">while trainer.epoch_number &lt;= args.max_epochs:
		print("Creating a random permutation of %d training sentences." % len(sentence_starts))
		training_order = numpy.random.permutation(sentence_starts)
		training_iter = theanolm.OrderedBatchIterator(
				args.training_file, 
				dictionary,
				training_order,
				batch_size=args.batch_size,
				max_sequence_length=args.sequence_length)
		
		while trainer.update_minibatch(training_iter, args.learning_rate):
			if (args.verbose_interval &gt;= 1) and \
			   (trainer.total_updates % args.verbose_interval == 0):
				trainer.print_update_stats()
	
			if (args.validation_interval &gt;= 1) and \
			   (trainer.total_updates % args.validation_interval == 0):
				validation_cost = scorer.negative_log_probability(validation_iter)
				if numpy.isnan(validation_cost):
					print("Stopping because an invalid floating point operation was performed while computing validation set cost. (Gradients exploded or vanished?)")
					return best_params
				if numpy.isinf(validation_cost):
					print("Stopping because validation set cost exploded to infinity.")
					return best_params
	
				trainer.append_validation_cost(validation_cost)
				validations_since_best = trainer.validations_since_min_cost()
				if validations_since_best == 0:
					best_params = rnnlm.get_state()
				elif (args.wait_improvement &gt;= 0) and \
				     (validations_since_best &gt; args.wait_improvement):
					print("Stopping because validation set cost did not decrease in previous %d validations." % (args.wait_improvement + 1))
					return best_params
	
			if (args.save_interval &gt;= 1) and \
			   (trainer.total_updates % args.save_interval == 0):
				&#47&#47 Save the best parameters and the current state.
				if not best_params is None:
					save_model(args.model_path, best_params)
				save_training_state(args.state_path)

</a>	print("Stopping because %d epochs was reached." % args.max_epochs)
	validation_cost = scorer.negative_log_probability(validation_iter)
	trainer.append_validation_cost(validation_cost)
	if trainer.validations_since_min_cost() == 0:</code></pre>