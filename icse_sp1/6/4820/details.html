<html><h3>01568d0d6eb789dc0a026d28bd232958babee07f,opennmt/decoders/self_attention_decoder.py,SelfAttentionDecoder,_self_attention_stack,#SelfAttentionDecoder#Any#Any#Any#Any#Any#,42
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
          if memory is None:
            values = encoded
          elif tf.contrib.framework.nest.is_sequence(memory):
            <a id="change">if l &gt;= len(memory):
              raise ValueError(If the encoder memory is a sequence,
                               it must contain one memory per decoder layer)
           </a> values = memory[l]
          else:
            values = memory
          keys = values</code></pre><h3>After Change</h3><pre><code class='java'>
    for l in range(self.num_layers):
      with tf.variable_scope("layer_{}".format(l)):
        with tf.variable_scope("masked_multi_head"):
          inputs_norm = <a id="change">transformer.norm(inputs)</a>
          <a id="change">encoded = transformer.multi_head_attention(
              self.num_heads,
              inputs_norm,
              inputs_norm,
              inputs_norm,
              mode,
              values_length=sequence_length,
              mask_future=True,
              dropout=self.attention_dropout)</a>
          encoded = transformer.drop_and_add(
              inputs,
              encoded,
              mode,
              dropout=self.dropout)

        with tf.variable_scope("multi_head"):
          values = memory if memory is not None else encoded
          keys = values

          context = transformer.multi_head_attention(
              self.num_heads,
              transformer.norm(encoded),
              keys,
              values,
              mode,
              values_length=memory_sequence_length,
              dropout=self.attention_dropout)
          context = transformer.drop_and_add(
              encoded,
              context,
              mode,
              dropout=self.dropout)

        with tf.variable_scope("ffn"):
          transformed = transformer.feed_forward(
              transformer.norm(context),
              self.ffn_inner_dim,
              mode,
              dropout=self.relu_dropout)
          transformed = transformer.drop_and_add(
              context,
              transformed,
              mode,
              dropout=self.dropout)

        inputs = transformed

    <a id="change">outputs = transformer.norm(inputs)</a>
    return outputs

  def decode(self,
             inputs,</code></pre><img src="30908279.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-tf/commit/01568d0d6eb789dc0a026d28bd232958babee07f#diff-c60905dae72132d03a5b0c8b514b10766b681565075afe76acedd57537c3f622L46' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-tf</div><div id='commit'> Commit Name: 01568d0d6eb789dc0a026d28bd232958babee07f</div><div id='time'> Time: 2017-11-06</div><div id='author'> Author: guillaumekln@users.noreply.github.com</div><div id='file'> File Name: opennmt/decoders/self_attention_decoder.py</div><div id='class'> Class Name: SelfAttentionDecoder</div><div id='method'> Method Name: _self_attention_stack</div><BR><BR><div id='link'><a href='https://github.com/geomstats/geomstats/commit/bee606d2a668d462c9eea4cc23d7cfd8fbeb36a3#diff-0e7d1ffda02753c05cba4add6c203a1a3b9cfca9daef95296d3a82ad52f2e604L332' target='_blank'>Link</a></div><div id='project'> Project Name: geomstats/geomstats</div><div id='commit'> Commit Name: bee606d2a668d462c9eea4cc23d7cfd8fbeb36a3</div><div id='time'> Time: 2020-01-15</div><div id='author'> Author: hadizaatiti@gmail.com</div><div id='file'> File Name: geomstats/geometry/hyperbolic_space.py</div><div id='class'> Class Name: HyperbolicMetric</div><div id='method'> Method Name: log</div><BR><BR><div id='link'><a href='https://github.com/keras-team/keras/commit/11685f68d52266188abc117687493c80a5580718#diff-b33e355dd27235d2b7b60307276fae5c0630d0663baca2d3dd0288fcf9969cd1L392' target='_blank'>Link</a></div><div id='project'> Project Name: keras-team/keras</div><div id='commit'> Commit Name: 11685f68d52266188abc117687493c80a5580718</div><div id='time'> Time: 2015-06-25</div><div id='author'> Author: lchen3@gmail.com</div><div id='file'> File Name: keras/layers/recurrent.py</div><div id='class'> Class Name: MutatedRNN_1</div><div id='method'> Method Name: __init__</div><BR>