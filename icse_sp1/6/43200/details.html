<html><h3>700abc65fd2172a2c6809dd9b72cf50fc2407772,allennlp/models/encoder_decoders/composed_seq2seq.py,ComposedSeq2Seq,__init__,#ComposedSeq2Seq#Any#Any#Any#Any#Any#Any#Any#,48
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                )

            source_embedder = self._source_text_embedder._token_embedders[tied_source_embedder_key]
            <a id="change">if not isinstance(source_embedder, Embedding):
                raise ConfigurationError(
                    "Unable to tie embeddings,"
                    "Selected source embedder is not an instance of `Embedding`."
                )
           </a> if source_embedder.get_output_dim() != self._decoder.target_embedder.get_output_dim():
                raise ConfigurationError(
                    f"Output Dimensions mismatch between" f"source embedder and target embedder."
                )</code></pre><h3>After Change</h3><pre><code class='java'>
        encoder: Seq2SeqEncoder,
        decoder: SeqDecoder,
        tied_source_embedder_key: Optional[str] = None,
        initializer: InitializerApplicator = InitializerApplicator()<a id="change">,
        **kwargs,
    ) -&gt; None:

        super().__init__(vocab, **kwargs)

        self._source_text_embedder = source_text_embedder
        self._encoder = encoder
        self._decoder = decoder

        if self._encoder.get_output_dim() != self._decoder.get_output_dim():
            raise ConfigurationError(
   </a>             f"Encoder output dimension {self._encoder.get_output_dim()} should be"
                f" equal to decoder dimension {self._decoder.get_output_dim()}."
            )
        if tied_source_embedder_key:</code></pre><img src="202776822.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/700abc65fd2172a2c6809dd9b72cf50fc2407772#diff-72ee7e31870989d3f338643934daa891065889ef15d85ff22ee1e91db8472f5dL40' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 700abc65fd2172a2c6809dd9b72cf50fc2407772</div><div id='time'> Time: 2020-02-03</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: allennlp/models/encoder_decoders/composed_seq2seq.py</div><div id='class'> Class Name: ComposedSeq2Seq</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/ce060badd12d3047e3af81cf97d0b62805e397e5#diff-cad89872ab3c3193655302987028940da9f907382ec05d34a006fd85ea622fe3L185' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: ce060badd12d3047e3af81cf97d0b62805e397e5</div><div id='time'> Time: 2018-12-20</div><div id='author'> Author: brendanr@allenai.org</div><div id='file'> File Name: allennlp/models/bidirectional_lm.py</div><div id='class'> Class Name: BidirectionalLanguageModel</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/a58eb2cc4ab506f1611c06a308cb6a2ae710707b#diff-5db2f703cfbeaf284f3f0549054d15c1d433c2618ce554d1329c5baffa6f4c2fL327' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: a58eb2cc4ab506f1611c06a308cb6a2ae710707b</div><div id='time'> Time: 2017-09-26</div><div id='author'> Author: joelgrus@gmail.com</div><div id='file'> File Name: allennlp/training/trainer.py</div><div id='class'> Class Name: Trainer</div><div id='method'> Method Name: _restore_checkpoint</div><BR>