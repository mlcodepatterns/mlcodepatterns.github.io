<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    with self.eval_graph.graph.as_default():
      &#47&#47 run eval data through the model
      n_tasks = self.n_tasks
      <a id="change">with self._get_shared_session(train=False).as_default():
        feed_dict = self.construct_feed_dict(X)
        data = self._get_shared_session(train=False).run(
            self.eval_graph.output, feed_dict=feed_dict)
        batch_outputs = np.asarray(data[:n_tasks], dtype=float)
        &#47&#47 reshape to batch_size x n_tasks x ...
        if batch_outputs.ndim == 3:
          batch_outputs = batch_outputs.transpose((1, 0, 2))
        elif batch_outputs.ndim == 2:
          batch_outputs = batch_outputs.transpose((1, 0))
        else:
          raise ValueError(
              &quotUnrecognized rank combination for output: %s &quot %
              (batch_outputs.shape,))

      &#47&#47 Note that softmax is already applied in construct_grpah
     </a> outputs = batch_outputs

    return np.copy(outputs)
</code></pre><h3>After Change</h3><pre><code class='java'>

          &#47&#47 weight decay
          if self.penalty != 0.0:
<a id="change">            penalty = model_ops.weight_decay(self.penalty_type, self.penalty)
            loss += penalty

      return loss 

  def fit(self, dataset, nb_epoch=</a>10, max_checkpoints_to_keep=5, 
	  log_every_N_batches=50, **kwargs):
    Fit the model.
</code></pre>