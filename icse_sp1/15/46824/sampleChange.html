<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    @lab_api
    def sample(self):
        &#47&#47 NOTE the purpose of multi-body is to parallelize and get more batch_sizes
        batches = <a id="change">[body.memory.sample() for body in self.agent.nanflat_body_a]</a>
        &#47&#47 Package data into pytorch variables
        for batch_b in batches:
            util.to_torch_batch(batch_b, self.net.gpu)
        &#47&#47 Concat state
        <a id="change">combined_states = torch.cat(
            [batch_b[&quotstates&quot] for batch_b in batches], dim=1)</a>
        combined_next_states = torch.cat(
            [batch_b[&quotnext_states&quot] for batch_b in batches], dim=1)
        batch = {&quotstates&quot: combined_states, &quotnext_states&quot: combined_next_states}
        &#47&#47 use recursive packaging to carry sub data</code></pre><h3>After Change</h3><pre><code class='java'>
        Samples a batch from memory.
        Note that multitask&quots bodies are parallelized copies with similar envs, just to get more batch sizes
        &quot&quot&quot
        <a id="change">batches = []</a>
        <a id="change">for body in self.agent.nanflat_body_a:
            body_batch = body.memory.sample()
            &#47&#47 one-hot actions to calc q_targets
            if body.is_discrete:
                body_batch[&quotactions&quot] = util.to_one_hot(body_batch[&quotactions&quot], body.action_space.high)
            body_batch = util.to_torch_batch(body_batch, self.net.gpu)
            batches.append(body_batch)
        &#47&#47 Concat states at dim=1 for feedforward
       </a> batch = {
            &quotstates&quot: torch.cat([body_batch[&quotstates&quot] for body_batch in batches], dim=1),
            &quotnext_states&quot: torch.cat([body_batch[&quotnext_states&quot] for body_batch in batches], dim=1),
        }</code></pre>