<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
epochs = 3 &#47&#47 The number of epochs
best_model = None

<a id="change">for epoch in range(1, epochs + 1):
    epoch_start_time = time.time()
    train()
    val_loss = evaluate(model, val_data)
    print(&quot-&quot * 89)
    print(&quot| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | &quot
          &quotvalid ppl {:8.2f}&quot.format(epoch, (time.time() - epoch_start_time),
                                     val_loss, math.exp(val_loss)))
    print(&quot-&quot * 89)

    if val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        best_model = model

    scheduler.step()


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Evaluate the model with the test dataset
&#47&#47 -------------------------------------
&#47&#47
&#47&#47 Apply the best model to check the result with the test dataset.

</a>test_loss = evaluate(best_model, test_data)
print(&quot=&quot * 89)
print(&quot| End of training | test loss {:5.2f} | test ppl {:8.2f}&quot.format(
    test_loss, math.exp(test_loss)))</code></pre><h3>After Change</h3><pre><code class='java'>
nlayers = 2 &#47&#47 the number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead = 2 &#47&#47 the number of heads in the multiheadattention models
dropout = 0.2 &#47&#47 the dropout value
<a id="change">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</a>


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Run the model
&#47&#47 -------------
&#47&#47


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 `CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss&#47&#47torch.nn.CrossEntropyLoss&gt;`__
&#47&#47 is applied to track the loss and
&#47&#47 `SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd&#47&#47torch.optim.SGD&gt;`__
&#47&#47 implements stochastic gradient descent method as the optimizer. The initial
&#47&#47 learning rate is set to 5.0. `StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr&#47&#47torch.optim.lr_scheduler.StepLR&gt;`__ is
&#47&#47 applied to adjust the learn rate through epochs. During the
&#47&#47 training, we use
&#47&#47 `nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm&#47&#47torch.nn.utils.clip_grad_norm_&gt;`__
&#47&#47 function to scale all the gradient together to prevent exploding.
&#47&#47

criterion = nn.CrossEntropyLoss()
lr = 5.0 &#47&#47 learning rate
<a id="change">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</a>
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

import time
def train():</code></pre>