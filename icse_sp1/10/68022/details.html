<html><h3>d1e6e624ef891543c5bba32bb0a696d350714693,coremltools/models/utils.py,,_convert_nn_spec_to_half_precision,#Any#,172
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    for layer in layers:
        layer_type = layer.WhichOneof(&quotlayer&quot)

        <a id="change">if layer_type in ignored_layers:
            continue

        &#47&#47 Convolution
       </a> if layer_type == &quotconvolution&quot:
            _wp_to_fp16wp(layer.convolution.weights)
            if layer.convolution.hasBias:
                _wp_to_fp16wp(layer.convolution.bias)

        &#47&#47 Batchnorm
        elif layer_type == &quotbatchnorm&quot:
            _wp_to_fp16wp(layer.batchnorm.gamma)
            _wp_to_fp16wp(layer.batchnorm.beta)
            _wp_to_fp16wp(layer.batchnorm.mean)
            _wp_to_fp16wp(layer.batchnorm.variance)

        &#47&#47 InnerProduct
        elif layer_type == &quotinnerProduct&quot:
            _wp_to_fp16wp(layer.innerProduct.weights)
            if layer.innerProduct.hasBias:
                _wp_to_fp16wp(layer.innerProduct.bias)

        &#47&#47 batchedMatmul
        elif layer_type == &quotbatchedMatmul&quot:
            _wp_to_fp16wp(layer.batchedMatmul.weights)
            if layer.batchedMatmul.hasBias:
                _wp_to_fp16wp(layer.batchedMatmul.bias)

        &#47&#47 Embedding layer
        elif layer_type == &quotembedding&quot:
            _wp_to_fp16wp(layer.embedding.weights)
            if layer.embedding.hasBias:
                _wp_to_fp16wp(layer.embedding.bias)

        &#47&#47 EmbeddingND layer
        elif layer_type == &quotembeddingND&quot:
            _wp_to_fp16wp(layer.embeddingND.weights)
            if layer.embeddingND.hasBias:
                _wp_to_fp16wp(layer.embeddingND.bias)

        &#47&#47 Scale layer
        elif layer_type == &quotscale&quot:
            _wp_to_fp16wp(layer.scale.scale)
            if layer.scale.hasBias:
                _wp_to_fp16wp(layer.scale.bias)

        &#47&#47 Bias layer
        elif layer_type == &quotbias&quot:
            _wp_to_fp16wp(layer.bias.bias)

        &#47&#47 LoadConstant layer
        elif layer_type == &quotloadConstant&quot:
            _wp_to_fp16wp(layer.loadConstant.data)

        &#47&#47 Activation layer
        elif layer_type == &quotactivation&quot:
            activation_type = layer.activation.WhichOneof(&quotNonlinearityType&quot)
            if activation_type == &quotPReLU&quot:
                _wp_to_fp16wp(layer.activation.PReLU.alpha)
            elif activation_type == &quotparametricSoftplus&quot:
                _wp_to_fp16wp(layer.activation.parametricSoftplus.alpha)
                _wp_to_fp16wp(layer.activation.parametricSoftplus.beta)

        &#47&#47 Simple Recurrent
        elif layer_type == &quotsimpleRecurrent&quot:
            _wp_to_fp16wp(layer.simpleRecurrent.weightMatrix)
            _wp_to_fp16wp(layer.simpleRecurrent.recursionMatrix)
            if layer.simpleRecurrent.hasBiasVector:
                _wp_to_fp16wp(layer.simpleRecurrent.biasVector)

        &#47&#47 GRU
        elif layer_type == &quotgru&quot:
            &#47&#47 Weight Matrix
            _wp_to_fp16wp(layer.gru.updateGateWeightMatrix)
            _wp_to_fp16wp(layer.gru.resetGateWeightMatrix)
            _wp_to_fp16wp(layer.gru.outputGateWeightMatrix)

            &#47&#47 Recursion Weights
            _wp_to_fp16wp(layer.gru.updateGateRecursionMatrix)
            _wp_to_fp16wp(layer.gru.resetGateRecursionMatrix)
            _wp_to_fp16wp(layer.gru.outputGateRecursionMatrix)

            if layer.gru.hasBiasVectors:
                _wp_to_fp16wp(layer.gru.updateGateBiasVector)
                _wp_to_fp16wp(layer.gru.resetGateBiasVector)
                _wp_to_fp16wp(layer.gru.outputGateBiasVector)

        &#47&#47 LSTM Layers
        elif layer_type in [&quotuniDirectionalLSTM&quot, &quotbiDirectionalLSTM&quot]:

            def _lstmwp_to_fp16_lstmwp(lstm_wp, has_peephole=True):
                assert lstm_wp
                _wp_to_fp16wp(lstm_wp.inputGateWeightMatrix)
                _wp_to_fp16wp(lstm_wp.forgetGateWeightMatrix)
                _wp_to_fp16wp(lstm_wp.blockInputWeightMatrix)
                _wp_to_fp16wp(lstm_wp.outputGateWeightMatrix)

                _wp_to_fp16wp(lstm_wp.inputGateRecursionMatrix)
                _wp_to_fp16wp(lstm_wp.forgetGateRecursionMatrix)
                _wp_to_fp16wp(lstm_wp.blockInputRecursionMatrix)
                _wp_to_fp16wp(lstm_wp.outputGateRecursionMatrix)

                _wp_to_fp16wp(lstm_wp.inputGateBiasVector)
                _wp_to_fp16wp(lstm_wp.forgetGateBiasVector)
                _wp_to_fp16wp(lstm_wp.blockInputBiasVector)
                _wp_to_fp16wp(lstm_wp.outputGateBiasVector)

                if has_peephole:
                    _wp_to_fp16wp(lstm_wp.inputGatePeepholeVector)
                    _wp_to_fp16wp(lstm_wp.forgetGatePeepholeVector)
                    _wp_to_fp16wp(lstm_wp.outputGatePeepholeVector)

            if layer_type == &quotuniDirectionalLSTM&quot:
                _lstmwp_to_fp16_lstmwp(
                    lstm_wp=layer.uniDirectionalLSTM.weightParams,
                    has_peephole=layer.uniDirectionalLSTM.params.hasPeepholeVectors
                )
            elif layer_type == &quotbiDirectionalLSTM&quot:
                for lstm_wp in layer.biDirectionalLSTM.weightParams:
                    _lstmwp_to_fp16_lstmwp(
                        lstm_wp=lstm_wp,
                        has_peephole=layer.biDirectionalLSTM.params.hasPeepholeVectors
                    )

        elif layer_type == &quotcustom&quot:
            print (&quotSkipping custom layer {}. Weights for this layer need to&quot
                   &quotbe converted manually&quot.format(layer.name))
            continue

        elif <a id="change">layer_type</a> in quantized_layers:
            <a id="change">raise Exception(&quotHalf precision for &quot + layer_type +
                            &quot not yet implemented\n&quot)</a>
        else:
            raise Exception(&quotUnknown layer &quot + layer_type)

    return spec</code></pre><h3>After Change</h3><pre><code class='java'>
                  &quotbe converted manually&quot.format(layer.name))
            continue

        <a id="change">if layer_type not in quantized_layers:
            params = getattr(layer, layer_type, None)
            params = params.ListFields() if params else []
            param_types = [p[0].message_type.name if p[0].message_type else None for p in params]

            if &quotWeightParams&quot in param_types:
                raise NotImplementedError(&quotQuantization for layer "&quot + layer_type + &quot" not implemented.&quot)
            continue  &#47&#47 print(&quotSkipping layer {}. No need to quantize.&quot.format(layer.name))

        &#47&#47 Convolution
       </a> if layer_type == &quotconvolution&quot:
            _wp_to_fp16wp(layer.convolution.weights)
            if layer.convolution.hasBias:
                _wp_to_fp16wp(layer.convolution.bias)</code></pre><img src="312191563.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/apple/coremltools/commit/d1e6e624ef891543c5bba32bb0a696d350714693#diff-e401ca9a79a05199664b882927b7fa56d795167e9b0fb5e5630364910a7a6944L173' target='_blank'>Link</a></div><div id='project'> Project Name: apple/coremltools</div><div id='commit'> Commit Name: d1e6e624ef891543c5bba32bb0a696d350714693</div><div id='time'> Time: 2019-08-20</div><div id='author'> Author: smq@apple.com</div><div id='file'> File Name: coremltools/models/utils.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: _convert_nn_spec_to_half_precision</div><BR><BR><div id='link'><a href='https://github.com/keras-team/keras/commit/ee8ff00a2a8a307c952fb8e7bef241188c7fb12b#diff-8d37272f22c146505301f9d7f2a729d60681f649b77fea252346332e7ac31327L1059' target='_blank'>Link</a></div><div id='project'> Project Name: keras-team/keras</div><div id='commit'> Commit Name: ee8ff00a2a8a307c952fb8e7bef241188c7fb12b</div><div id='time'> Time: 2016-07-03</div><div id='author'> Author: francois.chollet@gmail.com</div><div id='file'> File Name: keras/backend/tensorflow_backend.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pool2d</div><BR><BR><div id='link'><a href='https://github.com/acl-org/acl-anthology/commit/7419eacac2dfa909b280881524e685d7ea4d7ec7#diff-7db43524af3373c6565669485bfc7fb0dd98fd8e91e86bbce9e200e07b445e3aL62' target='_blank'>Link</a></div><div id='project'> Project Name: acl-org/acl-anthology</div><div id='commit'> Commit Name: 7419eacac2dfa909b280881524e685d7ea4d7ec7</div><div id='time'> Time: 2020-04-24</div><div id='author'> Author: post@cs.jhu.edu</div><div id='file'> File Name: bin/add_attachments.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: add_attachment</div><BR>