<html><h3>68fbfd1876c367323acf830736bae1af499cc0fe,onmt/modules/Transformer.py,TransformerDecoder,forward,#TransformerDecoder#Any#Any#Any#Any#,262
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        outputs = output.transpose(0, 1).contiguous()
        if state.previous_input is not None:
            outputs = outputs[state.previous_input.size(0):]
            <a id="change">attn = attn[:, state.previous_input.size(0):].squeeze()</a>
            <a id="change">attn = torch.stack([attn])</a>
        attns["std"] = attn
        if self._copy:
            attns["copy"] = attn
</code></pre><h3>After Change</h3><pre><code class='java'>
        tgt_pad_mask = tgt_words.data.eq(padding_idx).unsqueeze(1) \
            .expand(tgt_batch, tgt_len, tgt_len)

        <a id="change">saved_inputs = []</a>
        for i in range(self.num_layers):
            prev_layer_input = None
            <a id="change">if state.previous_input is not None:
                prev_layer_input = state.previous_layer_inputs[i]
           </a> output, attn, all_input \
                = self.transformer_layers[i](output, src_memory_bank,
                                             src_pad_mask, tgt_pad_mask,
                                             previous_input=prev_layer_input)
            <a id="change">saved_inputs.append(all_input)</a>

        <a id="change">saved_inputs = torch.stack(saved_inputs)</a>
        output = self.layer_norm(output)
        
        &#47&#47 Process the result and update the attentions.
        outputs = output.transpose(0, 1).contiguous()</code></pre><img src="257530665.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-py/commit/68fbfd1876c367323acf830736bae1af499cc0fe#diff-ddc9a22e1f7ac3bba1afe68bb2934687c850ec654820ee7d7c8438901a1c957eL272' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-py</div><div id='commit'> Commit Name: 68fbfd1876c367323acf830736bae1af499cc0fe</div><div id='time'> Time: 2018-03-07</div><div id='author'> Author: dengyuntian@gmail.com</div><div id='file'> File Name: onmt/modules/Transformer.py</div><div id='class'> Class Name: TransformerDecoder</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/80cfb0e5f889c65a972ebde6c6dae4278b5e28c1#diff-2e76868a2403842075a80eb5d839820a2539b393891dacd357ee4541788b4e5dL341' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: 80cfb0e5f889c65a972ebde6c6dae4278b5e28c1</div><div id='time'> Time: 2020-02-14</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/attacks/base.py</div><div id='class'> Class Name: MinimizationAttack</div><div id='method'> Method Name: __call__</div><BR><BR><div id='link'><a href='https://github.com/fgnt/pb_bss/commit/2cfa9aa576ae7544f76e66854edd304690a5822b#diff-e7d0b909b1c87fcf10136cc84ffd6e386ba47c4736de8b589198d37e6062cf74L372' target='_blank'>Link</a></div><div id='project'> Project Name: fgnt/pb_bss</div><div id='commit'> Commit Name: 2cfa9aa576ae7544f76e66854edd304690a5822b</div><div id='time'> Time: 2019-08-08</div><div id='author'> Author: mail@lukas-drude.de</div><div id='file'> File Name: paderbox/speech_enhancement/beamformer_wrapper.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_multi_source_bf_vector_from_masks</div><BR>