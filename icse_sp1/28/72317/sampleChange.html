<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
  @flagsaver.flagsaver
  def benchmark_8_gpu_coco(self):
    Run RetinaNet model accuracy test with 8 GPUs.
    <a id="change">self.num_gpus = 8</a>
    self._setup()
    params = <a id="change">copy.deepcopy(self.params_override)</a>
    params[&quottrain&quot][&quottotal_steps&quot] = 1875  &#47&#47 One epoch.
    &#47&#47 The iterations_per_loop must be one, otherwise the number of examples per
    &#47&#47 second would be wrong. Currently only support calling callback per batch
    &#47&#47 when each loop only runs on one batch, i.e. host loop for one step. The
    &#47&#47 performance of this situation might be lower than the case of
    &#47&#47 iterations_per_loop &gt; 1.
    &#47&#47 Related bug: b/135933080
    params[&quottrain&quot][&quotiterations_per_loop&quot] = 1
    params[&quoteval&quot][&quoteval_samples&quot] = 8
    <a id="change">FLAGS.num_gpus</a> = <a id="change">self.num_gpus</a>
    <a id="change">FLAGS.params_override = json.dumps(params)</a>
    FLAGS.model_dir = self._get_model_dir(&quotreal_benchmark_8_gpu_coco&quot)
    &#47&#47 Use negative value to avoid saving checkpoints.
    <a id="change">FLAGS.save_checkpoint_freq</a> = <a id="change">-1</a>
    <a id="change">if self.timer_callback is None:
      logging.error(&quotCannot measure performance without timer callback&quot)
    else:
      self._run_and_report_benchmark()

 </a> @flagsaver.flagsaver
  def benchmark_1_gpu_coco(self):
    Run RetinaNet model accuracy test with 1 GPU.
    self.num_gpus = 1</code></pre><h3>After Change</h3><pre><code class='java'>
  def benchmark_8_gpu_coco(self):
    Run RetinaNet model accuracy test with 8 GPUs.
    self._setup()
    params = <a id="change">self._params()</a>
    params[&quottrain&quot][&quottotal_steps&quot] = 1875  &#47&#47 One epoch.
    &#47&#47 The iterations_per_loop must be one, otherwise the number of examples per
    &#47&#47 second would be wrong. Currently only support calling callback per batch
    &#47&#47 when each loop only runs on one batch, i.e. host loop for one step. The
    &#47&#47 performance of this situation might be lower than the case of
    &#47&#47 iterations_per_loop &gt; 1.
    &#47&#47 Related bug: b/135933080
    params[&quottrain&quot][&quotiterations_per_loop&quot] = 1
    params[&quoteval&quot][&quoteval_samples&quot] = 8
    FLAGS.num_gpus = 8
    FLAGS.model_dir = self._get_model_dir(&quotreal_benchmark_8_gpu_coco&quot)
    FLAGS.strategy_type = &quotmirrored&quot
    <a id="change">self._run_and_report_benchmark(params)</a>

  @flagsaver.flagsaver
  def benchmark_1_gpu_coco(self):
    Run RetinaNet model accuracy test with 1 GPU.</code></pre>