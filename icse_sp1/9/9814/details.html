<html><h3>345fcc9ff27a62015f257186af2941984d6a3cd7,finetune.py,,finetune,#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,312
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        aux_loss_fn = None

    if args.thresh_test_preds:
        <a id="change">thresholds = pd.read_csv(args.thresh_test_preds, header=None).values.squeeze()</a>
    elif len(last_thresholds) &gt; 0:
        &#47&#47 Re-use previous thresholds, if provided.
        &#47&#47 Why? More accurate reporting, and not that slow. Don&quott compute thresholds on training, for example -- but can recycle val threshold
        thresholds = last_thresholds
    else:
        &#47&#47 Default thresholds -- faster, but less accurate
        thresholds = np.array([default_threshold for _ in range(int(model.out_dim/heads_per_class))])

    total_loss = 0
    total_classifier_loss = 0
    total_lm_loss = 0
    total_multihead_variance_loss = 0
    class_accuracies = torch.zeros(model.out_dim).cuda()
    if model.out_dim/heads_per_class &gt; 1 and not args.use_softmax:
        keys = list(args.non_binary_cols)
    elif args.use_softmax:
        keys = [str(m) for m in range(model.out_dim)]
    else:
        keys = [&quot&quot]
    info_dicts = [{&quotfp&quot : 0, &quottp&quot : 0, &quotfn&quot : 0, &quottn&quot : 0, &quotstd&quot : 0,
                   &quotmetric&quot : args.metric, &quotmicro&quot : args.micro} for k in keys]

    &#47&#47 Sanity check -- should do this sooner. Does &#47&#47classes match expected output?
    assert model.out_dim == len(keys) * heads_per_class, "model.out_dim does not match keys (%s) x heads_per_class (%d)" % (keys, heads_per_class)

    batch_adjustment = 1. / len(text)
    &#47&#47 Save all outputs *IF* small enough, and requested for thresholding -- basically, on validation
    &#47&#47if threshold_validation and LR is not None:
    all_batches = []
    all_stds = []
    all_labels = []
    print(&quotRunning %d batches&quot % len(text))
    for i, data in tqdm(enumerate(text), total=len(text), unit="batch", desc=tqdm_desc, position=1, ncols=100):
        text_batch, labels_batch, length_batch = get_supervised_batch(data, args.cuda, model, args.ids, args, heads_per_class=args.heads_per_class)
        class_out, (lm_out, _) = transform(model, text_batch, labels_batch, length_batch, args, LR)
        class_std = None
        if heads_per_class &gt; 1:
            all_heads, class_out, class_std = class_out
            classifier_loss = clf_loss_fn(all_heads, labels_batch)
        else:
            classifier_loss = clf_loss_fn(class_out, labels_batch)
        loss = classifier_loss
        classifier_loss = classifier_loss.clone() &#47&#47 save for reporting
        &#47&#47 Also compute multihead variance loss -- from classifier [divide by output size since it scales linearly]
        if args.aux_head_variance_loss_weight &gt; 0.:
            multihead_variance_loss = model.classifier.get_last_layer_variance() / model.out_dim
            loss = loss + multihead_variance_loss * args.aux_head_variance_loss_weight
        &#47&#47 Divide by &#47&#47 batches? Since we&quotre looking at the parameters here, and should be batch independent.
        &#47&#47 multihead_variance_loss *= batch_adjustment

        if args.aux_lm_loss:
            lm_labels = text_batch[1:]
            lm_losses = aux_loss_fn(lm_out[:-1].view(-1, lm_out.size(2)).contiguous().float(),
                                      lm_labels.contiguous().view(-1))

            padding_mask = (torch.arange(lm_labels.size(0)).unsqueeze(1).cuda() &gt; length_batch).float()
            portion_unpadded = padding_mask.sum() / padding_mask.size(0)
            lm_loss = portion_unpadded * torch.mean(lm_losses * (padding_mask.view(-1).float()))

            &#47&#47 Scale LM loss -- since it&quots so big
            if args.aux_lm_loss_weight &gt; 0.:
                loss = loss + lm_loss * args.aux_lm_loss_weight

        &#47&#47 Training
        if LR is not None:
            LR.optimizer.zero_grad()
            loss.backward()
            LR.optimizer.step()
            LR.step()

        &#47&#47 Remove loss from CUDA -- kill gradients and save memory.
        total_loss += loss.detach().cpu().numpy()
        if args.use_softmax:
            labels_batch = onehot(labels_batch.squeeze(), model.out_dim)
            class_out = onehot(torch.max(class_out, -1)[1].squeeze(), int(model.out_dim/heads_per_class))
        total_classifier_loss += classifier_loss.detach().cpu().numpy()
        if args.aux_lm_loss:
            total_lm_loss += lm_loss.detach().cpu().numpy()
        if args.aux_head_variance_loss_weight &gt; 0:
            total_multihead_variance_loss += multihead_variance_loss.detach().cpu().numpy()
        for j in range(int(model.out_dim/heads_per_class)):
            std = None
            if class_std is not None:
                std = class_std[:,j]
            <a id="change">info_dicts[j]</a> = update_info_dict(info_dicts[j], labels_batch[:, j], class_out[:, j], thresholds[j], std=std)
        &#47&#47 Save, for overall thresholding (not on training)
        if threshold_validation and LR is None:
            all_labels.append(labels_batch.detach().cpu().numpy())</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            class_out, clf_out = class_out
            if args.dual_thresh:
                class_out = <a id="change">class_out[:, :-1]</a>
            classifier_loss = clf_loss_fn(class_out, labels_batch)
            if args.use_softmax:
                <a id="change">class_out = F.softmax(class_out, -1)</a>

        loss = classifier_loss
        classifier_loss = classifier_loss.clone() &#47&#47 save for reporting
        &#47&#47 Also compute multihead variance loss -- from classifier [divide by output size since it scales linearly]
        if args.aux_head_variance_loss_weight &gt; 0.:
            multihead_variance_loss = model.classifier.get_last_layer_variance() / model.out_dim
            loss = loss + multihead_variance_loss * args.aux_head_variance_loss_weight
        &#47&#47 Divide by &#47&#47 batches? Since we&quotre looking at the parameters here, and should be batch independent.
        &#47&#47 multihead_variance_loss *= batch_adjustment

        if args.aux_lm_loss:
            lm_labels = text_batch[1:]
            lm_losses = aux_loss_fn(lm_out[:-1].view(-1, lm_out.size(2)).contiguous().float(),
                                      lm_labels.contiguous().view(-1))

            padding_mask = (torch.arange(lm_labels.size(0)).unsqueeze(1).cuda() &gt; length_batch).float()
            portion_unpadded = padding_mask.sum() / padding_mask.size(0)
            lm_loss = portion_unpadded * torch.mean(lm_losses * (padding_mask.view(-1).float()))

            &#47&#47 Scale LM loss -- since it&quots so big
            if args.aux_lm_loss_weight &gt; 0.:
                loss = loss + lm_loss * args.aux_lm_loss_weight

        &#47&#47 Training
        if LR is not None:
            LR.optimizer.zero_grad()
            loss.backward()
            LR.optimizer.step()
            LR.step()

        &#47&#47 Remove loss from CUDA -- kill gradients and save memory.
        total_loss += loss.detach().cpu().numpy()
        if args.use_softmax:
            labels_batch = onehot(labels_batch.squeeze(), model.out_dim)
            class_out = onehot(clf_out.view(-1), int(model.out_dim/heads_per_class))
        total_classifier_loss += classifier_loss.detach().cpu().numpy()
        if args.aux_lm_loss:
            total_lm_loss += lm_loss.detach().cpu().numpy()
        if args.aux_head_variance_loss_weight &gt; 0:
            total_multihead_variance_loss += multihead_variance_loss.detach().cpu().numpy()
        for j in range(int(model.out_dim/heads_per_class)):
            std = None
            if class_std is not None:
                std = class_std[:,j]
            <a id="change">info_dicts[j]</a> = update_info_dict(info_dicts[j], labels_batch[:, j], class_out[:, j], thresholds[j], std=std)
        &#47&#47 Save, for overall thresholding (not on training)
        if threshold_validation and LR is None:
            all_labels.append(labels_batch.detach().cpu().numpy())</code></pre><img src="67533478.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NVIDIA/sentiment-discovery/commit/345fcc9ff27a62015f257186af2941984d6a3cd7#diff-a5ff12394959301eb25d323d44fc987a79336e66a44eb3e4def7ae3515f35430L350' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/sentiment-discovery</div><div id='commit'> Commit Name: 345fcc9ff27a62015f257186af2941984d6a3cd7</div><div id='time'> Time: 2018-12-05</div><div id='author'> Author: raulp@nvidia.com</div><div id='file'> File Name: finetune.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: finetune</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/e12bd6a5e5195e158384059da3d5d47638ba12a1#diff-979c0874574a84c17305bec3f03f60aa69b3332579745e4c16d75d0f85e43cadL145' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: e12bd6a5e5195e158384059da3d5d47638ba12a1</div><div id='time'> Time: 2019-11-01</div><div id='author'> Author: pengchong@google.com</div><div id='file'> File Name: official/vision/detection/dataloader/maskrcnn_parser.py</div><div id='class'> Class Name: Parser</div><div id='method'> Method Name: _parse_train_data</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/tpu/commit/11b0078497d44560e1528343b6744451b3400928#diff-717b553873e416ca728cb8321488e4a92bfa7dc393bd0d7a2f650c5f4d70f3aaL145' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/tpu</div><div id='commit'> Commit Name: 11b0078497d44560e1528343b6744451b3400928</div><div id='time'> Time: 2019-11-01</div><div id='author'> Author: pengchong@google.com</div><div id='file'> File Name: models/official/detection/dataloader/maskrcnn_parser.py</div><div id='class'> Class Name: Parser</div><div id='method'> Method Name: _parse_train_data</div><BR>