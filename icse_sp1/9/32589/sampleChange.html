<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            because these depend on the given distributions and each loss function.

        
        <a id="change">if input_var is not None:
            _input_var = deepcopy(input_var)
        else:
            _input_var = deepcopy(p.input_var)
            if q is not None:
                _input_var += deepcopy(q.input_var)
                _input_var = sorted(set(_input_var), key=_input_var.index)
       </a> super().__init__(_input_var)
        self.p = p
        self.q = q
</code></pre><h3>After Change</h3><pre><code class='java'>
    &gt;&gt;&gt; p = Generator()
    &gt;&gt;&gt; q = Inference()
    &gt;&gt;&gt; prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),
  <a id="change">  ...                var=["z"], features_shape=[64], name="p_{prior}")
    ...
    &gt;&gt;&gt; &#47&#47 Define a loss functio</a>n (VAE)
    &gt;&gt;&gt; reconst = -p.log_<a id="change">prob().expectation(q)
    &gt;&gt;&gt; kl = KullbackLeibler(q,prior)
    &gt;&gt;&gt; loss_cls = (reconst - kl).mean()
    &gt;&gt;&gt; print(loss_cls)
    mean \\left(- D_{KL} \\left[q(z|x)||p_{prior}(z) \\right]</a> - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)
    &gt;&gt;&gt; &#47&#47 Evaluate this loss function
    &gt;&gt;&gt; data = torch.randn(1, 128)  &#47&#47 Pseudo data
    &gt;&gt;&gt; loss = loss_cls.eval({"x": data})</code></pre>