<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

  def test_load_from_pretrained_eager_mode(self):
    Tests loading pretrained model in eager execution mode.
    <a id="change">with context.eager_mode():
      source_model = MLP(
          hidden_layer_size=self.hidden_layer_size,
          feature_dim=self.feature_dim,
          model_dir=None,
          batch_size=10)

      source_model.fit(self.dataset, nb_epoch=1000, checkpoint_interval=0)

      dest_model = MLP(
          feature_dim=self.feature_dim,
          hidden_layer_size=self.hidden_layer_size,
          n_tasks=10)

      assignment_map = dict()
      value_map = dict()
      dest_vars = dest_model.model.trainable_variables[:-2]

      for idx, dest_var in enumerate(dest_vars):
        source_var = source_model.model.trainable_variables[idx]
        assignment_map[source_var] = dest_var
        value_map[source_var] = source_var.numpy()

      dest_model.load_from_pretrained(
          source_model=source_model,
          value_map=value_map,
          assignment_map=assignment_map)

      for source_var, dest_var in assignment_map.items():
        np.testing.assert_array_almost_equal(source_var.numpy(),
                                             dest_var.numpy())

 </a> def test_restore_equivalency_graph_mode(self):
    Test for restore based pretrained model loading in graph mode.
    source_model = MLP(
        model_dir="./MLP/",</code></pre><h3>After Change</h3><pre><code class='java'>

  def test_load_from_pretrained_eager_mode(self):
    Tests loading pretrained model in eager execution mode.
    <a id="change">with context.eager_mode():
      self.test_load_from_pretrained_graph_mode()

 </a> def test_restore_equivalency_graph_mode(self):
    Test for restore based pretrained model loading in graph mode.
    source_model = MLP(
        feature_dim=self.feature_dim, hidden_layer_size=self.hidden_layer_size)</code></pre>