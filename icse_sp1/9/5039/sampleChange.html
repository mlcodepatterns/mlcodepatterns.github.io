<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        See base class.
        assignments = []
        <a id="change">for (grad, param) in grads_and_vars:
            if grad is None or param is None:
                continue

            param_name = self._get_variable_name(param.name)

            m = tf.get_variable(
                name=param_name + "/adam_m",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            v = tf.get_variable(
                name=param_name + "/adam_v",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())

            &#47&#47 Standard Adam update.
            next_m = (
                tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
            next_v = (
                tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                          tf.square(grad)))

            update = next_m / (tf.sqrt(next_v) + self.epsilon)

            &#47&#47 Just adding the square of the weights to the loss function is *not*
            &#47&#47 the correct way of using L2 regularization/weight decay with Adam,
            &#47&#47 since that will interact with the m and v parameters in strange ways.
            &#47&#47
            &#47&#47 Instead we want ot decay the weights in a manner that doesn&quott interact
            &#47&#47 with the m/v parameters. This is equivalent to adding the square
            &#47&#47 of the weights to the loss with plain (non-momentum) SGD.
            if self._do_use_weight_decay(param_name):
                update += self.weight_decay_rate * param

            update_with_lr = self.learning_rate * update

            next_param = param - update_with_lr

            assignments.extend(
                    [param.assign(next_param),
                     m.assign(next_m),
                     v.assign(next_v)])
       </a> return tf.group(*assignments, name=name)

    def _do_use_weight_decay(self, param_name):
        Whether to use L2 weight decay for `param_name`.</code></pre><h3>After Change</h3><pre><code class='java'>

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        See base class.
        <a id="change">with tf.name_scope(self._name) as name:
            assignments = []
            for (grad, param) in grads_and_vars:
                if grad is None or param is None:
                    continue

                param_name = self._get_variable_name(param.name)

                m = tf.get_variable(
                    name=param_name + "/adam_m",
                    shape=param.shape.as_list(),
                    dtype=tf.float32,
                    trainable=False,
                    initializer=tf.zeros_initializer())
                v = tf.get_variable(
                    name=param_name + "/adam_v",
                    shape=param.shape.as_list(),
                    dtype=tf.float32,
                    trainable=False,
                    initializer=tf.zeros_initializer())

                &#47&#47 Standard Adam update.
                next_m = (
                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
                next_v = (
                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                              tf.square(grad)))

                update = next_m / (tf.sqrt(next_v) + self.epsilon)

                &#47&#47 Just adding the square of the weights to the loss function is *not*
                &#47&#47 the correct way of using L2 regularization/weight decay with Adam,
                &#47&#47 since that will interact with the m and v parameters in strange ways.
                &#47&#47
                &#47&#47 Instead we want ot decay the weights in a manner that doesn&quott interact
                &#47&#47 with the m/v parameters. This is equivalent to adding the square
                &#47&#47 of the weights to the loss with plain (non-momentum) SGD.
                if self._do_use_weight_decay(param_name):
                    update += self.weight_decay_rate * param

                update_with_lr = self.learning_rate * update

                next_param = param - update_with_lr

                assignments.extend(
                        [param.assign(next_param),
                         m.assign(next_m),
                         v.assign(next_v)])

            update_ops = assignments
            if global_step is None:
                apply_updates = self._finish(update_ops, name)
            else:
                with tf.control_dependencies([self._finish(update_ops, "update")]):
                    with tf.colocate_with(global_step):
                        apply_updates = tf.assign_add(global_step, 1, name=name)

       </a> return apply_updates

    def _do_use_weight_decay(self, param_name):
        Whether to use L2 weight decay for `param_name`.</code></pre>