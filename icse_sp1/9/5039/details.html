<html><h3>cf0cb3360a77041187c655891da4ddffbe4b13dd,texar/core/optimization.py,AdamWeightDecayOptimizer,apply_gradients,#AdamWeightDecayOptimizer#Any#Any#Any#,443
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        See base class.
        assignments = []
        <a id="change">for (grad, param) in grads_and_vars:
            if grad is None or param is None:
                continue

            param_name = self._get_variable_name(param.name)

            m = tf.get_variable(
                name=param_name + "/adam_m",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            v = tf.get_variable(
                name=param_name + "/adam_v",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())

            &#47&#47 Standard Adam update.
            next_m = (
                tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
            next_v = (
                tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                          tf.square(grad)))

            update = next_m / (tf.sqrt(next_v) + self.epsilon)

            &#47&#47 Just adding the square of the weights to the loss function is *not*
            &#47&#47 the correct way of using L2 regularization/weight decay with Adam,
            &#47&#47 since that will interact with the m and v parameters in strange ways.
            &#47&#47
            &#47&#47 Instead we want ot decay the weights in a manner that doesn&quott interact
            &#47&#47 with the m/v parameters. This is equivalent to adding the square
            &#47&#47 of the weights to the loss with plain (non-momentum) SGD.
            if self._do_use_weight_decay(param_name):
                update += self.weight_decay_rate * param

            update_with_lr = self.learning_rate * update

            next_param = param - update_with_lr

            assignments.extend(
                    [param.assign(next_param),
                     m.assign(next_m),
                     v.assign(next_v)])
       </a> return tf.group(*assignments, name=name)

    def _do_use_weight_decay(self, param_name):
        Whether to use L2 weight decay for `param_name`.</code></pre><h3>After Change</h3><pre><code class='java'>

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        See base class.
        <a id="change">with tf.name_scope(self._name) as name:
            assignments = []
            for (grad, param) in grads_and_vars:
                if grad is None or param is None:
                    continue

                param_name = self._get_variable_name(param.name)

                m = tf.get_variable(
                    name=param_name + "/adam_m",
                    shape=param.shape.as_list(),
                    dtype=tf.float32,
                    trainable=False,
                    initializer=tf.zeros_initializer())
                v = tf.get_variable(
                    name=param_name + "/adam_v",
                    shape=param.shape.as_list(),
                    dtype=tf.float32,
                    trainable=False,
                    initializer=tf.zeros_initializer())

                &#47&#47 Standard Adam update.
                next_m = (
                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
                next_v = (
                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                              tf.square(grad)))

                update = next_m / (tf.sqrt(next_v) + self.epsilon)

                &#47&#47 Just adding the square of the weights to the loss function is *not*
                &#47&#47 the correct way of using L2 regularization/weight decay with Adam,
                &#47&#47 since that will interact with the m and v parameters in strange ways.
                &#47&#47
                &#47&#47 Instead we want ot decay the weights in a manner that doesn&quott interact
                &#47&#47 with the m/v parameters. This is equivalent to adding the square
                &#47&#47 of the weights to the loss with plain (non-momentum) SGD.
                if self._do_use_weight_decay(param_name):
                    update += self.weight_decay_rate * param

                update_with_lr = self.learning_rate * update

                next_param = param - update_with_lr

                assignments.extend(
                        [param.assign(next_param),
                         m.assign(next_m),
                         v.assign(next_v)])

            update_ops = assignments
            if global_step is None:
                apply_updates = self._finish(update_ops, name)
            else:
                with tf.control_dependencies([self._finish(update_ops, "update")]):
                    with tf.colocate_with(global_step):
                        apply_updates = tf.assign_add(global_step, 1, name=name)

       </a> return apply_updates

    def _do_use_weight_decay(self, param_name):
        Whether to use L2 weight decay for `param_name`.</code></pre><img src="33098888.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/asyml/texar/commit/cf0cb3360a77041187c655891da4ddffbe4b13dd#diff-b89b23bb5bd7e0de33d86ed3a5842c59b3e818a108686a5e53b521160238ca74L442' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: cf0cb3360a77041187c655891da4ddffbe4b13dd</div><div id='time'> Time: 2018-11-27</div><div id='author'> Author: haoranshi97@gmail.com</div><div id='file'> File Name: texar/core/optimization.py</div><div id='class'> Class Name: AdamWeightDecayOptimizer</div><div id='method'> Method Name: apply_gradients</div><BR><BR><div id='link'><a href='https://github.com/pantsbuild/pants/commit/0093628056bbf454faf35709ee3c61acc133d0e9#diff-21d47889f9ebc1d29fd053648f327e1f30bda26ccfb10c837fee7536dda474e6L99' target='_blank'>Link</a></div><div id='project'> Project Name: pantsbuild/pants</div><div id='commit'> Commit Name: 0093628056bbf454faf35709ee3c61acc133d0e9</div><div id='time'> Time: 2016-11-05</div><div id='author'> Author: wisechengyi@gmail.com</div><div id='file'> File Name: contrib/scrooge/src/python/pants/contrib/scrooge/tasks/thrift_linter.py</div><div id='class'> Class Name: ThriftLinter</div><div id='method'> Method Name: execute</div><BR><BR><div id='link'><a href='https://github.com/ncullen93/torchsample/commit/10b7c76231fe2fd1ee0ab5260ec67a424c37a1a9#diff-b3b934d999eb60c8b28f562b73c81279c415c0299e404a57d3a24e21bac45a31L112' target='_blank'>Link</a></div><div id='project'> Project Name: ncullen93/torchsample</div><div id='commit'> Commit Name: 10b7c76231fe2fd1ee0ab5260ec67a424c37a1a9</div><div id='time'> Time: 2017-04-28</div><div id='author'> Author: ncullen@Nicks-MacBook-Pro.local</div><div id='file'> File Name: torchsample/modules/super_module.py</div><div id='class'> Class Name: SuperModule</div><div id='method'> Method Name: fit</div><BR>