<html><h3>a0d0c18fcde09f06be9ca1556052cf8064df4ac6,examples/reinforcement_learning/tutorial_A3C.py,,,#,52
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
GLOBAL_RUNNING_R = []
GLOBAL_EP = 0  &#47&#47 will increase during training, stop training when it &gt;= MAX_GLOBAL_EP

<a id="change">env = gym.make(GAME)</a>

<a id="change">N_S = env.observation_space.shape[0]</a>
N_A = <a id="change">env</a>.action_space.shape[0]

A_BOUND = [<a id="change">env</a>.action_space.low, <a id="change">env</a>.action_space.high]
A_BOUND[0] = A_BOUND[0].reshape(1, N_A)
<a id="change">A_BOUND[1] = A_BOUND[1].reshape(1, N_A)</a>
&#47&#47 print(A_BOUND)


class ACNet(object):</code></pre><h3>After Change</h3><pre><code class='java'>
ENTROPY_BETA = 0.005  &#47&#47 factor for entropy boosted exploration
LR_A = 0.00005  &#47&#47 learning rate for actor
LR_C = 0.0001  &#47&#47 learning rate for critic
GLOBAL_RUNNING_R = <a id="change">[]</a>
GLOBAL_EP = 0  &#47&#47 will increase during training, stop training when it &gt;= MAX_GLOBAL_EP


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47  Asynchronous Advantage Actor Critic (A3C)  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47</code></pre><img src="23513959.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/a0d0c18fcde09f06be9ca1556052cf8064df4ac6#diff-436fddbd886f9015c9ca09639011738efa50232cb57dd7b5498c01b00e4f3421L60' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: a0d0c18fcde09f06be9ca1556052cf8064df4ac6</div><div id='time'> Time: 2019-06-09</div><div id='author'> Author: 1402434478@qq.com</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_A3C.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/a0113631d7e9d6ec65531457ce03e06c902a4d0b#diff-b5802caa2922b0ddd80ef16c8ac3fa4a5de5a683293480ddfc449f5ac7159324L103' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: a0113631d7e9d6ec65531457ce03e06c902a4d0b</div><div id='time'> Time: 2017-10-17</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch06/01_dqn_pong.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/3bb0fd78af4f9dbddf7c01dde71844a283099650#diff-b5802caa2922b0ddd80ef16c8ac3fa4a5de5a683293480ddfc449f5ac7159324L71' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 3bb0fd78af4f9dbddf7c01dde71844a283099650</div><div id='time'> Time: 2017-10-16</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch06/01_dqn_pong.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>