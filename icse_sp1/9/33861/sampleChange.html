<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    For more information, see the MVNKLDivergence function documentation.
    
    <a id="change">return MVNKLDivergence()(mean_1, chol_covar_1, mean_2, covar_2)</a>


def _exact_predict(test_mean, test_test_covar, train_y, train_mean,
                   train_train_covar, train_test_covar, test_train_covar, alpha=None):</code></pre><h3>After Change</h3><pre><code class='java'>

    Where D is the dimensionality of the distributions.
    
    <a id="change">mu_diffs = mean_2 - mean_1</a>

    &#47&#47 ExactGPMarginalLogLikelihood gives us -0.5 * [\mu_2 -
    &#47&#47 \mu_1)\Sigma_{2}^{-1}(\mu_2 - \mu_1) + logdet(\Sigma_{2}) + const]
    &#47&#47 Multiplying that by -2 gives us two of the terms in the KL divergence
    &#47&#47 (plus an unwanted constant that we can subtract out).
    <a id="change">K_part = exact_gp_marginal_log_likelihood(covar_2, mu_diffs)</a>

    &#47&#47 Get logdet(\Sigma_{1})
    <a id="change">log_det_covar1 = chol_covar_1.diag().log().sum(0) * 2</a>

    &#47&#47 Get Tr(\Sigma_2^{-1}\Sigma_{1})
    <a id="change">trace = invmm(covar_2, chol_covar_1.t().mm(chol_covar_1)).trace()</a>

    &#47&#47 get D
    D = len(mu_diffs)

    &#47&#47 Compute the KL Divergence. We subtract out D * log(2 * pi) to get rid
    &#47&#47 of the extra unwanted constant term that ExactGPMarginalLogLikelihood gives us.
    res = 0.5 * (<a id="change">trace</a> - log_det_covar1 - 2 * K_part - (1 + math.log(2 * math.pi)) * D)

    <a id="change">return res</a>


def _exact_predict(test_mean, test_test_covar, train_y, train_mean,
                   train_train_covar, train_test_covar, test_train_covar, alpha=None):</code></pre>