<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        Model.__init__(self, params)
        training = self.phase == &quottraining&quot

        <a id="change">data = mx.sym.Variable(name="data")</a>
        &#47&#47 Layer1
        conv1 = mx.symbol.Convolution(name=&quotconv1&quot, data=data, kernel=(11, 11), stride=(4, 4), num_filter=96)
        relu1 = mx.symbol.Activation(name=&quotrelu1&quot, data=conv1, act_type=&quotrelu&quot)
        pool1 = mx.symbol.Pooling(name=&quotpool1&quot, data=relu1, pool_type="max", kernel=(2, 2), stride=(2, 2))
        &#47&#47 Layer2
        conv2 = mx.symbol.Convolution(name=&quotconv2&quot, data=pool1, kernel=(5, 5), num_filter=256)
        relu2 = mx.symbol.Activation(name=&quotrelu2&quot, data=conv2, act_type="relu")
        pool2 = mx.symbol.Pooling(name=&quotpool2&quot, data=relu2, kernel=(2, 2), stride=(2, 2), pool_type="max")
        &#47&#47 Layer3
        conv3 = mx.symbol.Convolution(name=&quotconv3&quot, data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=512)
        relu3 = mx.symbol.Activation(name=&quotrelu3&quot, data=conv3, act_type="relu")
        &#47&#47 Layer4
        conv4 = mx.symbol.Convolution(name=&quotconv4&quot, data=relu3, kernel=(3, 3), pad=(1, 1), num_filter=1024)
        relu4 = mx.symbol.Activation(name=&quotrelu4&quot, data=conv4, act_type="relu")
        &#47&#47 Layer5
        conv5 = mx.symbol.Convolution(name=&quotconv5&quot, data=relu4, kernel=(3, 3), pad=(1, 1), num_filter=1024)
        relu5 = mx.symbol.Activation(name=&quotrelu5&quot, data=conv5, act_type="relu")
        pool5 = mx.symbol.Pooling(name=&quotpool5&quot, data=relu5, kernel=(2, 2), stride=(2, 2), pool_type="max")
        &#47&#47 Layer6
        flatten = mx.symbol.Flatten(data=pool5)
        fc6 = mx.symbol.FullyConnected(name=&quotfc6&quot, data=flatten, num_hidden=3072)
        relu6 = mx.symbol.Activation(name=&quotrelu6&quot, data=fc6, act_type="relu")
        drop6 = mx.symbol.Dropout(name=&quotdrop6&quot, data=relu6, p=0.5) if training else relu6
        &#47&#47 Layer7
        fc7 = mx.symbol.FullyConnected(name=&quotfc7&quot, data=drop6, num_hidden=4096)
        relu7 = mx.symbol.Activation(name=&quotrelu7&quot, data=fc7, act_type="relu")
        drop7 = mx.symbol.Dropout(name=&quotdrop7&quot, data=relu7, p=0.5) if training else relu7

        <a id="change">self.__output</a> = self.add_head_nodes(drop7)
</code></pre><h3>After Change</h3><pre><code class='java'>
        Model.__init__(self, params)
        training = self.phase == &quottraining&quot

        <a id="change">data = self.add_data_node()</a>
        &#47&#47 Layer1
        conv1 = mx.symbol.Convolution(name=&quotconv1&quot, data=data, kernel=(11, 11), stride=(4, 4), num_filter=96)
        relu1 = mx.symbol.Activation(name=&quotrelu1&quot, data=conv1, act_type=&quotrelu&quot)
        pool1 = mx.symbol.Pooling(name=&quotpool1&quot, data=relu1, pool_type="max", kernel=(2, 2), stride=(2, 2))
        &#47&#47 Layer2
        conv2 = mx.symbol.Convolution(name=&quotconv2&quot, data=pool1, kernel=(5, 5), num_filter=256)
        relu2 = mx.symbol.Activation(name=&quotrelu2&quot, data=conv2, act_type="relu")
        pool2 = mx.symbol.Pooling(name=&quotpool2&quot, data=relu2, kernel=(2, 2), stride=(2, 2), pool_type="max")
        &#47&#47 Layer3
        conv3 = mx.symbol.Convolution(name=&quotconv3&quot, data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=512)
        relu3 = mx.symbol.Activation(name=&quotrelu3&quot, data=conv3, act_type="relu")
        &#47&#47 Layer4
        conv4 = mx.symbol.Convolution(name=&quotconv4&quot, data=relu3, kernel=(3, 3), pad=(1, 1), num_filter=1024)
        relu4 = mx.symbol.Activation(name=&quotrelu4&quot, data=conv4, act_type="relu")
        &#47&#47 Layer5
        conv5 = mx.symbol.Convolution(name=&quotconv5&quot, data=relu4, kernel=(3, 3), pad=(1, 1), num_filter=1024)
        relu5 = mx.symbol.Activation(name=&quotrelu5&quot, data=conv5, act_type="relu")
        pool5 = mx.symbol.Pooling(name=&quotpool5&quot, data=relu5, kernel=(2, 2), stride=(2, 2), pool_type="max")
        &#47&#47 Layer6
        flatten = mx.symbol.Flatten(data=pool5)
        fc6 = mx.symbol.FullyConnected(name=&quotfc6&quot, data=flatten, num_hidden=3072)
        relu6 = mx.symbol.Activation(name=&quotrelu6&quot, data=fc6, act_type="relu")
        drop6 = mx.symbol.Dropout(name=&quotdrop6&quot, data=relu6, p=0.5) if training else relu6
        &#47&#47 Layer7
        fc7 = mx.symbol.FullyConnected(name=&quotfc7&quot, data=drop6, num_hidden=4096)
        relu7 = mx.symbol.Activation(name=&quotrelu7&quot, data=fc7, act_type="relu")
        drop7 = mx.symbol.Dropout(name=&quotdrop7&quot, data=relu7, p=0.5) if training else relu7

        <a id="change">self.__output</a> = self.add_head_nodes(drop7)
</code></pre>