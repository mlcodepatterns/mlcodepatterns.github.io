<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      [batch, height_in*scale, width_in*scale, channels].
  
  shape = shape_utils.combined_static_and_dynamic_shape(input_tensor)
  <a id="change">shape_before_tile = [shape[0], shape[1], 1, shape[2], 1, shape[3]]</a>
  <a id="change">shape_after_tile = [shape[0], shape[1] * scale, shape[2] * scale, shape[3]]</a>
  data_reshaped = tf.reshape(input_tensor, shape_before_tile)
  resized_tensor = tf.tile(data_reshaped, [1, 1, scale, 1, scale, 1])
  resized_tensor = tf.reshape(resized_tensor, shape_after_tile)
  <a id="change">return resized_tensor</a>


def matmul_gather_on_zeroth_axis(params, indices, scope=None):
  Matrix multiplication based implementation of tf.gather on zeroth axis.</code></pre><h3>After Change</h3><pre><code class='java'>
    data_up: A float32 tensor of size
      [batch, height_in*scale, width_in*scale, channels].
  
  <a id="change">with tf.name_scope(&quotnearest_neighbor_upsampling&quot):
    (batch_size, height, width,
     channels) = shape_utils.combined_static_and_dynamic_shape(input_tensor)
    output_tensor = tf.reshape(
        input_tensor, [batch_size, height, 1, width, 1, channels]) * tf.ones(
            [1, 1, scale, 1, scale, 1], dtype=input_tensor.dtype)
    return tf.reshape(output_tensor,
                      [batch_size, height * scale, width * scale, channels])


</a>def matmul_gather_on_zeroth_axis(params, indices, scope=None):
  Matrix multiplication based implementation of tf.gather on zeroth axis.

  TODO(rathodv, jonathanhuang): enable sparse matmul option.</code></pre>