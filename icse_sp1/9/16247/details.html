<html><h3>9d56361641a64ff73ac630812ecd4964eedbc7aa,gat/graph_attention_layer.py,GraphAttention,call,#GraphAttention#Any#,72
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            for slice in combination_slices:
                dense = Dense(1)(slice)  &#47&#47 N x 1 (basically "a(Wh_i, Wh_j)" in the paper)
                &#47&#47 TODO: masking
                <a id="change">e_i = K.reshape(dense, (1, -1))</a>  &#47&#47 1 x N (e_i in the paper)
                softmax = K.squeeze(K.softmax(e_i))  &#47&#47 N (alpha_i in the paper)
                softmax_broadcast = K.transpose(K.reshape(K.tile(softmax, [self.F_]), [self.F_, -1]))
                node_features = K.sum(softmax_broadcast * linear_transf, axis=0)</code></pre><h3>After Change</h3><pre><code class='java'>
        X = inputs[0]  &#47&#47 input graph (B x F)
        G = inputs[1]  &#47&#47 full graph (N x F) (this is necessary in code, but not in theory. Check section 2.2 of the paper)
        B = K.shape(X)[0]  &#47&#47 Get batch size at runtime
        <a id="change">N = K.shape(G)[0]</a>  &#47&#47 Get number of nodes in the graph at runtime

        outputs = []  &#47&#47 Will store the outputs of each attention head (B x F&quot or B x KF&quot)
        for head in range(self.attention_heads):</code></pre><img src="95471536.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/danielegrattarola/keras-gat/commit/9d56361641a64ff73ac630812ecd4964eedbc7aa#diff-b5eead7f2a637c33998de1add9a690c0678e849a3d16791260f0f0a16506c7c3L72' target='_blank'>Link</a></div><div id='project'> Project Name: danielegrattarola/keras-gat</div><div id='commit'> Commit Name: 9d56361641a64ff73ac630812ecd4964eedbc7aa</div><div id='time'> Time: 2017-11-09</div><div id='author'> Author: daniele.grattarola@gmail.com</div><div id='file'> File Name: gat/graph_attention_layer.py</div><div id='class'> Class Name: GraphAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/fizyr/keras-retinanet/commit/eb737460e317877af263183245ebac792c02aa20#diff-85e0892ca409f8732b8a2b9a5fee91b4884b0130590402c2b56a2f76530a5345L121' target='_blank'>Link</a></div><div id='project'> Project Name: fizyr/keras-retinanet</div><div id='commit'> Commit Name: eb737460e317877af263183245ebac792c02aa20</div><div id='time'> Time: 2017-10-23</div><div id='author'> Author: hansg91@gmail.com</div><div id='file'> File Name: keras_retinanet/layers/_misc.py</div><div id='class'> Class Name: NonMaximumSuppression</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/GPflow/GPflow/commit/f746253f6d61780397e0aebd997f0f1b5db17ed6#diff-3bfc3bc9d6a4dc9cd329b6742c163ebf15a36e9ec3d1e85a9f21cf2b8329e1afL185' target='_blank'>Link</a></div><div id='project'> Project Name: GPflow/GPflow</div><div id='commit'> Commit Name: f746253f6d61780397e0aebd997f0f1b5db17ed6</div><div id='time'> Time: 2018-07-17</div><div id='author'> Author: dutordoirv@gmail.com</div><div id='file'> File Name: gpflow/quadrature.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: ndiag_mc</div><BR>