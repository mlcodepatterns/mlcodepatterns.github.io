<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
y = torch.randn((2, 2))
z = x + y  &#47&#47 These are Tensor types, and backprop would not be possible

<a id="change">var_x = autograd.Variable(x, requires_grad=True)</a>
var_y = autograd.Variable(y, requires_grad=True)
&#47&#47 var_z contains enough information to compute gradients, as we saw above
var_z = var_x + var_y
print(var_z.grad_fn)</code></pre><h3>After Change</h3><pre><code class='java'>

&#47&#47 Now z has the computation history that relates itself to x and y
&#47&#47 Can we just take its values, and **detach** it from its history?
<a id="change">new_z = z.detach()</a>

&#47&#47 ... does new_z have information to backprop to x and y?
&#47&#47 NO!
print(new_z.grad_fn)</code></pre>