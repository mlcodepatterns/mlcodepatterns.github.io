<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      lr_t = tf.reshape(learning_rate, [1])
      ce_t = tf.reshape(current_epoch, [1])

      host_call = <a id="change">(host_call_fn, [gs_t, loss_t, lr_t, ce_t])</a>

  else:
    train_op = None
</code></pre><h3>After Change</h3><pre><code class='java'>
      optimizer = tf.tpu.CrossShardOptimizer(optimizer)

      if params[&quotadd_summaries&quot]:
        <a id="change">summary_writer = tf2.summary.create_file_writer(
            FLAGS.model_dir, max_queue=params[&quotiterations_per_loop&quot])</a>
        <a id="change">with summary_writer.as_default():
          should_record = tf.equal(global_step % params[&quotiterations_per_loop&quot],
                                   0)
          with tf2.summary.record_if(should_record):
            tf2.summary.scalar(&quotloss&quot, loss, step=global_step)
            tf2.summary.scalar(&quotlearning_rate&quot, learning_rate, step=global_step)
            tf2.summary.scalar(&quotcurrent_epoch&quot, current_epoch, step=global_step)

    &#47&#47 Batch normalization requires UPDATE_OPS to be added as a dependency to
    &#47&#47 the train operation.
   </a> update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

    with tf.control_dependencies(update_ops + tf.summary.all_v2_summary_ops()):
      train_op = optimizer.minimize(loss, global_step)</code></pre>