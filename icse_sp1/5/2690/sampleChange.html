<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self._obs_pl, reuse=True)
        log_pi_t = self._policy_dist.log_p_t  &#47&#47 N

        self._vf_t = <a id="change">self._vf.get_output_for(self._obs_pl, reuse=True)</a>  &#47&#47 N
        self._vf_params = self._vf.get_params_internal()

        log_target_t = self._qf.get_output_for(</code></pre><h3>After Change</h3><pre><code class='java'>
        actions, log_pi_t = self._policy.actions_for(
            observations=self._obs_pl, with_log_pis=True, reuse=True)

        self._V_t = self._V(<a id="change">[self._obs_pl]</a>)
        self._V_params = self._V.trainable_variables

        log_target_t = self._Q([self._obs_pl, actions])
        scaled_log_pi = self._scale_entropy * (log_pi_t)

        self._kl_surrogate_loss_t = tf.reduce_mean(log_pi_t * tf.stop_gradient(
            scaled_log_pi - log_target_t + self._V_t)
        )

        self._V_loss_t = 0.5 * tf.reduce_mean(
            (self._V_t - tf.stop_gradient(log_target_t - scaled_log_pi))**2
        )

        <a id="change">policy_loss = (self._kl_surrogate_loss_t
                       + self._policy.distribution.reg_loss_t)</a>

        policy_train_op = tf.train.AdamOptimizer(self._policy_lr).minimize(
            loss=policy_loss,
            var_list=self._policy.get_params_internal()</code></pre>