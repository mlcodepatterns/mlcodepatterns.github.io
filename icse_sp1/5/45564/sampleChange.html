<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        lengths, or have inconsistent types.
    
    optimizer_utils.check_updates_parameters(updates, parameters)
    <a id="change">for update, parameter in zip(updates, parameters):
      &#47&#47 TODO(petebu): Add support for sparse tensors.
      &#47&#47 TODO(petebu): Consider caching learning_rate cast.
      &#47&#47 TODO(petebu): Consider the case when all updates are None.
      if update is not None:
        optimizer_utils.check_same_dtype(update, parameter)
        mom, ms, mg = self._get_or_create_moving_vars(parameter)
        learning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)
        decay = tf.cast(self.decay, update.dtype.base_dtype)
        momentum = tf.cast(self.momentum, update.dtype.base_dtype)
        epsilon = tf.cast(self.epsilon, update.dtype.base_dtype)
        if self.centered:
          tf.raw_ops.ResourceApplyCenteredRMSProp(
              var=parameter.handle,
              mg=mg.handle,
              ms=ms.handle,
              mom=mom.handle,
              lr=learning_rate,
              rho=decay,
              momentum=momentum,
              epsilon=epsilon,
              grad=update)
        else:
          tf.raw_ops.ResourceApplyRMSProp(
              var=parameter.handle,
              ms=ms.handle,
              mom=mom.handle,
              lr=learning_rate,
              rho=decay,
              momentum=momentum,
              epsilon=epsilon,
              grad=update)


</a>class ReferenceRMSProp(base.Module):
  Reference version of the RMSProp module.

  See: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</code></pre><h3>After Change</h3><pre><code class='java'>
    
    optimizer_utils.check_updates_parameters(updates, parameters)
    self._initialize(parameters)
    <a id="change">for update, parameter, mom, ms, mg in six.moves.zip_longest(
        updates, parameters, self.mom, self.ms, self.mg):
      &#47&#47 TODO(petebu): Add support for sparse tensors.
      &#47&#47 TODO(petebu): Consider caching learning_rate cast.
      &#47&#47 TODO(petebu): Consider the case when all updates are None.
      if update is not None:
        optimizer_utils.check_same_dtype(update, parameter)
        learning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)
        decay = tf.cast(self.decay, update.dtype.base_dtype)
        momentum = tf.cast(self.momentum, update.dtype.base_dtype)
        epsilon = tf.cast(self.epsilon, update.dtype.base_dtype)
        if self.centered:
          tf.raw_ops.ResourceApplyCenteredRMSProp(
              var=parameter.handle,
              mg=mg.handle,
              ms=ms.handle,
              mom=mom.handle,
              lr=learning_rate,
              rho=decay,
              momentum=momentum,
              epsilon=epsilon,
              grad=update)
        else:
          tf.raw_ops.ResourceApplyRMSProp(
              var=parameter.handle,
              ms=ms.handle,
              mom=mom.handle,
              lr=learning_rate,
              rho=decay,
              momentum=momentum,
              epsilon=epsilon,
              grad=update)


</a>class ReferenceRMSProp(base.Module):
  Reference version of the RMSProp module.

  See: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</code></pre>