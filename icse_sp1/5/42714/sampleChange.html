<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            ctx = arr.context
            groups[ctx].append(arr)
        return groups
    <a id="change">norm_groups = group_by_ctx(norm_arrays)</a>

    &#47&#47 reduce
    ctx, dtype = arrays[0].context, &quotfloat32&quot
    norms = [nd.add_n(*g).as_in_context(ctx) <a id="change">for</a> g in <a id="change">norm_groups.values()</a>]
    total_norm = nd.add_n(*norms).sqrt()
    scale = total_norm / max_norm
    &#47&#47 is_finite = 0 if NaN or Inf, 1 otherwise.</code></pre><h3>After Change</h3><pre><code class='java'>
            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
        max_norm : NDArray, optional, default is None
            max value for global 2-norm of gradients.
<a id="change">        </a>
      <a id="change">  self.fp32_trainer.allreduce_</a>grads()
        step_size = batch_size * self._scaler.loss_scale
        if max_norm:
            _, ratio, is_finite = nlp.utils.grad_global_norm(self.fp32_trainer._params,</code></pre>