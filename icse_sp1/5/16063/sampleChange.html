<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        Creates an optimizer from a specification dict.
        
        <a id="change">return util.get_object(
            obj=spec,
            predefined_objects=tensorforce.core.optimizers.optimizers,
            kwargs=kwargs
        )</a>

    &#47&#47 modified minimize
    def apply_step(self, variables, diffs, global_step=None, gate_gradients=None, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None):
        diffs_and_vars = self.compute_diffs(diffs, var_list=variables, gate_gradients=gate_gradients, aggregation_method=aggregation_method, colocate_gradients_with_ops=colocate_gradients_with_ops, grad_loss=grad_loss)</code></pre><h3>After Change</h3><pre><code class='java'>
        
        Creates an optimizer from a specification dict.
        
        <a id="change">optimizer = util.get_object(
            obj=spec,
            predefined_objects=tensorforce.core.optimizers.optimizers,
            kwargs=kwargs
        )</a>
        assert isinstance(optimizer, Optimizer)
        <a id="change">return optimizer</a>

    &#47&#47 modified minimize
    def apply_step(self, variables, diffs, global_step=None, gate_gradients=None, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None):
        diffs_and_vars = self.compute_diffs(diffs, var_list=variables, gate_gradients=gate_gradients, aggregation_method=aggregation_method, colocate_gradients_with_ops=colocate_gradients_with_ops, grad_loss=grad_loss)</code></pre>