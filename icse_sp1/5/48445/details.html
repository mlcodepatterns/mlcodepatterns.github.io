<html><h3>812339ca3d9b68b5f87f9c391aa918f068d5b004,torch/distributed/optim/zero_redundancy_optimizer.py,ZeroRedundancyOptimizer,__init__,#ZeroRedundancyOptimizer#Any#Any#Any#Any#,124
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.optim = optim(self.partition_parameters()[self.rank], **default)

        &#47&#47 - Sync local and global param_groups keys
        <a id="change">for global_group, local_group in zip(
            self.param_groups, self.optim.param_groups
        ):
            for k, v in local_group.items():
                if k != "params":
                    global_group[k] = v

        &#47&#47  Optional consolidated optimizer state
       </a> self._all_states: List[Dict[str, Any]] = []

        &#47&#47 Current default device is set by the parameters allocated to this rank
        self._device = list(self.per_device_params.keys())[0]</code></pre><h3>After Change</h3><pre><code class='java'>
        self._partition_parameters_cache: List[List[Dict]] = []
        self._index_to_param_cache: Dict[int, torch.Tensor] = {}
        self._all_params = params
        self._reference_is_trainable_mask = list(<a id="change">map(_is_trainable, self._all_params)</a>)

        &#47&#47 Build the wrapped optimizer, responsible for a shard of the params
        self.group = group if group is not None else dist.group.WORLD</code></pre><img src="223643396.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/812339ca3d9b68b5f87f9c391aa918f068d5b004#diff-30d3614ba0cf90cf509ef5d96b4e120eb8bfc65a7602b8438a3718f7f197ef6cL125' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 812339ca3d9b68b5f87f9c391aa918f068d5b004</div><div id='time'> Time: 2021-03-01</div><div id='author'> Author: benjamin.lefaudeux@gmail.com</div><div id='file'> File Name: torch/distributed/optim/zero_redundancy_optimizer.py</div><div id='class'> Class Name: ZeroRedundancyOptimizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/scikit-optimize/scikit-optimize/commit/544875dd8a7fea49a86e5623d37274159b4ba7b5#diff-50c937365fd8785e82a6fcd859a637c1b1d1cc460121e42e1504d0b7d749e78bL159' target='_blank'>Link</a></div><div id='project'> Project Name: scikit-optimize/scikit-optimize</div><div id='commit'> Commit Name: 544875dd8a7fea49a86e5623d37274159b4ba7b5</div><div id='time'> Time: 2017-01-10</div><div id='author'> Author: betatim@gmail.com</div><div id='file'> File Name: skopt/optimizer/base.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: base_minimize</div><BR><BR><div id='link'><a href='https://github.com/Pinafore/qb/commit/166cb2c804d081401f0efb52745d214fa633fae1#diff-50e40d902a37848fa0e869a92f9604c7780272ebe66d5d9fccdde151c6994569L38' target='_blank'>Link</a></div><div id='project'> Project Name: Pinafore/qb</div><div id='commit'> Commit Name: 166cb2c804d081401f0efb52745d214fa633fae1</div><div id='time'> Time: 2017-04-13</div><div id='author'> Author: ski.rodriguez@gmail.com</div><div id='file'> File Name: qanta/guesser/elasticsearch.py</div><div id='class'> Class Name: ElasticSearchGuesser</div><div id='method'> Method Name: guess</div><BR>