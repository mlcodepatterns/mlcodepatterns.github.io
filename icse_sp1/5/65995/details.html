<html><h3>812339ca3d9b68b5f87f9c391aa918f068d5b004,torch/distributed/optim/zero_redundancy_optimizer.py,ZeroRedundancyOptimizer,__init__,#ZeroRedundancyOptimizer#Any#Any#Any#Any#,124
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for global_group, local_group in zip(
            self.param_groups, self.optim.param_groups
        ):
            <a id="change">for k, v in local_group.items():
                if k != "params":
                    global_group[k] = v

        &#47&#47  Optional consolidated optimizer state
       </a> self._all_states: List[Dict[str, Any]] = []

        &#47&#47 Current default device is set by the parameters allocated to this rank
        self._device = list(self.per_device_params.keys())[0]</code></pre><h3>After Change</h3><pre><code class='java'>
        self._partition_parameters_cache: List[List[Dict]] = []
        self._index_to_param_cache: Dict[int, torch.Tensor] = {}
        self._all_params = params
        <a id="change">self._reference_is_trainable_mask = list(map(_is_trainable, self._all_params))</a>

        &#47&#47 Build the wrapped optimizer, responsible for a shard of the params
        self.group = group if group is not None else dist.group.WORLD
        self.world_size = dist.get_world_size(self.group)</code></pre><img src="302548747.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/812339ca3d9b68b5f87f9c391aa918f068d5b004#diff-30d3614ba0cf90cf509ef5d96b4e120eb8bfc65a7602b8438a3718f7f197ef6cL125' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 812339ca3d9b68b5f87f9c391aa918f068d5b004</div><div id='time'> Time: 2021-03-01</div><div id='author'> Author: benjamin.lefaudeux@gmail.com</div><div id='file'> File Name: torch/distributed/optim/zero_redundancy_optimizer.py</div><div id='class'> Class Name: ZeroRedundancyOptimizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/miso-belica/sumy/commit/c0a5504ce6ff8a850eabd5a33488ccf535a55b4f#diff-59dd6b84f9618de08bf615fbc37bc997030837f30678a2ce3dfec0931672ba0fL21' target='_blank'>Link</a></div><div id='project'> Project Name: miso-belica/sumy</div><div id='commit'> Commit Name: c0a5504ce6ff8a850eabd5a33488ccf535a55b4f</div><div id='time'> Time: 2013-05-21</div><div id='author'> Author: miso.belica@gmail.com</div><div id='file'> File Name: sumy/summarizers/edmundson_key.py</div><div id='class'> Class Name: EdmundsonKeyMethod</div><div id='method'> Method Name: _compute_significant_words</div><BR><BR><div id='link'><a href='https://github.com/onnx/onnx-tensorflow/commit/bab79f5264c139829c365b0e4924210c67088e43#diff-2d136e43a927c6b391844d982c7b396a5259b3b32e0cd34953986ea7a7577309L57' target='_blank'>Link</a></div><div id='project'> Project Name: onnx/onnx-tensorflow</div><div id='commit'> Commit Name: bab79f5264c139829c365b0e4924210c67088e43</div><div id='time'> Time: 2017-10-20</div><div id='author'> Author: tian.jin1@ibm.com</div><div id='file'> File Name: onnxtf/backend.py</div><div id='class'> Class Name: TensorflowBackend</div><div id='method'> Method Name: run_node</div><BR>