<html><h3>b2951813547426828d313a80c52de8a619e99731,gpytorch/lazy/constant_mul_lazy_tensor.py,ConstantMulLazyTensor,__init__,#ConstantMulLazyTensor#Any#Any#,56
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    "Got a constant with %d dimensions - expected a 0D or 1D tensor" % constant.ndimension()
                )
            elif constant.numel() &gt; 1:
                if not (base_lazy_tensor.ndimension() == 3 and <a id="change">base_lazy_tensor.size(0)</a> == constant.numel()):
                    numel = constant.numel()
                    raise RuntimeError(
                        "A constant with size %d expedts a 3D lazy var. with batch size %d. "</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, base_lazy_tensor, constant):
        if not torch.is_tensor(constant):
            constant = torch.tensor(constant, device=base_lazy_tensor.device, dtype=base_lazy_tensor.dtype)
        <a id="change">orig_constant = constant</a>

        &#47&#47 Make sure that the constant can be expanded to the appropriate size
        try:
            <a id="change">constant = orig_constant.expand(base_lazy_tensor.batch_shape)</a>
        except RuntimeError:
            raise RuntimeError(
                "ConstantMulLazyTensor of size {} received an invalid constant of size {}.".format(
                    base_lazy_tensor.shape, orig_constant.shape</code></pre><img src="196255426.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/cornellius-gp/gpytorch/commit/b2951813547426828d313a80c52de8a619e99731#diff-5a33a970d4df7e5234ae3caa389eb12a97e4534081406f8954d5661400d8a937L57' target='_blank'>Link</a></div><div id='project'> Project Name: cornellius-gp/gpytorch</div><div id='commit'> Commit Name: b2951813547426828d313a80c52de8a619e99731</div><div id='time'> Time: 2018-11-26</div><div id='author'> Author: gpleiss@gmail.com</div><div id='file'> File Name: gpytorch/lazy/constant_mul_lazy_tensor.py</div><div id='class'> Class Name: ConstantMulLazyTensor</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/bed0a0ae26451c9897cf1ee0f7302e42eba9b42c#diff-a6a596f1b879b5a9b6619ca2e20a4a5ec1725229f46a2e80291aa2a89ef6dc5cL34' target='_blank'>Link</a></div><div id='project'> Project Name: jadore801120/attention-is-all-you-need-pytorch</div><div id='commit'> Commit Name: bed0a0ae26451c9897cf1ee0f7302e42eba9b42c</div><div id='time'> Time: 2018-08-23</div><div id='author'> Author: yhhuang@nlg.csie.ntu.edu.tw</div><div id='file'> File Name: transformer/Models.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_attn_subsequent_mask</div><BR><BR><div id='link'><a href='https://github.com/r9y9/deepvoice3_pytorch/commit/96ecbdba66058274e2ceb9f58261daa4a45eb873#diff-e2bb38e254f3c8b6c9bbea51b962fb9b89d3e4f3c96ddefc79a56b912617316aL238' target='_blank'>Link</a></div><div id='project'> Project Name: r9y9/deepvoice3_pytorch</div><div id='commit'> Commit Name: 96ecbdba66058274e2ceb9f58261daa4a45eb873</div><div id='time'> Time: 2019-12-21</div><div id='author'> Author: zryuichi@gmail.com</div><div id='file'> File Name: deepvoice3_pytorch/modules.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_mask_from_lengths</div><BR>