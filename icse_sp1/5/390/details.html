<html><h3>a4376fc2e3c1eec67decf36bc0a1771ad17a771e,src/attacks/carlini.py,CarliniL2Method,generate,#CarliniL2Method#Any#,139
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
                    &#47&#47 Check whether last attack was successful:
                    &#47&#47 Attack success criterion: first term of the loss function is &lt;= 0
                    last_attack_success = <a id="change">loss[0]</a>-l2dist &lt;= 0                                         
                    attack_success = attack_success or last_attack_success
                    
                    if last_attack_success and l2dist &lt; best_l2dist:</code></pre><h3>After Change</h3><pre><code class='java'>
                    adv_image = self._sess.run(self._adv_image)
                    z = self.classifier.predict(np.array([adv_image]), logits=True)[0]
                    loss, l2dist, grad_l2z = self._sess.run([self._loss, self._l2dist, self._grad_l2z], {self._z: z})
                    <a id="change">grad_z2p = self.classifier.class_gradient(np.array([adv_image]), logits=True)[0]</a>
                    grad_l2p = np.zeros(shape=self.classifier.input_shape)
                    for i in range(self.classifier.nb_classes):
                        <a id="change">grad_l2p += grad_z2p[i] * grad_l2z[i]</a>

                    &#47&#47 Update the pertubation with decayed learning rate
                    lr *= (1. / (1. + self.decay * it))
                    self._sess.run(self._update_pert, {self._grad_l2p: grad_l2p, self._lr: lr})</code></pre><img src="3703784.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/a4376fc2e3c1eec67decf36bc0a1771ad17a771e#diff-a6a5fb3cf2192f6a45eff5fc40824af84e9112e77410e7ea3c1b5601ffaf14b6L139' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: a4376fc2e3c1eec67decf36bc0a1771ad17a771e</div><div id='time'> Time: 2018-04-24</div><div id='author'> Author: M.N.Tran@ibm.com</div><div id='file'> File Name: src/attacks/carlini.py</div><div id='class'> Class Name: CarliniL2Method</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/063c572b5b09fb8492beee938589cf5984f84926#diff-bf46b5635935dba7e8c56cce13c2da745568eac81086b4af55364cb1c973bcd6L234' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 063c572b5b09fb8492beee938589cf5984f84926</div><div id='time'> Time: 2020-12-18</div><div id='author'> Author: beat.buesser@ie.ibm.com</div><div id='file'> File Name: art/estimators/classification/ensemble.py</div><div id='class'> Class Name: EnsembleClassifier</div><div id='method'> Method Name: class_gradient</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/246d18f21ba69f06e86da51ab16e499efacbf30f#diff-cfb4ea3a1019e16a8a1b760d26cbeab2c45359ce614a0050afcc4fc7da4d2742L147' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 246d18f21ba69f06e86da51ab16e499efacbf30f</div><div id='time'> Time: 2018-04-17</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: src/attacks/saliency_map.py</div><div id='class'> Class Name: SaliencyMapMethod</div><div id='method'> Method Name: _saliency_map</div><BR>