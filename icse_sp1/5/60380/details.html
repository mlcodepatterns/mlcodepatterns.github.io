<html><h3>27568a7ebed1a35f08ac0390f35b3de9b8dad0dd,fairseq/models/levenshtein_transformer.py,LevenshteinTransformerDecoder,extract_features,#LevenshteinTransformerDecoder#Any#Any#Any#Any#,548
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        )

        &#47&#47 embed tokens and positions
        x = self.embed_scale * self.embed_tokens(<a id="change">prev_output_tokens.long()</a>)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)

        if positions is not None:
            x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)

        &#47&#47 B x T x C -&gt; T x B x C
        x = x.transpose(0, 1)
        attn = None
        inner_states = [x]

        &#47&#47 decoder layers
        decoder_padding_mask = prev_output_tokens.eq(self.padding_idx)
        layers = self.layers if layers is None else layers
        early_exit = len(layers) if early_exit is None else early_exit
        for _, layer in enumerate(layers[:early_exit]):
            x, attn = layer(
                x,
                encoder_out[0] if encoder_out is not None else None,
                encoder_out[1] if encoder_out is not None else None,
                self_attn_mask=None,
                self_attn_padding_mask=decoder_padding_mask,
            )
            inner_states.append(x)

        if self.layer_norm:
            x = self.layer_norm(x)

        &#47&#47 T x B x C -&gt; B x T x C
        x = x.transpose(0, 1)

        if self.project_out_dim is not None:
            x = self.project_out_dim(x)

        <a id="change">return x, attn, inner_states</a>

    def forward_mask_ins(self, prev_output_tokens, encoder_out=None, **unused):
        features, attn, _ = self.extract_features(
            prev_output_tokens,</code></pre><h3>After Change</h3><pre><code class='java'>
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)

        <a id="change">return x, {"attn": attn, "inner_states": inner_states}</a>

    def forward_mask_ins(self, prev_output_tokens, encoder_out=None, **unused):
        features, extra = self.extract_features(
            prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[1], layers=self.layers_msk, **unused</code></pre><img src="277861359.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/27568a7ebed1a35f08ac0390f35b3de9b8dad0dd#diff-408774411f25f5b4ba7f99eb97f35f1d0d37c2a722a9e7af6ff1dfc3c6bf70a6L577' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 27568a7ebed1a35f08ac0390f35b3de9b8dad0dd</div><div id='time'> Time: 2019-11-13</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/models/levenshtein_transformer.py</div><div id='class'> Class Name: LevenshteinTransformerDecoder</div><div id='method'> Method Name: extract_features</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/5980bd68fcd65ffe90fe560ff7db3f01e861426e#diff-a769335fea7dfb463e777880f66930e492d86986541ed084a6aa715293ed8884L45' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: 5980bd68fcd65ffe90fe560ff7db3f01e861426e</div><div id='time'> Time: 2017-11-02</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: torch_geometric/transform/graclus.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: normalized_cut</div><BR><BR><div id='link'><a href='https://github.com/arraiy/torchgeometry/commit/73a339fac0a9574ee16527ebf9b6d71073bb688b#diff-9f58a180b69dd52dc116ea8b8a33d09b8cb85809984be10702f286e0f08a9d89L101' target='_blank'>Link</a></div><div id='project'> Project Name: arraiy/torchgeometry</div><div id='commit'> Commit Name: 73a339fac0a9574ee16527ebf9b6d71073bb688b</div><div id='time'> Time: 2019-01-14</div><div id='author'> Author: kaplanonu@gmail.com</div><div id='file'> File Name: torchgeometry/conversions.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: convert_points_to_homogeneous</div><BR>