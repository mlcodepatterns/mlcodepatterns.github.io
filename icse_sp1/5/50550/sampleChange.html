<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        e = e / np.sqrt(self.d_head)

        &#47&#47 generate mask
        <a id="change">mask = th.zeros(batch_size, max_len_x, max_len_mem).to(e.device)</a>
        for i in range(batch_size):
            mask[i, :lengths_x[i], :lengths_mem[i]].fill_(1)
        mask = mask.unsqueeze(1)
        e.masked_fill_(mask == 0, -float(&quotinf&quot))</code></pre><h3>After Change</h3><pre><code class='java'>
        max_len_x = max(lengths_x)
        max_len_mem = max(lengths_mem)
        device = x.device
        <a id="change">lengths_x = th.tensor(lengths_x, dtype=th.int64, device=device)</a>
        lengths_mem = th.tensor(lengths_mem, dtype=th.int64, device=device)

        queries = self.proj_q(x).view(-1, self.num_heads, self.d_head)
        keys = self.proj_k(mem).view(-1, self.num_heads, self.d_head)
        values = self.proj_v(mem).view(-1, self.num_heads, self.d_head)

        &#47&#47 padding to (B, max_len_x/mem, num_heads, d_head)
        queries = F.pad_packed_tensor(queries, lengths_x, 0)
        keys = F.pad_packed_tensor(keys, lengths_mem, 0)
        values = F.pad_packed_tensor(values, lengths_mem, 0)

        &#47&#47 attention score with shape (B, num_heads, max_len_x, max_len_mem)
        e = th.einsum(&quotbxhd,byhd-&gt;bhxy&quot, queries, keys)
        &#47&#47 normalize
        e = e / np.sqrt(self.d_head)

        &#47&#47 generate mask
        mask = _gen_mask(lengths_x, lengths_mem, max_len_x, max_len_mem)
        <a id="change">e = e.masked_fill(mask == 0, -float(&quotinf&quot))</a>

        &#47&#47 apply softmax
        alpha = th.softmax(e, dim=-1)
        &#47&#47 the following line addresses the NaN issue, see</code></pre>