<html><h3>ac04dcf9ced65fdd2cafc5c967400cabf32d3c6a,tensorforce/tests/test_quickstart_example.py,TestQuickstartExample,test_example,#TestQuickstartExample#,34
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            env = OpenAIGym(&quotCartPole-v0&quot)

            &#47&#47 Create a Trust Region Policy Optimization agent
            agent = PPOAgent(config=<a id="change">Configuration(
                log_level=&quotinfo&quot,
                batch_size=4096,

                gae_lambda=0.97,
                learning_rate=0.001,
                entropy_penalty=0.01,
                epochs=5,
                optimizer_batch_size=512,
                loss_clipping=0.2,
                states=env.states,
                actions=env.actions,
                network=layered_network_builder([
                    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),
                    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
                ])
            )</a>)
            runner = Runner(agent=agent, environment=env)

            def episode_finished(r):</code></pre><h3>After Change</h3><pre><code class='java'>
        for _ in xrange(3):
            &#47&#47 Create an OpenAIgym environment
            env = OpenAIGym(&quotCartPole-v0&quot)
            config = <a id="change">Configuration(
                batch_size=4096,
                &#47&#47 Agent
                preprocessing=None,
                exploration=None,
                reward_preprocessing=None,
                &#47&#47 BatchAgent
                keep_last_timestep=True,  &#47&#47 not documented!
                &#47&#47 PPOAgent
                step_optimizer=dict(
                    type=&quotadam&quot,
                    learning_rate=1e-3
                ),
                optimization_steps=10,
                &#47&#47 Model
                scope=&quotppo&quot,
                discount=0.99,
                &#47&#47 DistributionModel
                distributions=None,  &#47&#47 not documented!!!
                entropy_regularization=0.01,
                &#47&#47 PGModel
                baseline_mode=None,
                baseline=None,
                baseline_optimizer=None,
                gae_lambda=None,
                normalize_rewards=False,
                &#47&#47 PGLRModel
                likelihood_ratio_clipping=0.2,
                &#47&#47 Logging
                log_level=&quotinfo&quot,
                &#47&#47 TensorFlow Summaries
                summary_logdir=None,
                summary_labels=[&quottotal-loss&quot],
                summary_frequency=1,
                &#47&#47 Distributed
                distributed=False,
                device=None
            )</a>

            network_spec = [
                dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),
                dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
            ]
            &#47&#47 Create a Trust Region Policy Optimization agent
            <a id="change">agent = PPOAgent(
                states_spec=env.states,
                actions_spec=env.actions,
                network_spec=network_spec,
                config=config
            )</a>
            runner = Runner(agent=agent, environment=env)

            def episode_finished(r):
                &#47&#47 Test if mean reward over 50 should ensure that learning took off</code></pre><img src="323060161.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/reinforceio/tensorforce/commit/ac04dcf9ced65fdd2cafc5c967400cabf32d3c6a#diff-e322391c2a1248fd92324f09827a4d6b87af6a76717eb8f492776fc9d41b155aL39' target='_blank'>Link</a></div><div id='project'> Project Name: reinforceio/tensorforce</div><div id='commit'> Commit Name: ac04dcf9ced65fdd2cafc5c967400cabf32d3c6a</div><div id='time'> Time: 2017-10-15</div><div id='author'> Author: mi.schaarschmidt@gmail.com</div><div id='file'> File Name: tensorforce/tests/test_quickstart_example.py</div><div id='class'> Class Name: TestQuickstartExample</div><div id='method'> Method Name: test_example</div><BR><BR><div id='link'><a href='https://github.com/reinforceio/tensorforce/commit/e46c1ded997580101e5a5dd3ef0e6501e82f59af#diff-76ee19ac970860ef226791b2af61d0de2b340ecc07e724d3adc4ed3b30be168fL395' target='_blank'>Link</a></div><div id='project'> Project Name: reinforceio/tensorforce</div><div id='commit'> Commit Name: e46c1ded997580101e5a5dd3ef0e6501e82f59af</div><div id='time'> Time: 2017-10-16</div><div id='author'> Author: mi.schaarschmidt@gmail.com</div><div id='file'> File Name: tensorforce/tests/test_tutorial_code.py</div><div id='class'> Class Name: TestTutorialCode</div><div id='method'> Method Name: test_blogpost_introduction_runner</div><BR><BR><div id='link'><a href='https://github.com/reinforceio/tensorforce/commit/863b8dee69df21ff479b0f28422f2bf2b14f05bd#diff-0874f112033c11d75839017d9f3948cc6ea2c8cd058ae2b82b174650929756a9L29' target='_blank'>Link</a></div><div id='project'> Project Name: reinforceio/tensorforce</div><div id='commit'> Commit Name: 863b8dee69df21ff479b0f28422f2bf2b14f05bd</div><div id='time'> Time: 2017-10-15</div><div id='author'> Author: mi.schaarschmidt@gmail.com</div><div id='file'> File Name: examples/quickstart.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>