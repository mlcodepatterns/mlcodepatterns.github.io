<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = (<a id="change">(</a>x - mean).pow(<a id="change">2</a>).sum(<a id="change">-1</a>, keepdim=True).div(x.size(<a id="change">-1</a>) - 1) + self.eps).sqrt()
        <a id="change">d = (std + self.eps) + self.b</a>
        <a id="change">return self.a * (x - mean) / d</a>


def pytorch_lstm(insz, hsz, rnntype, nlayers, dropout, unif=0, batch_first=False, initializer=None):
    if nlayers == 1:</code></pre><h3>After Change</h3><pre><code class='java'>
    sub-sequent `FFN` sub-layer.

    There are 3 uses of multi-head attention in the Transformer.
    For encoder-decoder l<a id="change">ayers, the queries come from the </a>previous d<a id="change">ecoder layer, a</a>nd the memory<a id="change"> keys come from
    the encoder.  For encod</a>er layers<a id="change">, the K,</a> Q and V all come from the output of the previous layer of the encoder.
    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using
    future values
    </code></pre>