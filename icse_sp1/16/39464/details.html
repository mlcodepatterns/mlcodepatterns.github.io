<html><h3>78a7f7de24c34235d0784a5781f46de34d2336eb,python/eight_mile/pytorch/layers.py,LayerNorm,forward,#LayerNorm#Any#,408
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = (<a id="change">(</a>x - mean).pow(<a id="change">2</a>).sum(<a id="change">-1</a>, keepdim=True).div(x.size(<a id="change">-1</a>) - 1) + self.eps).sqrt()
        <a id="change">d = (std + self.eps) + self.b</a>
        <a id="change">return self.a * (x - mean) / d</a>


def pytorch_lstm(insz, hsz, rnntype, nlayers, dropout, unif=0, batch_first=False, initializer=None):
    if nlayers == 1:</code></pre><h3>After Change</h3><pre><code class='java'>
    sub-sequent `FFN` sub-layer.

    There are 3 uses of multi-head attention in the Transformer.
    For encoder-decoder l<a id="change">ayers, the queries come from the </a>previous d<a id="change">ecoder layer, a</a>nd the memory<a id="change"> keys come from
    the encoder.  For encod</a>er layers<a id="change">, the K,</a> Q and V all come from the output of the previous layer of the encoder.
    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using
    future values
    </code></pre><img src="189314120.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 13</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dpressel/mead-baseline/commit/78a7f7de24c34235d0784a5781f46de34d2336eb#diff-c41fa94148789d4fdd61157e7ff95333b730d8f4e1f417ecfcec8007cafc71caL408' target='_blank'>Link</a></div><div id='project'> Project Name: dpressel/mead-baseline</div><div id='commit'> Commit Name: 78a7f7de24c34235d0784a5781f46de34d2336eb</div><div id='time'> Time: 2019-10-29</div><div id='author'> Author: dpressel@gmail.com</div><div id='file'> File Name: python/eight_mile/pytorch/layers.py</div><div id='class'> Class Name: LayerNorm</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/6ad34e5887f847aeb9f681e8f290d5877b76e52b#diff-11930f0302250974226de0a096f71e9da6996e1ee6b5430a5aad5520deffb583L105' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: 6ad34e5887f847aeb9f681e8f290d5877b76e52b</div><div id='time'> Time: 2020-05-13</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: torch_geometric/nn/models/schnet.py</div><div id='class'> Class Name: GaussianSmearing</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/d08f673481a2cfaa0ce702e80cc22cdd25e600e5#diff-11930f0302250974226de0a096f71e9da6996e1ee6b5430a5aad5520deffb583L105' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: d08f673481a2cfaa0ce702e80cc22cdd25e600e5</div><div id='time'> Time: 2020-05-31</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: torch_geometric/nn/models/schnet.py</div><div id='class'> Class Name: GaussianSmearing</div><div id='method'> Method Name: forward</div><BR>