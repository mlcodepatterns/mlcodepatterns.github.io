<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    

    use_double = int(x_gpu.dtype in [np.float64, np.complex128])
    <a id="change">use_complex = int(x_gpu.dtype in [np.complex64, np.complex128])</a>
    real_type = np.float64 if use_double else np.float32

    &#47&#47 Set this to False when debugging to make sure the compiled kernel is
    &#47&#47 not cached:
    <a id="change">cache_dir = None</a>
    maxabs_mod = \
               SourceModule(maxabs_mod_template.substitute(use_double=use_double,
                                                           use_complex=use_complex),
                            cache_dir=cache_dir)

    <a id="change">maxabs = maxabs_mod.get_function("maxabs")</a>
    m_gpu = gpuarray.empty(1, real_type)
    maxabs(x_gpu, m_gpu, <a id="change">np.uint32(x_gpu.size)</a>,
           block=(1, 1, 1), grid=(1, 1))

    return m_gpu</code></pre><h3>After Change</h3><pre><code class='java'>
    

    try:
        func = maxabs.cache[<a id="change">x_gpu.dtype</a>]
    except KeyError:
        ctype = tools.dtype_to_ctype(x_gpu.dtype)
        use_double = int(x_gpu.dtype in [np.float64, np.complex128])        
        ret_type = np.float64 if use_double else np.float32
        func = reduction.ReductionKernel(ret_type, neutral="0",
                                           reduce_expr="max(a,b)", 
                                           map_expr="abs(x[i])",
                                           arguments="{ctype} *x".format(ctype=ctype))
        maxabs.cache[x_gpu.dtype] = func
    return <a id="change">func(x_gpu)</a>
maxabs.cache = {}

cumsum_template = Template(
&#47&#47include &lt;pycuda-complex.hpp&gt;</code></pre>