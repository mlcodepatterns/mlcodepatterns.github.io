<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            z = beta[0] + np.dot(X, beta[1:])       &#47&#47 cache z

        &#47&#47 Initialize loss accumulators
        <a id="change">L, DL = list(), list()</a>
        for t in range(0, self.max_iter):
            if self.solver == &quotbatch-gradient&quot:
                grad = _grad_L2loss(self.distr,
                                    alpha, self.Tau,</code></pre><h3>After Change</h3><pre><code class='java'>
                beta_old = deepcopy(beta)
                beta, z = \
                    self._cdfast(X, y, z, ActiveSet, beta, reg_lambda)
                <a id="change">if t &gt; 1:
                    if np.linalg.norm(beta - beta_old) / \
                        np.linalg.norm(beta_old) &lt; tol / lr:
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
            &#47&#47 Apply proximal operator
           </a> beta[1:] = self._prox(beta[1:], reg_lambda * alpha)

            &#47&#47 Update active set
            if self.solver == &quotcdfast&quot:</code></pre>