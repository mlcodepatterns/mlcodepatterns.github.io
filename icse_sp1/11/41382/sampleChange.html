<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            TensorType: Q-values tensor of shape [BATCH_SIZE, 1].
        
        &#47&#47 Continuous case -&gt; concat actions to model_out.
        <a id="change">if actions is not None:
            return self.twin_q_net([model_out, actions])
        &#47&#47 Discrete case -&gt; return q-vals for all actions.
        else:
            return self.twin_q_net(model_out)

   </a> def get_policy_output(self, model_out: TensorType) -&gt; TensorType:
        Returns policy outputs, given the output of self.__call__().

        For continuous action spaces, these will be the mean/stddev</code></pre><h3>After Change</h3><pre><code class='java'>
        Returns:
            TensorType: Q-values tensor of shape [BATCH_SIZE, 1].
        
        <a id="change">return self._get_q_value(model_out, actions, self.twin_q_net)</a>

    def _get_q_value(self, model_out, actions, net):
        &#47&#47 Model outs may come as original Tuple/Dict observations, concat them
        &#47&#47 here if this is the case.</code></pre>