<html><h3>e170a8bad93da7676dadfdc8be32072a7bb46f75,official/transformer/v2/transformer_main.py,TransformerTask,train,#TransformerTask#,201
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    cased_score, uncased_score = None, None
    cased_score_history, uncased_score_history = [], []
    <a id="change">for i in range(1, iterations + 1):
      print("Start train iteration:{}/{}".format(i, iterations))
      history = None
      if params["use_ctl"]:
        if not self.use_tpu:
          raise NotImplementedError(
              "Custom training loop on GPUs is not implemented.")
        train_steps_per_eval = tf.convert_to_tensor(
            flags_obj.steps_between_evals, dtype=tf.int32)

        &#47&#47 Runs training steps.
        train_steps(train_ds_iterator, train_steps_per_eval)
        train_loss = train_loss_metric.result().numpy().astype(float)
        logging.info("Train Step: %d/%d / loss = %s",
                     i * flags_obj.steps_between_evals, flags_obj.train_steps,
                     train_loss)

        checkpoint_name = checkpoint.save(
            os.path.join(
                flags_obj.model_dir,
                "ctl_step_{}.ckpt".format(i * flags_obj.steps_between_evals)))
        logging.info("Saved checkpoint to %s", checkpoint_name)
      else:
        if self.use_tpu:
          raise NotImplementedError(
              "Keras model.fit on TPUs is not implemented.")
        history = model.fit(
            train_ds,
            initial_epoch=i - 1,
            epochs=i,
            steps_per_epoch=flags_obj.steps_between_evals,
            callbacks=callbacks,
            &#47&#47 If TimeHistory is enabled, progress bar would be messy. Increase
            &#47&#47 the verbose level to get rid of it.
            verbose=(2 if flags_obj.enable_time_history else 1))
        logging.info("Train history: {}".format(history.history))

      print("End train iteration:{}/{} global step:{}".format(
          i,
          iterations,
          i*flags_obj.steps_between_evals))

      if (flags_obj.bleu_source and flags_obj.bleu_ref):
        uncased_score, cased_score = self.eval()
        cased_score_history.append([i, cased_score])
        uncased_score_history.append([i, uncased_score])

   </a> stats = ({
        "loss": train_loss
    } if history is None else misc.build_stats(history, callbacks))
    if uncased_score and cased_score:</code></pre><h3>After Change</h3><pre><code class='java'>
      model = transformer.create_model(params, is_train=True)
      opt = self._create_optimizer()

      <a id="change">current_step = 0</a>
      checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)
      latest_checkpoint = tf.train.latest_checkpoint(flags_obj.model_dir)
      if latest_checkpoint:
        checkpoint.restore(latest_checkpoint)
        logging.info("Loaded checkpoint %s", latest_checkpoint)
        current_step = opt.iterations.numpy()

      if params["use_ctl"]:
        train_loss_metric = tf.keras.metrics.Mean(
            "training_loss", dtype=tf.float32)
      else:
        model.compile(opt)

    model.summary()

    if self.use_tpu:
      &#47&#47 Different from experimental_distribute_dataset,
      &#47&#47 experimental_distribute_datasets_from_function requires
      &#47&#47 per-replica/local batch size.
      params["batch_size"] /= self.distribution_strategy.num_replicas_in_sync
      train_ds = (
          self.distribution_strategy
          .experimental_distribute_datasets_from_function(
              lambda ctx: data_pipeline.train_input_fn(params, ctx)))
    else:
      train_ds = data_pipeline.train_input_fn(params)
      map_data_fn = data_pipeline.map_data_for_transformer_fn
      train_ds = train_ds.map(
          map_data_fn, num_parallel_calls=params["num_parallel_calls"])
    if params["use_ctl"]:
      train_ds_iterator = iter(train_ds)

    callbacks = self._create_callbacks(flags_obj.model_dir, 0, params)

    &#47&#47 TODO(b/139418525): Refactor the custom training loop logic.
    @tf.function
    def train_steps(iterator, steps):
      Training steps function for TPU runs.

      Args:
        iterator: The input iterator of the training dataset.
        steps: An integer, the number of training steps.

      Returns:
        A float, the loss value.
      

      def _step_fn(inputs):
        Per-replica step function.
        inputs, targets = inputs
        with tf.GradientTape() as tape:
          logits = model([inputs, targets], training=True)
          loss = metrics.transformer_loss(logits, targets,
                                          params["label_smoothing"],
                                          params["vocab_size"])
          &#47&#47 Scales the loss, which results in using the average loss across all
          &#47&#47 of the replicas for backprop.
          scaled_loss = loss / self.distribution_strategy.num_replicas_in_sync

        &#47&#47 De-dupes variables due to keras tracking issues.
        tvars = list(
            object_identity.ObjectIdentitySet(model.trainable_variables))
        grads = tape.gradient(scaled_loss, tvars)
        opt.apply_gradients(zip(grads, tvars))
        &#47&#47 For reporting, the metric takes the mean of losses.
        train_loss_metric.update_state(loss)

      for _ in tf.range(steps):
        train_loss_metric.reset_states()
        self.distribution_strategy.experimental_run_v2(
            _step_fn, args=(next(iterator),))

    cased_score, uncased_score = None, None
    cased_score_history, uncased_score_history = [], []
    <a id="change">while current_step &lt; flags_obj.train_steps:
      remaining_steps = flags_obj.train_steps - current_step
      train_steps_per_eval = (
          remaining_steps if remaining_steps &lt; flags_obj.steps_between_evals
          else flags_obj.steps_between_evals)
      current_iteration = current_step // flags_obj.steps_between_evals

      print("Start train iteration at global step:{}".format(current_step))
      history = None
      if params["use_ctl"]:
        if not self.use_tpu:
          raise NotImplementedError(
              "Custom training loop on GPUs is not implemented.")
        &#47&#47 Runs training steps.
        train_steps(train_ds_iterator,
                    tf.convert_to_tensor(train_steps_per_eval, dtype=tf.int32))
        current_step += train_steps_per_eval
        train_loss = train_loss_metric.result().numpy().astype(float)
        logging.info("Train Step: %d/%d / loss = %s",
                     current_step, flags_obj.train_steps, train_loss)

        checkpoint_name = checkpoint.save(
            os.path.join(
                flags_obj.model_dir,
                "ctl_step_{}.ckpt".format(current_step)))
        logging.info("Saved checkpoint to %s", checkpoint_name)
      else:
        if self.use_tpu:
          raise NotImplementedError(
              "Keras model.fit on TPUs is not implemented.")
        history = model.fit(
            train_ds,
            initial_epoch=current_iteration,
            epochs=current_iteration + 1,
            steps_per_epoch=train_steps_per_eval,
            callbacks=callbacks,
            &#47&#47 If TimeHistory is enabled, progress bar would be messy. Increase
            &#47&#47 the verbose level to get rid of it.
            verbose=(2 if flags_obj.enable_time_history else 1))
        current_step += train_steps_per_eval
        logging.info("Train history: {}".format(history.history))

      print("End train iteration at global step:{}".format(current_step))

      if (flags_obj.bleu_source and flags_obj.bleu_ref):
        uncased_score, cased_score = self.eval()
        cased_score_history.append([current_iteration + 1, cased_score])
        uncased_score_history.append([current_iteration + 1, uncased_score])

   </a> stats = ({
        "loss": train_loss
    } if history is None else misc.build_stats(history, callbacks))
    if uncased_score and cased_score:</code></pre><img src="32729490.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/e170a8bad93da7676dadfdc8be32072a7bb46f75#diff-1f7207ecaf127124ee22dc55b178987056c4714adbbcd568660f697568778fccL201' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: e170a8bad93da7676dadfdc8be32072a7bb46f75</div><div id='time'> Time: 2019-08-30</div><div id='author'> Author: gardener@tensorflow.org</div><div id='file'> File Name: official/transformer/v2/transformer_main.py</div><div id='class'> Class Name: TransformerTask</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/lengstrom/fast-style-transfer/commit/31bfc7c1ce9dac771dbd5e2edf2d5e789d8b1aaa#diff-24edcdcba3747a424b769b374189e8346f6c2f7bdb1e16d2d9ffbe9b926d45a7L26' target='_blank'>Link</a></div><div id='project'> Project Name: lengstrom/fast-style-transfer</div><div id='commit'> Commit Name: 31bfc7c1ce9dac771dbd5e2edf2d5e789d8b1aaa</div><div id='time'> Time: 2016-12-23</div><div id='author'> Author: jonathan.e.m.bocker@gmail.com</div><div id='file'> File Name: evaluate.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: from_pipe</div><BR><BR><div id='link'><a href='https://github.com/TheAlgorithms/Python/commit/e58a5e68424df74a4c7b30df04162c775044405c#diff-7173fe43966737b28390b5a18c47336b00a8903af9b110d3d5a8450fafdbba3bL46' target='_blank'>Link</a></div><div id='project'> Project Name: TheAlgorithms/Python</div><div id='commit'> Commit Name: e58a5e68424df74a4c7b30df04162c775044405c</div><div id='time'> Time: 2019-07-30</div><div id='author'> Author: 44649323+FrogBattle@users.noreply.github.com</div><div id='file'> File Name: sorts/tim_sort.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: tim_sort</div><BR>