<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        name = &quot&quot

    if args_.max_vocab_size:
        <a id="change">if args_.embedding_path and &quot.bin&quot in args_.embedding_path:
            raise NotImplementedError(
                &quotNot implemented for binary fastText model.&quot)

       </a> size = min(<a id="change">len(token_embedding._idx_to_token)</a>, args_.max_vocab_size)
        <a id="change">token_embedding._idx_to_token = token_embedding._idx_to_token[:size]</a>
        token_embedding._idx_to_vec = token_embedding._idx_to_vec[:size]
        <a id="change">token_embedding._token_to_idx = {
            token: idx
            for idx, token in enumerate(token_embedding._idx_to_token)
        }</a>

    similarity_results = evaluation.evaluate_similarity(
        args_, token_embedding, ctx, logfile=os.path.join(
            args_.logdir, &quotsimilarity{}.tsv&quot.format(name)))</code></pre><h3>After Change</h3><pre><code class='java'>
    enforce_max_size(token_embedding_, args_.max_vocab_size)
    known_tokens = set(token_embedding_.idx_to_token)
    &#47&#47 Auto-extend token_embedding with unknown extra eval tokens
    <a id="change">if token_embedding_.unknown_lookup is not None:
        eval_tokens = evaluation.get_tokens_in_evaluation_datasets(args_)
        &#47&#47 pylint: disable=pointless-statement
        token_embedding_[[
            t for t in eval_tokens - known_tokens
            if t in token_embedding_.unknown_lookup
        ]]

        if len(token_embedding_.idx_to_token) &gt; args_.max_vocab_size:
            logging.warning(&quotComputing embeddings for OOV words that occur &quot
                            &quotin the evaluation dataset lead to having &quot
                            &quotmore words than --max-vocab-size. &quot
                            &quotHave %s words (--max-vocab-size %s)&quot,
                            len(token_embedding_.idx_to_token),
                            args_.max_vocab_size)

   </a> similarity_results = evaluation.evaluate_similarity(
        args_, token_embedding_, ctx, logfile=os.path.join(
            args_.logdir, &quotsimilarity{}.tsv&quot.format(name)))
    analogy_results = evaluation.evaluate_analogy(</code></pre>