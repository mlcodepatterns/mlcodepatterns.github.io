<html><h3>563207ae6bd9a8565243b04dcd92af9c33bac524,deepchem/models/keras_model.py,KerasModel,fit_generator,#KerasModel#Any#Any#Any#Any#Any#Any#Any#,246
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        outputs = self.model(inputs)
        if isinstance(outputs, tf.Tensor):
          outputs = [outputs]
        <a id="change">if self._loss_outputs is not None:
          outputs = [outputs[i] for i in self._loss_outputs]
       </a> batch_loss = loss(outputs, labels, weights)
      if variables is None:
        vars = self.model.trainable_variables
      else:</code></pre><h3>After Change</h3><pre><code class='java'>
      loss = self._loss_fn
    var_key = None
    if variables is not None:
      var_key = <a id="change">tuple(v.experimental_ref() for v in variables)</a>

      &#47&#47 The optimizer creates internal variables the first time apply_gradients()
      &#47&#47 is called for a new set of variables.  If that happens inside a function
      &#47&#47 annotated with tf.function it throws an exception, so call it once here.

      zero_grads = [tf.zeros(v.shape) for v in variables]
      self._tf_optimizer.apply_gradients(zip(zero_grads, variables))
    if var_key not in self._gradient_fn_for_vars:
      self._gradient_fn_for_vars[var_key] = self._create_gradient_fn(variables)
    <a id="change">apply_gradient_for_batch = self._gradient_fn_for_vars[var_key]</a>
    time1 = time.time()

    &#47&#47 Main training loop.
</code></pre><img src="174688136.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/deepchem/deepchem/commit/563207ae6bd9a8565243b04dcd92af9c33bac524#diff-6959c92eed13049850ed08d423ac054a0f3781b89ba52eceaf68523bbc0cde89L246' target='_blank'>Link</a></div><div id='project'> Project Name: deepchem/deepchem</div><div id='commit'> Commit Name: 563207ae6bd9a8565243b04dcd92af9c33bac524</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: peastman@stanford.edu</div><div id='file'> File Name: deepchem/models/keras_model.py</div><div id='class'> Class Name: KerasModel</div><div id='method'> Method Name: fit_generator</div><BR><BR><div id='link'><a href='https://github.com/deepgram/kur/commit/35ed48386992d824973d8ed39cfa299614b7cd34#diff-9cf419b240430d6c470fb79e27ddaed17581d2ff4f26f697fcb58b14171ce2d3L225' target='_blank'>Link</a></div><div id='project'> Project Name: deepgram/kur</div><div id='commit'> Commit Name: 35ed48386992d824973d8ed39cfa299614b7cd34</div><div id='time'> Time: 2017-02-28</div><div id='author'> Author: ajsyp@syptech.net</div><div id='file'> File Name: kur/loggers/binary_logger.py</div><div id='class'> Class Name: BinaryLogger</div><div id='method'> Method Name: load_statistic</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/5f6c9b8ca29b56ab2d2e1acfc38ea61001b92c4b#diff-62ccfa30b3a30fa7466e46abfd7c9587f63f3c1b7eeaba23409528ade3efa02eL164' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 5f6c9b8ca29b56ab2d2e1acfc38ea61001b92c4b</div><div id='time'> Time: 2018-02-07</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: allennlp/common/testing/model_test_case.py</div><div id='class'> Class Name: ModelTestCase</div><div id='method'> Method Name: ensure_batch_predictions_are_consistent</div><BR>