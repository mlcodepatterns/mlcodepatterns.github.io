<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            z = beta[0] + np.dot(X, beta[1:])       &#47&#47 cache z

        &#47&#47 Initialize loss accumulators
        <a id="change">L, DL = list(), list()</a>
        for t in range(0, self.max_iter):
            if self.solver == &quotbatch-gradient&quot:
                grad = _grad_L2loss(self.distr,
                                    alpha, self.Tau,</code></pre><h3>After Change</h3><pre><code class='java'>
                                    reg_lambda, X, y, self.eta,
                                    beta)
                if t &gt; 1:
                    <a id="change">if np.linalg.norm(grad) / np.linalg.norm(beta) &lt; tol / lr:
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
               </a> beta = beta - self.learning_rate * grad

            elif self.solver == &quotcdfast&quot:
                <a id="change">beta_old = deepcopy(beta)</a>
                beta, z = \
                    self._cdfast(X, y, z, ActiveSet, beta, reg_lambda)
                if t &gt; 1:
                    if np.linalg.norm(beta - beta_old) / \</code></pre>