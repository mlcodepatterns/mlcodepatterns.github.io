<html><h3>2c4a6e537126f4123de7c97f30587310d3712c06,tests/data/tokenizers/character_tokenizer_test.py,TestCharacterTokenizer,test_splits_into_characters,#TestCharacterTokenizer#,7
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def test_splits_into_characters(self):
        tokenizer = CharacterTokenizer(start_tokens=[&quot&lt;S1&gt;&quot, &quot&lt;S2&gt;&quot], end_tokens=[&quot&lt;/S2&gt;&quot, &quot&lt;/S1&gt;&quot])
        sentence = "A, small sentence."
        <a id="change">tokens, _ = tokenizer.tokenize(sentence)</a>
        expected_tokens = ["&lt;S1&gt;", "&lt;S2&gt;", "A", ",", " ", "s", "m", "a", "l", "l", " ", "s", "e",
                           "n", "t", "e", "n", "c", "e", ".", &quot&lt;/S2&gt;&quot, &quot&lt;/S1&gt;&quot]
        assert tokens == expected_tokens
</code></pre><h3>After Change</h3><pre><code class='java'>
    def test_splits_into_characters(self):
        tokenizer = CharacterTokenizer(start_tokens=[&quot&lt;S1&gt;&quot, &quot&lt;S2&gt;&quot], end_tokens=[&quot&lt;/S2&gt;&quot, &quot&lt;/S1&gt;&quot])
        sentence = "A, small sentence."
        <a id="change">tokens = [t.text for t in tokenizer.tokenize(sentence)]</a>
        expected_tokens = ["&lt;S1&gt;", "&lt;S2&gt;", "A", ",", " ", "s", "m", "a", "l", "l", " ", "s", "e",
                           "n", "t", "e", "n", "c", "e", ".", &quot&lt;/S2&gt;&quot, &quot&lt;/S1&gt;&quot]
        assert tokens == expected_tokens
</code></pre><img src="84393199.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/2c4a6e537126f4123de7c97f30587310d3712c06#diff-68c74fcdad81055c248331d55789df8cf3efec77d63f3530101be8a440a25d26L10' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 2c4a6e537126f4123de7c97f30587310d3712c06</div><div id='time'> Time: 2017-09-13</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: tests/data/tokenizers/character_tokenizer_test.py</div><div id='class'> Class Name: TestCharacterTokenizer</div><div id='method'> Method Name: test_splits_into_characters</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/2c4a6e537126f4123de7c97f30587310d3712c06#diff-68c74fcdad81055c248331d55789df8cf3efec77d63f3530101be8a440a25d26L18' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 2c4a6e537126f4123de7c97f30587310d3712c06</div><div id='time'> Time: 2017-09-13</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: tests/data/tokenizers/character_tokenizer_test.py</div><div id='class'> Class Name: TestCharacterTokenizer</div><div id='method'> Method Name: test_handles_byte_encoding</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/2c4a6e537126f4123de7c97f30587310d3712c06#diff-2c0f7434cc68ae58bc9d165577e28096a04424a832d4dc742d64a33cd7c4e697L11' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 2c4a6e537126f4123de7c97f30587310d3712c06</div><div id='time'> Time: 2017-09-13</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: tests/data/tokenizers/word_tokenizer_test.py</div><div id='class'> Class Name: TestWordTokenizer</div><div id='method'> Method Name: test_passes_through_correctly</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/2c4a6e537126f4123de7c97f30587310d3712c06#diff-2c0f7434cc68ae58bc9d165577e28096a04424a832d4dc742d64a33cd7c4e697L21' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 2c4a6e537126f4123de7c97f30587310d3712c06</div><div id='time'> Time: 2017-09-13</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: tests/data/tokenizers/word_tokenizer_test.py</div><div id='class'> Class Name: TestWordTokenizer</div><div id='method'> Method Name: test_stems_and_filters_correctly</div><BR>