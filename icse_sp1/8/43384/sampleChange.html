<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
advantages = tf.placeholder(tf.float32, name="reward_signal")

&#47&#47 FIXME this doesn&quott work as a deep copy
<a id="change">score_old = tf.identity(score, name="score_old")</a>

&#47&#47 The loss function. This sends the weights in the direction of making actions
&#47&#47 that gave good advantage (reward over time) more likely, and actions that
&#47&#47 didn&quott less likely.</code></pre><h3>After Change</h3><pre><code class='java'>
input_y = tf.placeholder(tf.float32, [None, Out], name="input_y")
advantages = tf.placeholder(tf.float32, name="reward_signal")

score_old = tf.placeholder(tf.float32, <a id="change">[None, Out * 2]</a>, name="score_old")

&#47&#47 The loss function. This sends the weights in the direction of making actions
&#47&#47 that gave good advantage (reward over time) more likely, and actions that
&#47&#47 didn&quott less likely.
&#47&#47 Log likelihood: https://en.wikipedia.org/wiki/Multivariate_normal_distribution
&#47&#47 &#47&#47Likelihood_function
stds = tf.abs(score[0][Out:])  &#47&#47 TODO Use log_std instead
means = tf.abs(score[0][:Out])
stds_old = tf.abs(score_old[0][Out:])
means_old = tf.abs(score_old[0][:Out])
<a id="change">zs = (input_y - means) / stds</a>
zs_old = <a id="change">(input_y - means_old) / stds_old</a>
loglik_old = -0.5 * (
    tf.reduce_sum(tf.log(stds_old), axis=-1) + tf.reduce_sum(tf.square(zs_old),
                                                             axis=-1) + Out * tf.log(
        2 * np.pi))
loglik_new = -0.5 * (
    tf.reduce_sum(tf.log(stds), axis=-1) + tf.reduce_sum(tf.square(zs),
                                                         axis=-1) + Out * tf.log(
        2 * np.pi))
&#47&#47 FIXME this is giving nan values :(
<a id="change">likratio = tf.exp(loglik_new - loglik_old)</a>
loss = -tf.reduce_mean(likratio * advantages)
&#47&#47 loss = -tf.reduce_mean(loglik_new * advantages)
newGrads = tf.gradients(loss, tvars)
</code></pre>