<html><h3>174dd18905f12bfe0b418a4bad3575e6331478a4,python/dgl/nn/pytorch/glob.py,MultiHeadAttention,forward,#MultiHeadAttention#Any#Any#Any#Any#,663
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 generate mask
        mask = th.zeros(batch_size, max_len_x, max_len_mem).to(e.device)
        for i in <a id="change">range(batch_size)</a>:
            mask[i, :lengths_x[i], :lengths_mem[i]].fill_(1)
        mask = mask.unsqueeze(1)
        e.masked_fill_(mask == 0, -float(&quotinf&quot))</code></pre><h3>After Change</h3><pre><code class='java'>
        max_len_x = max(lengths_x)
        max_len_mem = max(lengths_mem)
        device = x.device
        lengths_x = th.tensor(lengths_x, dtype=<a id="change">th.int64</a>, device=device)
        <a id="change">lengths_mem = th.tensor(lengths_mem, dtype=th.int64, device=device)</a>

        queries = self.proj_q(x).view(-1, self.num_heads, self.d_head)
        keys = self.proj_k(mem).view(-1, self.num_heads, self.d_head)
        values = self.proj_v(mem).view(-1, self.num_heads, self.d_head)

        &#47&#47 padding to (B, max_len_x/mem, num_heads, d_head)
        queries = F.pad_packed_tensor(queries, lengths_x, 0)
        keys = F.pad_packed_tensor(keys, lengths_mem, 0)
        values = F.pad_packed_tensor(values, lengths_mem, 0)

        &#47&#47 attention score with shape (B, num_heads, max_len_x, max_len_mem)
        e = th.einsum(&quotbxhd,byhd-&gt;bhxy&quot, queries, keys)
        &#47&#47 normalize
        e = e / np.sqrt(self.d_head)

        &#47&#47 generate mask
        mask = _gen_mask(lengths_x, lengths_mem, max_len_x, max_len_mem)
        <a id="change">e = e.masked_fill(mask == 0, -float(&quotinf&quot))</a>

        &#47&#47 apply softmax
        alpha = th.softmax(e, dim=-1)
        &#47&#47 the following line addresses the NaN issue, see
        &#47&#47 https://github.com/dmlc/dgl/issues/2657
        <a id="change">alpha = alpha.masked_fill(mask == 0, 0.)</a>

        &#47&#47 sum of value weighted by alpha
        out = th.einsum(&quotbhxy,byhd-&gt;bxhd&quot, alpha, values)
        &#47&#47 project to output</code></pre><img src="174788843.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dmlc/dgl/commit/174dd18905f12bfe0b418a4bad3575e6331478a4#diff-fe817ca0cc1dee2841f4fece352be7cd7640af0d38d361246c64abb180d17e29L678' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/dgl</div><div id='commit'> Commit Name: 174dd18905f12bfe0b418a4bad3575e6331478a4</div><div id='time'> Time: 2021-02-22</div><div id='author'> Author: expye@outlook.com</div><div id='file'> File Name: python/dgl/nn/pytorch/glob.py</div><div id='class'> Class Name: MultiHeadAttention</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/dmlc/dgl/commit/cf8a3fb30547d6e980ecd8182f64a51df8e55c62#diff-2f891bb8d662a969491d25652279d2c8917e0731a79b707ff75ce560648463d6L230' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/dgl</div><div id='commit'> Commit Name: cf8a3fb30547d6e980ecd8182f64a51df8e55c62</div><div id='time'> Time: 2021-02-10</div><div id='author'> Author: expye@outlook.com</div><div id='file'> File Name: python/dgl/backend/pytorch/tensor.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pad_packed_tensor</div><BR><BR><div id='link'><a href='https://github.com/dmlc/dgl/commit/174dd18905f12bfe0b418a4bad3575e6331478a4#diff-fe817ca0cc1dee2841f4fece352be7cd7640af0d38d361246c64abb180d17e29L678' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/dgl</div><div id='commit'> Commit Name: 174dd18905f12bfe0b418a4bad3575e6331478a4</div><div id='time'> Time: 2021-02-22</div><div id='author'> Author: expye@outlook.com</div><div id='file'> File Name: python/dgl/nn/pytorch/glob.py</div><div id='class'> Class Name: MultiHeadAttention</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/dmlc/dgl/commit/cf8a3fb30547d6e980ecd8182f64a51df8e55c62#diff-2f891bb8d662a969491d25652279d2c8917e0731a79b707ff75ce560648463d6L249' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/dgl</div><div id='commit'> Commit Name: cf8a3fb30547d6e980ecd8182f64a51df8e55c62</div><div id='time'> Time: 2021-02-10</div><div id='author'> Author: expye@outlook.com</div><div id='file'> File Name: python/dgl/backend/pytorch/tensor.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pack_padded_tensor</div><BR>