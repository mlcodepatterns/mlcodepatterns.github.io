<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
def _expectation(p, kern, feat, none2, none3):
    if not kern.on_separate_dimensions:
        raise NotImplementedError("Product currently needs to be defined on separate dimensions.")  &#47&#47 pragma: no cover
    <a id="change">with tf.control_dependencies([
        tf.assert_equal(tf.rank(p.var), 2,
                        message="Product currently only supports diagonal Xcov.", name="assert_Xcov_diag"),
    ]):
        _expectation_fn = lambda k: _expectation(p, k, feat, None, None)
        return functools.reduce(tf.multiply, [_expectation_fn(k) for k in kern.kern_list])


</a>@dispatch(DiagonalGaussian, kernels.Product, InducingPoints, kernels.Product, InducingPoints)
@quadrature_fallback
def _expectation(p, kern1, feat1, kern2, feat2):
    if feat1 != feat2:</code></pre><h3>After Change</h3><pre><code class='java'>
                                  "different Product kernels is not supported.")

    kern = kern1
    <a id="change">feat = feat1</a>

    if not kern.on_separate_dimensions:
        raise NotImplementedError(
            "Product currently needs to be defined on separate dimensions.")  &#47&#47 pragma: no cover

    return functools.reduce(tf.multiply, [
        expectation(p, <a id="change">(k, feat)</a>, (k, feat)) for k in kern.kern_list])


&#47&#47 ============== Conversion to Gaussian from Diagonal or Markov ===============</code></pre>