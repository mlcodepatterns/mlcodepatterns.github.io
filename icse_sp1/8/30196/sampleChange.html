<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            _input_var = deepcopy(p.input_var)
            if q is not None:
                _input_var += deepcopy(q.input_var)
                <a id="change">_input_var = sorted(set(_input_var), key=_input_var.index)</a>
        super().__init__(_input_var)
        self.p = p
        self.q = q
</code></pre><h3>After Change</h3><pre><code class='java'>
    ...     def forward(self, z):
    ...         return {"probs": torch.sigmoid(self.model(z))}
    ...
<a id="change">    &gt;&gt;&gt; p = Generator()
    &gt;&gt;&gt; q = Inference()
    &gt;&gt;&gt; prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),
 </a>   ...                var=["z"], features_shape=[64], name="p_{prior}")
    ...
    &gt;&gt;&gt; &#47&#47 Define a loss function (VAE)
    &gt;&gt;&gt; reconst = -p.log_prob().expectation(q)
    &gt;&gt;&gt; kl = KullbackLeibler(q,prior)
    &gt;&gt;&gt; loss_cls = (reconst - kl).mean()
    &gt;&gt;&gt; print(loss_cls)
    mean \\left(- D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] <a id="change">- \\mathbb{E}_{q(z|x</a>)} \\left[\\log p(x|z) \\right] \\right)
    &gt;&gt;&gt; &#47&#47 Evaluate this loss function
    &gt;&gt;&gt; data = torch.randn(1, 128)  &#47&#47 Pseudo data
    &gt;&gt;&gt; loss = loss_cls.eval({"x": data})</code></pre>