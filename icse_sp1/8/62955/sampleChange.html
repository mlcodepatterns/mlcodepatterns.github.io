<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    model = load_from_disk(model_path)

    precision_list = []
    <a id="change">recall_list = []</a>
    f1_list = []

    if verbose:
        print("Batches:", end=&quot &quot)

    no_more_samples = False
    batch_number = 0
    while not no_more_samples:
        batch_number += 1

        batch = []
        for i in xrange(batch_size):
            try:
                batch.append(doc_generator.next())
            except StopIteration:
                no_more_samples = True
                break

        X, answers, kw_vector = build_test_matrices(
            batch,
            model,
            testset_path,
            ontology,
        )

        &#47&#47 Predict
        y_pred = model.scale_and_predict(X)

        &#47&#47 Evaluate the results
        precision, recall = evaluate_results(
            y_pred,
            kw_vector,
            answers,
        )

        f1 = (2 * precision * recall) / (precision + recall)

        precision_list.append(precision)
        <a id="change">recall_list.append(recall)</a>
        f1_list.append(f1)

        if verbose:
            sys.stdout.write(b&quot.&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
    start_time = time.clock()

    all_metrics = [&quotmap&quot, &quotmrr&quot, &quotndcg&quot, &quotr_prec&quot, &quotp_at_3&quot, &quotp_at_5&quot]
    metrics_agg = <a id="change">{m: [] for m in all_metrics}</a>

    if verbose:
        print("Batches:", end=&quot &quot)

    no_more_samples = False
    batch_number = 0
    while not no_more_samples:
        batch_number += 1

        batch = []
        for i in xrange(batch_size):
            try:
                batch.append(doc_generator.next())
            except StopIteration:
                no_more_samples = True
                break

        X, answers, kw_vector = build_test_matrices(
            batch,
            model,
            testset_path,
            ontology,
        )

        &#47&#47 Predict
        y_pred = model.scale_and_predict_confidence(X)

        &#47&#47 Evaluate the results
        metrics = evaluate_results(
            y_pred,
            kw_vector,
            answers,
        )
        for k, v in metrics.iteritems():
            metrics_agg[k].append(v)

        if verbose:
            sys.stdout.write(b&quot.&quot)
            sys.stdout.flush()

    if verbose:
        print()
        print("Testing finished in: {0:.2f}s".format(time.clock() - start_time))

    return {k: np.mean(v) <a id="change">for</a> k, v in metrics_agg.iteritems()}


def train(</code></pre>