<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
N, D_in, H, D_out = 64, 1000, 100, 10

&#47&#47 Create random Tensors to hold input and outputs, and wrap them in Variables.
x = Variable(<a id="change">torch.randn(N, D_in).type(dtype)</a>, requires_grad=False)
y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)

&#47&#47 Create random Tensors for weights, and wrap them in Variables.</code></pre><h3>After Change</h3><pre><code class='java'>


dtype = torch.float
<a id="change">device = torch.device("cpu")</a>
&#47&#47 dtype = torch.device("cuda:0") &#47&#47 Uncomment this to run on GPU

&#47&#47 N is batch size; D_in is input dimension;
&#47&#47 H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

&#47&#47 Create random Tensors to hold input and outputs.
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = <a id="change">torch.randn(N, D_out, device=device, dtype=dtype)</a>

&#47&#47 Create random Tensors for weights.
w1 = <a id="change">torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)</a>
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    &#47&#47 To apply our Function, we use Function.apply method. We alias this as &quotrelu&quot.
    relu = MyReLU.apply

    &#47&#47 Forward pass: compute predicted y using operations; we compute
    &#47&#47 ReLU using our custom autograd operation.
    y_pred = relu(x.mm(w1)).mm(w2)

    &#47&#47 Compute and print loss
    loss = (y_pred - y).pow(2).sum()
    print(t, <a id="change">loss.item()</a>)

    &#47&#47 Use autograd to compute the backward pass.
    loss.backward()</code></pre>