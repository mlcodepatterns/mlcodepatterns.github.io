<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._policy_lr = policy_lr
        self._qf_lr = qf_lr

        <a id="change">super(TD3, self).__init__(env_spec=env_spec,
                                  policy=policy,
                                  qf=qf,
                                  replay_buffer=replay_buffer,
                                  discount=discount,
                                  steps_per_epoch=steps_per_epoch,
                                  max_path_length=max_path_length,
                                  max_eval_path_length=max_eval_path_length,
                                  n_train_steps=n_train_steps,
                                  buffer_batch_size=buffer_batch_size,
                                  min_buffer_size=min_buffer_size,
                                  rollout_batch_size=rollout_batch_size,
                                  reward_scale=reward_scale,
                                  smooth_return=smooth_return,
                                  exploration_policy=exploration_policy)</a>

    def init_opt(self):
        Build the loss function and init the optimizer.
        with tf.name_scope(self._name):</code></pre><h3>After Change</h3><pre><code class='java'>
    

    def __init__(
            <a id="change">self</a>,
            env_spec,
            policy,
            qf,
            qf2,
            replay_buffer,
            *,  &#47&#47 Everything after this is numbers.
            target_update_tau=0.01,
            policy_weight_decay=0,
            qf_weight_decay=0,
            policy_optimizer=tf.compat.v1.train.AdamOptimizer,
            qf_optimizer=tf.compat.v1.train.AdamOptimizer,
            policy_lr=_Default(1e-4),
            qf_lr=_Default(1e-3),
            clip_pos_returns=False,
            clip_return=np.inf,
            discount=0.99,
            max_action=None,
            name=&quotTD3&quot,
            steps_per_epoch=20,
            max_path_length=None,
            max_eval_path_length=None,
            n_train_steps=50,
            buffer_batch_size=64,
            min_buffer_size=1e4,
            rollout_batch_size=1,
            reward_scale=1.,
            exploration_policy_sigma=0.2,
            actor_update_period=2,
            exploration_policy_clip=0.5,
            smooth_return=True,
            exploration_policy=None):
        action_bound = env_spec.action_space.high
        self._max_action = action_bound if max_action is None else max_action
        self._tau = target_update_tau
        self._policy_weight_decay = policy_weight_decay
        self._qf_weight_decay = qf_weight_decay
        self._name = name
        self._clip_pos_returns = clip_pos_returns
        self._clip_return = clip_return
        self._success_history = deque(maxlen=100)

        self._episode_rewards = []
        self._episode_policy_losses = []
        self._episode_qf_losses = []
        self._epoch_ys = []
        self._epoch_qs = []

        self._target_policy = policy.clone(&quottarget_policy&quot)
        self._target_qf = qf.clone(&quottarget_qf&quot)

        self.qf2 = qf2
        self.qf = qf

        self._exploration_policy_sigma = exploration_policy_sigma
        self._exploration_policy_clip = exploration_policy_clip
        self._actor_update_period = actor_update_period
        self._action_loss = None

        self._target_qf2 = qf2.clone(&quottarget_qf2&quot)
        self._policy_optimizer = policy_optimizer
        self._qf_optimizer = qf_optimizer
        self._policy_lr = policy_lr
        self._qf_lr = qf_lr

        self._policy = policy
        <a id="change">self._n_train_steps = n_train_steps</a>

        <a id="change">self._min_buffer_size = min_buffer_size</a>
        <a id="change">self._qf = qf</a>
        <a id="change">self._steps_per_epoch = steps_per_epoch</a>
        <a id="change">self._n_train_steps = n_train_steps</a>
        self._buffer_batch_size = buffer_batch_size
        <a id="change">self._discount = discount</a>
        self._reward_scale = reward_scale
        <a id="change">self._smooth_return = smooth_return</a>
        self.max_path_length = max_path_length
        <a id="change">self._max_eval_path_length = max_eval_path_length</a>

        &#47&#47 used by OffPolicyVectorizedSampler
        <a id="change">self.env_spec = env_spec</a>
        <a id="change">self.rollout_batch_size = rollout_batch_size</a>
        self.replay_buffer = replay_buffer
        <a id="change">self.policy = policy</a>
        <a id="change">self.exploration_policy = exploration_policy</a>

        <a id="change">self.sampler_cls = OffPolicyVectorizedSampler</a>

        <a id="change">self.init_opt()</a>

    def init_opt(self):
        Build the loss function and init the optimizer.
        with tf.name_scope(self._name):</code></pre>