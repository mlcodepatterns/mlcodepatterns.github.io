<html><h3>e4f051b6cce414997a97b896276563c4e361d0b8,ch09/04_cartpole_pg.py,,,#,35
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if len(batch_states) &lt; BATCH_SIZE:
            continue

        <a id="change">states_v</a> = <a id="change">Variable(torch.from_numpy(np.array(batch_states, dtype=np.float32)))</a>
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_scale_v = <a id="change">Variable(torch.FloatTensor(batch_scales))</a>

        optimizer.zero_grad()
        logits_v = net(states_v)
        <a id="change">log_prob_v</a> = F.log_softmax(logits_v, dim=1)
        <a id="change">log_prob_actions_v</a> = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]
        <a id="change">loss_policy_v</a> = -log_prob_actions_v.mean()

        prob_v = F.softmax(logits_v, dim=1)
        <a id="change">entropy_v</a> = -(prob_v * log_prob_v).sum(dim=1).mean()
        <a id="change">entropy_loss_v</a> = -ENTROPY_BETA * entropy_v
        loss_v = loss_policy_v + entropy_loss_v

        loss_v.backward()
        optimizer.step()

        &#47&#47 calc KL-div
        new_logits_v = net(states_v)
        new_prob_v = F.softmax(new_logits_v, dim=1)
        <a id="change">kl_div_v</a> = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()
        writer.add_scalar("kl", <a id="change">kl_div_v.data.cpu().numpy()[0]</a>, step_idx)

        grad_max = 0.0
        grad_means = 0.0
        grad_count = 0
        for p in net.parameters():
            grad_max = max(grad_max, p.grad.abs().max().data.cpu().numpy()[0])
            grad_means += (p.grad ** 2).mean().sqrt().data.cpu().numpy()[0]
            grad_count += 1

        writer.add_scalar("baseline", baseline, step_idx)
        writer.add_scalar("entropy", <a id="change">entropy_v</a>.data.cpu().numpy()[0], step_idx)
        writer.add_scalar("batch_scales", np.mean(batch_scales), step_idx)
        writer.add_scalar("loss_entropy", <a id="change">entropy_loss_v</a>.data.cpu().numpy()[0], step_idx)
        writer.add_scalar("loss_policy", loss_policy_v.data.cpu().numpy()[0], step_idx)
        writer.add_scalar("loss_total", loss_v.data.cpu().numpy()[0], step_idx)
        writer.add_scalar("grad_l2", grad_means / grad_count, step_idx)</code></pre><h3>After Change</h3><pre><code class='java'>
        if len(batch_states) &lt; BATCH_SIZE:
            continue

        <a id="change">states_v</a> = <a id="change">torch.FloatTensor(batch_states)</a>
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_scale_v = torch.FloatTensor(batch_scales)

        optimizer.zero_grad()
        logits_v = net(states_v)
        <a id="change">log_prob_v</a> = F.log_softmax(logits_v, dim=1)
        <a id="change">log_prob_actions_v</a> = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]
        <a id="change">loss_policy_v</a> = -log_prob_actions_v.mean()

        prob_v = F.softmax(logits_v, dim=1)
        <a id="change">entropy_v</a> = -(prob_v * log_prob_v).sum(dim=1).mean()
        <a id="change">entropy_loss_v</a> = -ENTROPY_BETA * entropy_v
        loss_v = loss_policy_v + entropy_loss_v

        loss_v.backward()
        optimizer.step()

        &#47&#47 calc KL-div
        new_logits_v = net(states_v)
        new_prob_v = F.softmax(new_logits_v, dim=1)
        <a id="change">kl_div_v</a> = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()
        writer.add_scalar("kl", kl_div_v.item(), step_idx)

        grad_max = 0.0
        grad_means = 0.0
        grad_count = 0
        for p in net.parameters():
            grad_max = max(grad_max, p.grad.abs().max().item())
            grad_means += (p.grad ** 2).mean().sqrt().item()
            grad_count += 1

        writer.add_scalar("baseline", baseline, step_idx)
        writer.add_scalar("entropy", entropy_v.item(), step_idx)
        writer.add_scalar("batch_scales", np.mean(batch_scales), step_idx)
        writer.add_scalar("loss_entropy", <a id="change">entropy_loss_v.item()</a>, step_idx)
        writer.add_scalar("loss_policy", loss_policy_v.item(), step_idx)
        writer.add_scalar("loss_total", loss_v.item(), step_idx)
        writer.add_scalar("grad_l2", grad_means / grad_count, step_idx)</code></pre><img src="197788666.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 29</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/e4f051b6cce414997a97b896276563c4e361d0b8#diff-5c9837e94ac1efc536eca2ec789d3a95f62b777cbcd0c76c44108af4f7b5e715L82' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: e4f051b6cce414997a97b896276563c4e361d0b8</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch09/04_cartpole_pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/171e9e18a10f2daea090bc6f4815db41072d66b6#diff-3fe5fdc433e1e3ee997d1ada0decb190930e5a2a2b82bf7896d0afd29b7a8764L90' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 171e9e18a10f2daea090bc6f4815db41072d66b6</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch10/01_cartpole_pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/e4f051b6cce414997a97b896276563c4e361d0b8#diff-5c9837e94ac1efc536eca2ec789d3a95f62b777cbcd0c76c44108af4f7b5e715L82' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: e4f051b6cce414997a97b896276563c4e361d0b8</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch09/04_cartpole_pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/d5b0cd8e7960c247bb7c5b7c832358f8831780fb#diff-76ef45280999506380a1cd22adfc9b9fd61476500ae25b7339a77ddf0a7423ffL91' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: d5b0cd8e7960c247bb7c5b7c832358f8831780fb</div><div id='time'> Time: 2018-04-29</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch15/03_train_trpo.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>