<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    @lab_api
    def init_algorithm_params(self):
        &quot&quot&quotInitialize other algorithm parameters&quot&quot&quot
        if <a id="change">self.algorithm_spec[&quotaction_policy&quot]</a> == &quotdefault&quot:
            if self.body.is_discrete:
                self.algorithm_spec[&quotaction_policy&quot] = &quotsoftmax&quot
            else:
                self.algorithm_spec[&quotaction_policy&quot] = &quotgaussian&quot
        util.set_attr(self, self.algorithm_spec, [
            &quotaction_policy&quot,
            &quotgamma&quot,  &#47&#47 the discount factor
            &quotadd_entropy&quot,
            &quotentropy_weight&quot,
            &quotcontinuous_action_clip&quot,
            &quottraining_frequency&quot,
            &quottraining_iters_per_batch&quot,
            &quotuse_GAE&quot,
            &quotlam&quot,
            &quotnum_step_returns&quot,
            &quotpolicy_loss_weight&quot,
            &quotval_loss_weight&quot,
        ])
        self.action_policy = act_fns[self.action_policy]
        self.to_train = 0
        &#47&#47 To save on a forward pass keep the log probs from each action
        <a id="change">self.saved_log_probs = []</a>
        <a id="change">self.entropy = []</a>
        &#47&#47 Select appropriate function for calculating state-action-value estimate (target)
        if self.use_GAE:
            self.get_target = self.get_gae_target
        else:</code></pre><h3>After Change</h3><pre><code class='java'>
        logger.info(util.self_desc(self))

    @lab_api
    def init_algorithm_params(<a id="change">self</a>):
        &quot&quot&quotInitialize other algorithm parameters&quot&quot&quot
        &#47&#47 set default
        <a id="change">util.set_attr(self, dict(
            action_pdtype=&quotdefault&quot,
            action_policy=&quotdefault&quot,
            action_policy_update=&quotno_update&quot,
            explore_var_start=np.nan,
            explore_var_end=np.nan,
            explore_anneal_epi=np.nan,
        ))</a>
        util.set_attr(self, self.algorithm_spec, [
            &quotaction_policy&quot,
            &#47&#47 theoretically, AC does not have policy update; but in this implementation we have such option
            &quotaction_policy_update&quot,
            &quotexplore_var_start&quot, &quotexplore_var_end&quot, &quotexplore_anneal_epi&quot,
            &quotgamma&quot,  &#47&#47 the discount factor
            &quotadd_entropy&quot,
            &quotentropy_weight&quot,
            &quotcontinuous_action_clip&quot,
            &quottraining_frequency&quot,
            &quottraining_iters_per_batch&quot,
            &quotuse_GAE&quot,
            &quotlam&quot,
            &quotnum_step_returns&quot,
            &quotpolicy_loss_weight&quot,
            &quotval_loss_weight&quot,
        ])
        self.to_train = 0
        self.action_policy = getattr(policy_util, self.action_policy)
        <a id="change">self.action_policy_update = getattr(policy_util, self.action_policy_update)</a>
        for body in self.agent.nanflat_body_a:
            body.explore_var = self.explore_var_start
        &#47&#47 Select appropriate function for calculating state-action-value estimate (target)
        if self.use_GAE:</code></pre>