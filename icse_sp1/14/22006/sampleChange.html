<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            nn.Conv2d(1, 1, 1),
        ).eval()
        mp = prepare_fx(m, {&quot&quot: torch.quantization.default_qconfig})
        <a id="change">mp(torch.randn(1, 1, 4, 4))</a>
        &#47&#47 TODO(future PR): prevent the need for copying here, we can copy the
        &#47&#47 modules but should reuse the underlying tensors
        <a id="change">mp_copy = copy.deepcopy(mp)</a>
        mq = convert_fx(mp_copy)

        mp_shadows_mq = add_shadow_loggers(&quotfp32_prepared&quot, mp, &quotint8&quot, mq, OutputLogger)

        &#47&#47 TODO(before land): test both scripted and non-scripted
        <a id="change">mp_shadows_mq = torch.jit.script(mp_shadows_mq)</a>

        &#47&#47 calibrate
        <a id="change">input_fp32 = torch.randn(1, 1, 4, 4)</a>
        mp_shadows_mq(input_fp32)

        &#47&#47 check activation result correctness
        <a id="change">act_compare_dict = extract_shadow_logger_info(
            mp_shadows_mq, OutputLogger)</a>
        self.assertTrue(len(act_compare_dict) == 2)
        self.assert_ns_compare_dict_valid(act_compare_dict)

    @override_qengines</code></pre><h3>After Change</h3><pre><code class='java'>
            nn.Conv2d(1, 1, 1),
        ).eval()
        self._test_match_shadow_activations(
            m, <a id="change">(torch.randn(1, 1, 4, 4),)</a>, results_len=2)

    @skipIfNoFBGEMM
    def test_add_shadow_loggers_fun(self):</code></pre>