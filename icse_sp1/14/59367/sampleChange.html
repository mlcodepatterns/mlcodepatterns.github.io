<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                &#47&#47 If gradient clipping is used, calculate the huber loss
                if config.clip_gradients &gt; 0.0:
                    huber_loss = tf.where(tf.abs(delta) &lt; config.clip_gradients, 0.5 * <a id="change">tf.square(delta)</a>, tf.abs(delta) - 0.5)
                    double_q_loss = tf.reduce_mean(huber_loss)
                else:
                    <a id="change">double_q_loss = tf.reduce_mean(tf.square(delta))</a>

                &#47&#47 Use the existing loss structure from the model here, then compute dqfd loss separately
                tf.losses.add_loss(double_q_loss)
</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 Surrogate loss as the mean squared error between actual observed rewards and expected rewards
                q_target = self.reward[:-1] + (1.0 - tf.cast(self.terminal[:-1], tf.float32)) * self.discount * target_value[action][1:]
                delta = q_target - q_value
                <a id="change">self.loss_per_instance = tf.square(delta)</a>

                &#47&#47 If gradient clipping is used, calculate the huber loss
                if config.clip_gradients &gt; 0.0:
                    huber_loss = tf.where(tf.abs(delta) &lt; config.clip_gradients, 0.5 * <a id="change">self.loss_per_instance</a>, tf.abs(delta) - 0.5)
                    double_q_loss = tf.reduce_mean(huber_loss)
                else:
                    <a id="change">double_q_loss = tf.reduce_mean(self.loss_per_instance)</a>

                &#47&#47 Use the existing loss structure from the model here, then compute dqfd loss separately
                tf.losses.add_loss(double_q_loss)
</code></pre>