<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        )
        self.encoder.hybridize()
        &#47&#47 Construct word embedding
        <a id="change">self.word_embed</a> = nn.Embedding(input_dim=vocab_size,
                                       output_dim=embed_size,
                                       weight_initializer=embed_initializer,
                                       dtype=dtype)
        if embed_size != units:
            <a id="change">self.embed_factorized_proj = nn.Dense(units=units,
                                                  flatten=False,
                                                  weight_initializer=weight_initializer,
                                                  bias_initializer=bias_initializer)</a>
        self.embed_layer_norm = nn.LayerNorm(epsilon=self.layer_norm_eps)
        self.embed_dropout = nn.Dropout(hidden_dropout_prob)
        &#47&#47 Construct token type embedding
        <a id="change">self.token_type_embed</a> = nn.Embedding(input_dim=num_token_types,
                                             output_dim=embed_size,
                                             weight_initializer=weight_initializer)
        <a id="change">self.token_pos_embed</a> = PositionalEmbedding(units=embed_size,
                                                   max_length=max_length,
                                                   dtype=self._dtype,
                                                   method=pos_embed_type)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.activation = activation
        self.embed_initializer = embed_initializer
        self.weight_initializer = weight_initializer
        self.bias_initializer = bias_initiali<a id="change">zer
</a>        self.layer_norm_eps = layer_norm_eps
        self._layout = layout
        if compute_layout is None or compute_layout == &quotauto&quot:
            self._compute_layout = layout
        else:
            self._compute_layout = compute_layout
        &#47&#47 Construct AlbertEncoder
        self.encoder = AlbertEncoder(
            units=units,
            hidden_size=hidden_size,
            num_layers=num_layers,
            num_heads=num_heads,
            num_groups=num_groups,
            attention_dropout_prob=attention_dropout_prob,
            hidden_dropout_prob=hidden_dropout_prob,
            output_attention=False,
            output_all_encodings=False,
            activation=activation,
            layer_norm_eps=layer_norm_eps,
            weight_initializer=weight_initializer,
            bias_initializer=bias_initializer,
            dtype=dtype,
            layout=self._compute_layout
        )
        self.encoder.hybridize()
        &#47&#47 Construct word embedding
        self.word_embed = nn.Embedding(input_dim=vocab_size,
                                       output_dim=embed_size,
                                       weight_initializer=embed_initializer,
                                       dtype=dtype)
        if embed_size != units:
            self.emb<a id="change">ed_factorized_proj = nn.Dense(units=units,
                                                  flatten=False,
                                                  weight_initializer=weight_initializer,
       </a>                                           bias_initializer=bias_initializer)
        self.embed_layer_norm = nn.LayerNorm(epsilon=self.layer_norm_eps)
        self.embed_dropout = nn.Dropout(hidden_dropout_prob)
        &#47&#47 Construct token type embedding</code></pre>