<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 embed fc and att feats
        fc_feats = self.fc_embed(fc_feats)
        <a id="change">att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)</a>

        &#47&#47 Project the attention feats first to reduce memory and computation comsumptions.
        <a id="change">p_att_feats = self.ctx2att(att_feats)</a>

        for i in range(seq.size(1) - 1):
            if self.training and i &gt;= 1 and self.ss_prob &gt; 0.0: &#47&#47 otherwiste no need to sample
                sample_prob = fc_feats.data.new(batch_size).uniform_(0, 1)
                sample_mask = sample_prob &lt; self.ss_prob
                if sample_mask.sum() == 0:
                    it = seq[:, i].clone()
                else:
                    sample_ind = sample_mask.nonzero().view(-1)
                    it = seq[:, i].data.clone()
                    &#47&#47prob_prev = torch.exp(outputs[-1].data.index_select(0, sample_ind)) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    &#47&#47it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1))
                    &#47&#47 prob_prev = torch.exp(outputs[-1].data) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    prob_prev = torch.exp(outputs[:, i-1].data) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1).index_select(0, sample_ind))
                    it = Variable(it, requires_grad=False)
            else:
                it = seq[:, i].clone()          
            &#47&#47 break if all the sequences end
            if i &gt;= 1 and seq[:, i].data.sum() == 0:
                break

            <a id="change">xt = self.embed(it)</a>

            output, state = self.core(xt, fc_feats, att_feats, p_att_feats, state, att_masks)
            <a id="change">output = F.log_softmax(self.logit(output))</a>
            outputs[:, i] = output
            &#47&#47 outputs.append(output)

        return outputs</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 outputs = []
        outputs = Variable(fc_feats.data.new(batch_size, seq.size(1) - 1, self.vocab_size+1).zero_())

        <a id="change">fc_feats</a>, att_feats, p_att_feats = self._prepare_feature(fc_feats, att_feats, att_masks)

        for i in range(seq.size(1) - 1):
            if self.training and i &gt;= 1 and self.ss_prob &gt; 0.0: &#47&#47 otherwiste no need to sample</code></pre>