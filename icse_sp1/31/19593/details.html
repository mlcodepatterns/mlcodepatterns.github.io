<html><h3>7239447415c21586291d5c993e30fba5df54b9db,foolbox/models/pytorch.py,PyTorchModel,batch_backward,#PyTorchModel#Any#Any#,215
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        assert gradients.ndim == 2

        gradients = torch.from_numpy(gradients).to(self.device)
        <a id="change">if self._old_pytorch():  &#47&#47 pragma: no cover
            gradients = Variable(gradients)

       </a> input_shape = images.shape
        images, dpdx = self._process_input(images)
        images = torch.from_numpy(images).to(self.device)
        <a id="change">if self._old_pytorch():  &#47&#47 pragma: no cover
            images = Variable(images, requires_grad=True)
        else:
            images.requires_grad_()
       </a> predictions = self._model(images)

        assert gradients.dim() == 2
        assert predictions.dim() == 2
        assert gradients.size() == predictions.size()

        &#47&#47 loss = torch.dot(predictions, gradients)
        &#47&#47 loss.backward()
        &#47&#47 should be the same as predictions.backward(gradient=gradients)
        predictions.backward(gradient=gradients)

        <a id="change">grad</a> = images.grad

        <a id="change">if self._old_pytorch():  &#47&#47 pragma: no cover
            grad = grad.data
       </a> <a id="change">grad = grad.to("cpu")</a>
        <a id="change">if not self._old_pytorch():
            grad = grad.detach()
       </a> <a id="change">grad = grad.numpy()</a>
        grad = self._process_gradient(dpdx, grad)
        assert grad.shape == input_shape

        return grad</code></pre><h3>After Change</h3><pre><code class='java'>
        predictions.backward(gradient=gradients)

        grad = images.grad
        <a id="change">grad = grad.detach().cpu().numpy()</a>
        grad = self._process_gradient(dpdx, grad)
        assert grad.shape == input_shape
        return grad
</code></pre><img src="108235763.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 23</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/7239447415c21586291d5c993e30fba5df54b9db#diff-607d4506afad4e24aefcce271c9b0e4ee0a55ee96eaaea094eb5f40404eb03a5L168' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: 7239447415c21586291d5c993e30fba5df54b9db</div><div id='time'> Time: 2019-05-17</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/models/pytorch.py</div><div id='class'> Class Name: PyTorchModel</div><div id='method'> Method Name: batch_backward</div><BR><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/7239447415c21586291d5c993e30fba5df54b9db#diff-607d4506afad4e24aefcce271c9b0e4ee0a55ee96eaaea094eb5f40404eb03a5L105' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: 7239447415c21586291d5c993e30fba5df54b9db</div><div id='time'> Time: 2019-05-17</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/models/pytorch.py</div><div id='class'> Class Name: PyTorchModel</div><div id='method'> Method Name: predictions_and_gradient</div><BR><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/7239447415c21586291d5c993e30fba5df54b9db#diff-607d4506afad4e24aefcce271c9b0e4ee0a55ee96eaaea094eb5f40404eb03a5L155' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: 7239447415c21586291d5c993e30fba5df54b9db</div><div id='time'> Time: 2019-05-17</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/models/pytorch.py</div><div id='class'> Class Name: PyTorchModel</div><div id='method'> Method Name: batch_gradients</div><BR><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/7239447415c21586291d5c993e30fba5df54b9db#diff-607d4506afad4e24aefcce271c9b0e4ee0a55ee96eaaea094eb5f40404eb03a5L215' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: 7239447415c21586291d5c993e30fba5df54b9db</div><div id='time'> Time: 2019-05-17</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/models/pytorch.py</div><div id='class'> Class Name: PyTorchModel</div><div id='method'> Method Name: batch_backward</div><BR>