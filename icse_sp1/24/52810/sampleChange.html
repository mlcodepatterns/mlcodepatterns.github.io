<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    @dtypes(*torch.testing.get_all_dtypes())
    def test_bool_scalarlist(self, device, dtype):
        for N in N_values:
            for foreach_bin_op, foreach_bin_op_, torch_bin_op in <a id="change">zip(self.foreach_bin_ops,
                                                                     self.foreach_bin_ops_,
                                                                     self.torch_bin_ops)</a>:
                tensors = self._get_test_data(device, dtype, N)
                scalars = [True for _ in range(N)]

                if dtype == torch.bool:
                    if self.device_type == &quotcuda&quot:
                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op(tensors, scalars)

                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op_(tensors, scalars)
                        return
                    else:
                        if foreach_bin_op == torch._foreach_sub:
                            with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with a bool tensor"):
                                foreach_bin_op_(tensors, scalars)

                            with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with a bool tensor"):
                                foreach_bin_op(tensors, scalars)
                        else:
                            <a id="change">with self.assertRaisesRegex(RuntimeError, "result type Float can&quott be cast to the desired"):
                                foreach_bin_op_(tensors, scalars)

                           </a> res = foreach_bin_op(tensors, scalars)
                            for r in res:
                                <a id="change">self.assertTrue(r.dtype == torch.float32)</a>
                else:
                    &#47&#47 we dont support bool and complex types on CUDA for now
                    if (dtype in torch.testing.get_all_complex_dtypes()) and self.device_type == &quotcuda&quot:
                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op_(tensors, scalars)

                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op(tensors, scalars)
                        return

                    if foreach_bin_op == torch._foreach_sub:
                        if self.device_type == "cpu":
                            &#47&#47 see TODO[Fix scalar list]
                            res = foreach_bin_op(tensors, scalars)
                            if dtype in torch.testing.integral_types():
                                self.assertEqual(res, [r.to(torch.float32) for r in [torch_bin_op(t, 1) for t in tensors]])

                                with self.assertRaisesRegex(RuntimeError, "result type Float can&quott be cast to the "):
                                    foreach_bin_op_(tensors, scalars)
                            else:
                                self.assertEqual(res, [torch_bin_op(t, 1) for t in tensors])
                                foreach_bin_op_(tensors, scalars)
                                self.assertEqual(res, tensors)
                        else:
                            &#47&#47 see TODO[Fix scalar list]
                            res = foreach_bin_op(tensors, scalars)
                            if dtype in torch.testing.integral_types():
                                <a id="change">self.assertEqual(res, [r.to(dtype) for r in [torch_bin_op(t, 1) for t in tensors]])</a>
                            else:
                                self.assertEqual(res, [torch_bin_op(t, 1) for t in tensors])

                            foreach_bin_op_(tensors, scalars)</code></pre><h3>After Change</h3><pre><code class='java'>
                        return
                    else:
                        if foreach_bin_op == torch._foreach_sub:
                            with <a id="change">self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with two bool tensors")</a>:
                                foreach_bin_op_(tensors, scalars)

                            <a id="change">with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with two bool tensors"):
                                foreach_bin_op(tensors, scalars)
                       </a> else:
                            <a id="change">expected = [torch_bin_op(t, s) for t, s in zip(tensors, scalars)]</a>
                            res = foreach_bin_op(tensors, scalars)
                            <a id="change">self.assertEqual(res, expected)</a>

                            if foreach_bin_op_ == torch._foreach_div_:
                                <a id="change">with self.assertRaisesRegex(RuntimeError, "result type Float can&quott be cast to the desired"):
                                    foreach_bin_op_(tensors, scalars)
                           </a> else:
                                foreach_bin_op_(tensors, scalars)
                                self.assertEqual(res, tensors)
                else:
                    &#47&#47 we dont support complex types on CUDA for now
                    if (dtype in torch.testing.get_all_complex_dtypes()) and self.device_type == &quotcuda&quot:
                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op_(tensors, scalars)

                        with self.assertRaisesRegex(RuntimeError, "not implemented for"):
                            foreach_bin_op(tensors, scalars)
                        return

                    if foreach_bin_op == torch._foreach_sub and self.device_type == "cpu":
                        with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with a bool tensor"):
                            foreach_bin_op_(tensors, scalars)

                        with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with a bool tensor"):
                            foreach_bin_op(tensors, scalars)
                    else:
                        if self.device_type == "cpu":
                            expected = [torch_bin_op(t, s) for t, s in zip(tensors, scalars)]
                            res = foreach_bin_op(tensors, scalars)

                            self.assertEqual(res, expected)

                            if dtype in torch.testing.integral_types() and foreach_bin_op_ == torch._foreach_div_:
                                with self.assertRaisesRegex(RuntimeError, "result type Float can&quott be cast to the desired "):
                                    foreach_bin_op_(tensors, scalars)
                            else:
                                foreach_bin_op_(tensors, scalars)
                                self.assertEqual(tensors, expected)
                        else:
                            if foreach_bin_op == torch._foreach_sub:
                                <a id="change">with self.assertRaisesRegex(RuntimeError, "Subtraction, the `-` operator, with a bo"):
                                    expected = [torch_bin_op(t, s) for t, s in zip(tensors, scalars)]

                               </a> res = foreach_bin_op(tensors, scalars)
                                foreach_bin_op_(tensors, scalars)
                                self.assertEqual(res, tensors)
                            else:</code></pre>