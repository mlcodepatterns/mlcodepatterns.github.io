<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.body.add(nn.Activation(&quotrelu&quot))
        self.body.add(nn.Conv2D(channels, kernel_size=1, strides=1))

        <a id="change">if use_se:
            self.se = nn.HybridSequential(prefix=&quot&quot)
            self.se.add(nn.Dense(channels // 4, use_bias=False))
            self.se.add(nn.Activation(&quotrelu&quot))
            self.se.add(nn.Dense(channels * 4, use_bias=False))
            self.se.add(nn.Activation(&quotsigmoid&quot))
        else:
            self.se = None

       </a> if not last_gamma:
            self.body.add(nn.BatchNorm())
        else:
            self.body.add(nn.BatchNorm(gamma_initializer=&quotzeros&quot))</code></pre><h3>After Change</h3><pre><code class='java'>
    downsample : bool, default False
        Whether to downsample the input.
    in_channels : int, default 0
        Number of input channel<a id="change">s. Default is 0, to infer from the graph.
    last_gamma :</a> bool, default False
        Whether to initialize the gamma of the last BatchNorm layer in each bottleneck to zero.
    use_se : <a id="change">bool, default False
        Whether to use Squeeze-and-Exc</a>itation module
    norm_layer : object
        Normalization layer used (default: :class:`mxnet.gluon.nn.BatchNorm`)
        Can be :class:`mxnet.gluon.nn.BatchNorm` or :class:`mxnet.gluon.contrib.nn.SyncBatchNorm`.
    norm_kwargs : dict
        Additional `norm_layer` arguments, for example `num_devices=4`
        for :class:`mxnet.gluon.contrib.nn.SyncBatchNorm`.
    
    def __init__(self, channels, stride, downsample=False, in_channels=0,
                 last_gamma=False, use_se=False, norm_layer=BatchNorm, norm_kwargs=None, **kwargs):
        super(BottleneckV1, self).__init__(**kwargs)
        self.body = nn.HybridSequential(prefix=&quot&quot)
        self.body.add(nn.Conv2D(channels//4, kernel_size=1, strides=stride))
     <a id="change">   self.body.add(norm_layer(**({} if norm_kwargs is None e</a>lse norm_kwargs)))
        self.body.add(nn.Activation(&quotrelu&quot))
        self.body.add(_conv3x3(channels//4, 1, channels//4))
        self.body.add(norm_layer(**({} if norm_kwargs is None else norm_kwargs)))</code></pre>