<html><h3>7fd53c15c6273327ef10c2458848fcaf2a85e406,finetune/base_models/gpt/featurizer.py,,gpt_featurizer,#Any#Any#Any#Any#Any#,133
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        features: The output of the featurizer_final state.
        sequence_features: The output of the featurizer at each timestep.
    
    <a id="change">initial_shape = [a or -1 for a in X.get_shape().as_list()]</a>
    X = tf.reshape(X, shape=[-1] + initial_shape[-2:])

    with tf.variable_scope(&quotmodel/featurizer&quot, reuse=reuse):
        embed_weights = tf.get_variable(
            name="we",
            shape=[encoder.vocab_size + config.max_length, config.n_embed],
            initializer=tf.random_normal_initializer(stddev=config.weight_stddev)
        )
        if config.train_embeddings:
            embed_weights = dropout(embed_weights, config.embed_p_drop, train)
        else:
            embed_weights = tf.stop_gradient(embed_weights)

        X = tf.reshape(X, [-1, config.max_length, 2])

        h = embed(X, embed_weights)
        for layer in range(config.n_layer):
            if (config.n_layer - layer) == config.num_layers_trained and config.num_layers_trained != config.n_layer:
                h = tf.stop_gradient(h)
                train_layer = False
            else:
                train_layer = train

            with tf.variable_scope(&quoth%d_&quot % layer):
                block_fn = functools.partial(block, n_head=config.n_heads, act_fn=config.act_fn,
                                             resid_pdrop=config.resid_p_drop, attn_pdrop=config.attn_p_drop,
                                             scope=&quoth%d&quot % layer, train=train_layer, scale=True)
                if config.low_memory_mode and train_layer:
                    block_fn = recompute_grad(block_fn, use_entire_scope=True)
                h = block_fn(h)

        &#47&#47 Use hidden state at classifier token as input to final proj. + softmax
        clf_h = tf.reshape(h, [-1, config.n_embed])  &#47&#47 [batch * seq_len, embed]
        clf_token = encoder[&quot_classify_&quot]
        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), tf.float32), 1), tf.int32)
        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32) * config.max_length + pool_idx)
        clf_h = tf.reshape(clf_h, shape=<a id="change">initial_shape[: -2] + [config.n_embed]</a>)
        seq_feats = tf.reshape(h, shape=<a id="change">initial_shape[:-1] + [config.n_embed]</a>)

        return {
            &quotembed_weights&quot: embed_weights,</code></pre><h3>After Change</h3><pre><code class='java'>
        features: The output of the featurizer_final state.
        sequence_features: The output of the featurizer at each timestep.
    
    <a id="change">initial_shape = tf.shape(X)</a>
    X = tf.reshape(X, shape=[-1] + initial_shape[-2:])
    X = tf.reshape(X, shape=tf.concat(([-1], initial_shape[-2:]), 0))

    with tf.variable_scope(&quotmodel/featurizer&quot, reuse=reuse):
        embed_weights = tf.get_variable(
            name="we",
            shape=[encoder.vocab_size + config.max_length, config.n_embed],
            initializer=tf.random_normal_initializer(stddev=config.weight_stddev)
        )
        if config.train_embeddings:
            embed_weights = dropout(embed_weights, config.embed_p_drop, train)
        else:
            embed_weights = tf.stop_gradient(embed_weights)

        X = tf.reshape(X, [-1, config.max_length, 2])

        h = embed(X, embed_weights)
        for layer in range(config.n_layer):
            if (config.n_layer - layer) == config.num_layers_trained and config.num_layers_trained != config.n_layer:
                h = tf.stop_gradient(h)
                train_layer = False
            else:
                train_layer = train

            with tf.variable_scope(&quoth%d_&quot % layer):
                block_fn = functools.partial(block, n_head=config.n_heads, act_fn=config.act_fn,
                                             resid_pdrop=config.resid_p_drop, attn_pdrop=config.attn_p_drop,
                                             scope=&quoth%d&quot % layer, train=train_layer, scale=True)
                if config.low_memory_mode and train_layer:
                    block_fn = recompute_grad(block_fn, use_entire_scope=True)
                h = block_fn(h)

        &#47&#47 Use hidden state at classifier token as input to final proj. + softmax
        clf_h = tf.reshape(h, [-1, config.n_embed])  &#47&#47 [batch * seq_len, embed]
        clf_token = encoder[&quot_classify_&quot]
        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), tf.float32), 1), tf.int32)
        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32) * config.max_length + pool_idx)
        clf_h = tf.reshape(clf_h, shape=<a id="change">tf.concat((initial_shape[: -2], [config.n_embed]), 0)</a>)
        seq_feats = tf.reshape(h, shape=<a id="change">tf.concat((initial_shape[:-1], [config.n_embed]), 0)</a>)

        return {
            &quotembed_weights&quot: embed_weights,</code></pre><img src="7347953.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 20</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/IndicoDataSolutions/finetune/commit/7fd53c15c6273327ef10c2458848fcaf2a85e406#diff-42a1d62f8fe428eae0d41f1825b03e3badbf8ffcc955675723c7ecffa3f638f3L147' target='_blank'>Link</a></div><div id='project'> Project Name: IndicoDataSolutions/finetune</div><div id='commit'> Commit Name: 7fd53c15c6273327ef10c2458848fcaf2a85e406</div><div id='time'> Time: 2019-03-21</div><div id='author'> Author: benlt@hotmail.co.uk</div><div id='file'> File Name: finetune/base_models/gpt/featurizer.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: gpt_featurizer</div><BR><BR><div id='link'><a href='https://github.com/IndicoDataSolutions/finetune/commit/7fd53c15c6273327ef10c2458848fcaf2a85e406#diff-42a1d62f8fe428eae0d41f1825b03e3badbf8ffcc955675723c7ecffa3f638f3L147' target='_blank'>Link</a></div><div id='project'> Project Name: IndicoDataSolutions/finetune</div><div id='commit'> Commit Name: 7fd53c15c6273327ef10c2458848fcaf2a85e406</div><div id='time'> Time: 2019-03-21</div><div id='author'> Author: benlt@hotmail.co.uk</div><div id='file'> File Name: finetune/base_models/gpt/featurizer.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: gpt_featurizer</div><BR><BR><div id='link'><a href='https://github.com/IndicoDataSolutions/finetune/commit/08e59a637de862e7c816d0e62f0f0d14a094c221#diff-8f0ad4944edbbd76c8dfbb7b91f26d59416f5b425ef9889b6ea0a0a6736aa177L19' target='_blank'>Link</a></div><div id='project'> Project Name: IndicoDataSolutions/finetune</div><div id='commit'> Commit Name: 08e59a637de862e7c816d0e62f0f0d14a094c221</div><div id='time'> Time: 2019-03-21</div><div id='author'> Author: benlt@hotmail.co.uk</div><div id='file'> File Name: finetune/base_models/textcnn/featurizer.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: textcnn_featurizer</div><BR><BR><div id='link'><a href='https://github.com/IndicoDataSolutions/finetune/commit/7fd53c15c6273327ef10c2458848fcaf2a85e406#diff-ee70097769b6c27eb61e5309639977406294c0fd8e13efc569db55ad31affbbaL139' target='_blank'>Link</a></div><div id='project'> Project Name: IndicoDataSolutions/finetune</div><div id='commit'> Commit Name: 7fd53c15c6273327ef10c2458848fcaf2a85e406</div><div id='time'> Time: 2019-03-21</div><div id='author'> Author: benlt@hotmail.co.uk</div><div id='file'> File Name: finetune/base_models/gpt2/featurizer.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: gpt2_featurizer</div><BR>