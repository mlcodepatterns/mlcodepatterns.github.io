<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()

    <a id="change">envs = [make_env() for _ in range(5)]</a>
    writer = SummaryWriter(comment="-pong-pg")

    <a id="change">net = common.AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n)</a>
    if args.cuda:
        net.cuda()
    print(net)

    <a id="change">agent</a> = ptan.agent.PolicyAgent(net, apply_softmax=True, cuda=args.cuda)
    <a id="change">exp_source</a> = ptan.experience.ExperienceSourceFirstLast(<a id="change">envs</a>, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)

    total_rewards = []
    step_rewards = MeanRingBuf(capacity=BASELINE_STEPS)
    step_idx = 0
    done_episodes = 0
    train_step_idx = 0

    batch_states, batch_actions, batch_scales = [], [], []
    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []
    m_grad_max, m_grad_mean = [], []

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        for step_idx, exp in enumerate(exp_source):
&#47&#47            step_rewards.add(exp.reward)

&#47&#47            baseline = step_rewards.mean()
            batch_states.append(np.array(exp.state, copy=False))
            batch_actions.append(int(exp.action))
            batch_scales.append(exp.reward)
            &#47&#47 handle new rewards
            <a id="change">new_rewards</a> = exp_source.pop_total_rewards()
            if new_rewards and tracker.reward(new_rewards[0], step_idx):
                break
</code></pre><h3>After Change</h3><pre><code class='java'>
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()

    <a id="change">env = make_env()</a>
    writer = SummaryWriter(comment="-pong-pg")

    <a id="change">net = common.AtariPGN(env.observation_space.shape, env.action_space.n)</a>
    if args.cuda:
        net.cuda()
    print(net)

    <a id="change">agent</a> = ptan.agent.PolicyAgent(net, apply_softmax=True, cuda=args.cuda)
    <a id="change">exp_source</a> = ptan.experience.ExperienceSourceFirstLast(<a id="change">env</a>, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)

    total_rewards = []
    step_rewards = MeanRingBuf(capacity=BASELINE_STEPS)
    step_idx = 0
    done_episodes = 0
    train_step_idx = 0

    batch_states, batch_actions, batch_scales = [], [], []
    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []
    m_grad_max, m_grad_mean = [], []

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        for step_idx, exp in enumerate(exp_source):
&#47&#47            step_rewards.add(exp.reward)

&#47&#47            baseline = step_rewards.mean()
            batch_states.append(np.array(exp.state, copy=False))
            batch_actions.append(int(exp.action))
            batch_scales.append(exp.reward)
            &#47&#47 handle new rewards
            <a id="change">new_rewards</a> = exp_source.pop_total_rewards()
            if new_rewards:
                if tracker.reward(new_rewards[0], step_idx):
                    break</code></pre>