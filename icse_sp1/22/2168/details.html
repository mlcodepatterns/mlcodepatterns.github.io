<html><h3>113c1d84a806358d5c9f1242a88edb3966a304ab,ch09/04_pong_pg.py,,,#,47
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()

    <a id="change">envs = [make_env() for _ in range(5)]</a>
    writer = SummaryWriter(comment="-pong-pg")

    <a id="change">net = common.AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n)</a>
    if args.cuda:
        net.cuda()
    print(net)

    <a id="change">agent</a> = ptan.agent.PolicyAgent(net, apply_softmax=True, cuda=args.cuda)
    <a id="change">exp_source</a> = ptan.experience.ExperienceSourceFirstLast(<a id="change">envs</a>, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)

    total_rewards = []
    step_rewards = MeanRingBuf(capacity=BASELINE_STEPS)
    step_idx = 0
    done_episodes = 0
    train_step_idx = 0

    batch_states, batch_actions, batch_scales = [], [], []
    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []
    m_grad_max, m_grad_mean = [], []

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        for step_idx, exp in enumerate(exp_source):
&#47&#47            step_rewards.add(exp.reward)

&#47&#47            baseline = step_rewards.mean()
            batch_states.append(np.array(exp.state, copy=False))
            batch_actions.append(int(exp.action))
            batch_scales.append(exp.reward)
            &#47&#47 handle new rewards
            <a id="change">new_rewards</a> = exp_source.pop_total_rewards()
            if new_rewards and tracker.reward(new_rewards[0], step_idx):
                break
</code></pre><h3>After Change</h3><pre><code class='java'>
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()

    <a id="change">env = make_env()</a>
    writer = SummaryWriter(comment="-pong-pg")

    <a id="change">net = common.AtariPGN(env.observation_space.shape, env.action_space.n)</a>
    if args.cuda:
        net.cuda()
    print(net)

    <a id="change">agent</a> = ptan.agent.PolicyAgent(net, apply_softmax=True, cuda=args.cuda)
    <a id="change">exp_source</a> = ptan.experience.ExperienceSourceFirstLast(<a id="change">env</a>, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)

    total_rewards = []
    step_rewards = MeanRingBuf(capacity=BASELINE_STEPS)
    step_idx = 0
    done_episodes = 0
    train_step_idx = 0

    batch_states, batch_actions, batch_scales = [], [], []
    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []
    m_grad_max, m_grad_mean = [], []

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        for step_idx, exp in enumerate(exp_source):
&#47&#47            step_rewards.add(exp.reward)

&#47&#47            baseline = step_rewards.mean()
            batch_states.append(np.array(exp.state, copy=False))
            batch_actions.append(int(exp.action))
            batch_scales.append(exp.reward)
            &#47&#47 handle new rewards
            <a id="change">new_rewards</a> = exp_source.pop_total_rewards()
            if new_rewards:
                if tracker.reward(new_rewards[0], step_idx):
                    break</code></pre><img src="17936656.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 23</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/113c1d84a806358d5c9f1242a88edb3966a304ab#diff-3a80d8d623f5d2851a396ab43e6f6421ba170ee8f5c8141a84b4bb4985bf70f6L52' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 113c1d84a806358d5c9f1242a88edb3966a304ab</div><div id='time'> Time: 2017-12-05</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch09/04_pong_pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/9205eee4cbfb730ea37c3e81a797698983fd6e87#diff-686a109e3df65b8f69b6fb44509f995775c189357dbbd1d091de0eb665501eedL70' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 9205eee4cbfb730ea37c3e81a797698983fd6e87</div><div id='time'> Time: 2017-12-06</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch09/05_pong_a2c.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/433fcf96eda8e924652c4a67878f6636e533b649#diff-3a80d8d623f5d2851a396ab43e6f6421ba170ee8f5c8141a84b4bb4985bf70f6L52' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 433fcf96eda8e924652c4a67878f6636e533b649</div><div id='time'> Time: 2017-12-07</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch09/04_pong_pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>