<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                    &#47&#47 Define regularization cost
                    self._log(&quotGraph: Calculating loss and gradients...&quot)
                    <a id="change">if self._reg_coeff is not None:
                        l2_cost = tf.squeeze(tf.reduce_sum(
                            [layer.regularization_coefficient * tf.nn.l2_loss(layer.weights) for layer in self._layers
                             if isinstance(layer, layers.fullyConnectedLayer)]))
                    else:
                        l2_cost = 0.0

                    &#47&#47 Define the cost function
                   </a> if self._loss_fn == &quotyolo&quot:
                        xx = tf.reshape(xx, [-1, self._grid_w * self._grid_h,
                                             self._NUM_BOXES * 5 + self._NUM_CLASSES])
                        yolo_loss = self._yolo_loss_function(y, xx)
                        num_image_loss = tf.cast(tf.shape(xx)[0], tf.float32)
                    gpu_cost = tf.squeeze(yolo_loss / num_image_loss + l2_cost)
                    device_costs.append(yolo_loss)

                    &#47&#47 Set the optimizer and get the gradients from it
                    gradients, variables, global_grad_norm = self._graph_get_gradients(gpu_cost, optimizer)
                    device_gradients.append(gradients)
                    device_variables.append(variables)

            &#47&#47 Average the gradients from each GPU and apply them
            average_gradients = self._graph_average_gradients(device_gradients)
            opt_variables = device_variables[0]
            self._graph_ops[&quotoptimizer&quot] = self._graph_apply_gradients(average_gradients, opt_variables, optimizer)

            &#47&#47 Average the costs and accuracies from each GPU
            self._yolo_loss = 0
            <a id="change">self._graph_ops[&quotcost&quot]</a> = tf.reduce_sum(device_costs) / self._batch_size + l2_cost

            &#47&#47 Calculate test and validation accuracy (on a single device at Tensorflow&quots discretion)
            if self._testing:</code></pre><h3>After Change</h3><pre><code class='java'>

                    &#47&#47 Define regularization cost
                    self._log(&quotGraph: Calculating loss and gradients...&quot)
                    <a id="change">l2_cost = self._graph_layer_loss()</a>

                    &#47&#47 Define the cost function
                    if self._loss_fn == &quotyolo&quot:
                        xx = tf.reshape(xx, [-1, self._grid_w * self._grid_h,
                                             self._NUM_BOXES * 5 + self._NUM_CLASSES])
                        yolo_loss = self._yolo_loss_function(y, xx)
                        num_image_loss = tf.cast(tf.shape(xx)[0], tf.float32)
                    gpu_cost = tf.squeeze(yolo_loss / num_image_loss + l2_cost)
                    device_costs.append(yolo_loss)

                    &#47&#47 Set the optimizer and get the gradients from it
                    gradients, variables, global_grad_norm = self._graph_get_gradients(gpu_cost, optimizer)
                    device_gradients.append(gradients)
                    device_variables.append(variables)

            &#47&#47 Average the gradients from each GPU and apply them
            average_gradients = self._graph_average_gradients(device_gradients)
            opt_variables = device_variables[0]
            self._graph_ops[&quotoptimizer&quot] = self._graph_apply_gradients(average_gradients, opt_variables, optimizer)

            &#47&#47 Average the costs and accuracies from each GPU
            self._yolo_loss = 0
            <a id="change">self._graph_ops[&quotcost&quot]</a> = tf.reduce_sum(device_costs) / self._batch_size + l2_cost

            &#47&#47 Calculate test and validation accuracy (on a single device at Tensorflow&quots discretion)
            if self._testing:</code></pre>