<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      images, labels = dataset.make_one_shot_iterator().get_next()
    else:
      images = tf.random_uniform(
          <a id="change">[batch_size, self.hparams.image_size,
           self.hparams.image_size, 3]</a>, minval=-1, maxval=1)
      <a id="change">labels = tf.random_uniform(
          [batch_size], minval=0, maxval=999, dtype=tf.int32)</a>
    if self.hparams.transpose_enabled:
      images = tensor_transform_fn(images, params[&quotoutput_perm&quot])
    one_hot_labels = tf.one_hot(labels, self.num_classes)
    <a id="change">return images, one_hot_labels</a>


def tensor_transform_fn(data, perm):
  Transpose function.</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 than a full batch size. As long as this validation is done with
    &#47&#47 consistent batch size, exactly the same images will be used.
    if not self.is_training:
      <a id="change">dataset = dataset.apply(batching.filter_irregular_batches(batch_size))</a>

    dataset = dataset.map(
        lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),
        num_parallel_calls=8)

    &#47&#47 For XLA, we must used fixed shapes. Because we repeat the source training
    &#47&#47 dataset indefinitely, this is not a dangerous operation.
    &#47&#47
    &#47&#47 When evaluating, prevent accidentally evaluating the same image twice by
    &#47&#47 dropping the final batch if it is less than a full batch size. As long as
    &#47&#47 this validation is done with consistent batch size, exactly the same
    &#47&#47 images will be used.
    def set_shapes(images, labels):
      images.set_shape(images.get_shape().merge_with(
          tf.TensorShape([None, None, None, batch_size])))
      labels.set_shape(labels.get_shape().merge_with(
          tf.TensorShape([batch_size])))
      return images, labels

    if self.is_training:
      <a id="change">dataset = dataset.map(set_shapes)</a>

    dataset = dataset.prefetch(32)  &#47&#47 Prefetch overlaps in-feed with training
    <a id="change">return dataset</a>  &#47&#47 Must return the dataset and not tensors for high perf!


class LoadEMAHook(tf.train.SessionRunHook):</code></pre>