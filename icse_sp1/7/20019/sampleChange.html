<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                if config.vocab_from_pretrained_embeddings:
                    &#47&#47 pretrained embeddings will get a freq count of 1
                    <a id="change">assert config.min_freq == 1, (
                        "If `vocab_from_pretrained_embeddings` is set, the vocab&quots "
                        "`min_freq` must be 1"
                    )</a>
                    if not config.vocab_from_train_data:  &#47&#47 Reset token counter.
                        tensorizer.vocab_builder._counter = collections.Counter()
                    pretrained_vocab = pretrained_embedding.embed_vocab
                    if config.vocab_size:</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 We don&quott need to load pretrained embeddings if we know the
            &#47&#47 embedding weights are going to be loaded from a snapshot.
            if config.pretrained_embeddings_path and not init_from_saved_state:
                <a id="change">if not any(
                    vocab_file.filepath == config.pretrained_embeddings_path
                    for vocab_file in tensorizer.vocab_config.vocab_files
                ):
                    raise ValueError(
                        f"Tensorizer&quots vocab files should include pretrained "
                        f"embeddings file {config.pretrained_embeddings_path}."
                    )
               </a> pretrained_embedding = PretrainedEmbedding(
                    config.pretrained_embeddings_path,  &#47&#47 doesn&quott support fbpkg
                    lowercase_tokens=config.lowercase_tokens,
                )</code></pre>