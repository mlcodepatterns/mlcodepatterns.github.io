<html><h3>1207689fa1e6ff2d321ccc182be13825b4e2575e,pytext/models/embeddings/word_embedding.py,WordEmbedding,from_config,#Any#Any#Any#Any#Any#,41
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                if config.vocab_from_pretrained_embeddings:
                    &#47&#47 pretrained embeddings will get a freq count of 1
                    <a id="change">assert config.min_freq == 1, (
                        "If `vocab_from_pretrained_embeddings` is set, the vocab&quots "
                        "`min_freq` must be 1"
                    )</a>
                    if not config.vocab_from_train_data:  &#47&#47 Reset token counter.
                        tensorizer.vocab_builder._counter = collections.Counter()
                    pretrained_vocab = pretrained_embedding.embed_vocab
                    if config.vocab_size:</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 We don&quott need to load pretrained embeddings if we know the
            &#47&#47 embedding weights are going to be loaded from a snapshot.
            if config.pretrained_embeddings_path and not init_from_saved_state:
                <a id="change">if not any(
                    vocab_file.filepath == config.pretrained_embeddings_path
                    for vocab_file in tensorizer.vocab_config.vocab_files
                ):
                    raise ValueError(
                        f"Tensorizer&quots vocab files should include pretrained "
                        f"embeddings file {config.pretrained_embeddings_path}."
                    )
               </a> pretrained_embedding = PretrainedEmbedding(
                    config.pretrained_embeddings_path,  &#47&#47 doesn&quott support fbpkg
                    lowercase_tokens=config.lowercase_tokens,
                )</code></pre><img src="110197889.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/pytext/commit/1207689fa1e6ff2d321ccc182be13825b4e2575e#diff-df022a950afa2aa2ac98913477123abede81c93f62568ab0ed290273354bacdcL61' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/pytext</div><div id='commit'> Commit Name: 1207689fa1e6ff2d321ccc182be13825b4e2575e</div><div id='time'> Time: 2019-07-12</div><div id='author'> Author: mikaell@fb.com</div><div id='file'> File Name: pytext/models/embeddings/word_embedding.py</div><div id='class'> Class Name: WordEmbedding</div><div id='method'> Method Name: from_config</div><BR><BR><div id='link'><a href='https://github.com/scikit-image/scikit-image/commit/6c47bd49ddb90e5196333224a4e6b211fb24d089#diff-d6dbe4bb727a50d11da326ef863213e450a2d1f3aa199321f3713d9faf96fb1bL104' target='_blank'>Link</a></div><div id='project'> Project Name: scikit-image/scikit-image</div><div id='commit'> Commit Name: 6c47bd49ddb90e5196333224a4e6b211fb24d089</div><div id='time'> Time: 2016-06-16</div><div id='author'> Author: thomas.walter@mines-paristech.fr</div><div id='file'> File Name: skimage/feature/texture.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: greycomatrix</div><BR><BR><div id='link'><a href='https://github.com/bethgelab/foolbox/commit/bf635f90dae66e4ddd3e1f342dca925b3c99faf7#diff-a7a04c81ff322c2d5ba15942841f0d9c820ffd9bf67d26c5c8c243d58b7e0600L35' target='_blank'>Link</a></div><div id='project'> Project Name: bethgelab/foolbox</div><div id='commit'> Commit Name: bf635f90dae66e4ddd3e1f342dca925b3c99faf7</div><div id='time'> Time: 2020-02-11</div><div id='author'> Author: git@jonasrauber.de</div><div id='file'> File Name: foolbox/attacks/binarization.py</div><div id='class'> Class Name: BinarizationRefinementAttack</div><div id='method'> Method Name: __call__</div><BR>