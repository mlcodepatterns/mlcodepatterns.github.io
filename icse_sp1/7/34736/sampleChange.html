<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 These are the metrics we&quotll pass up the way, and their new names
        train_metrics = {"train_loss", "ups", "wps", "gnorm", "clip"}
        <a id="change">valid_metrics = {"valid_loss"}</a>

        metrics = train_metrics if self.is_training else valid_metrics

        m = {k: self.trainer.meters[k].avg for k in metrics}

        &#47&#47 additionally output perplexity. note that fairseq models use base 2
        &#47&#47 in cross_entropy:
        &#47&#47 github.com/pytorch/fairseq/blob/master/fairseq/criterions/cross_entropy.py&#47&#47L55
        <a id="change">if "train_loss" in m:
            m["train_ppl"] = np.exp2(m["train_loss"])
       </a> if "valid_loss" in m:
            m["ppl"] = np.exp2(m["valid_loss"])

        for k, v in m.items():</code></pre><h3>After Change</h3><pre><code class='java'>
        if not hasattr(self, "trainer"):
            return {}

        output = {k: v.avg <a id="change">for</a> k, v in <a id="change">self.meters.items()</a>}

        if "nll_loss" in self.meters:
            &#47&#47 special case, we used sentence averaging so ppl comes from nll_loss
            output["ppl"] = np.exp2(self.meters["nll_loss"].avg)
        else:
            &#47&#47 normal case, just use loss
            output["ppl"] = np.exp2(self.meters["loss"].avg)

        &#47&#47 Fairseq trainer metrics we&quotll pass up the way
        trainer_metrics = {"ups", "wps", "gnorm", "clip"}
        if self.is_training:
            for k in trainer_metrics:
                output[k] = self.trainer.meters[k].avg

        &#47&#47 for display purposes
        <a id="change">output = {k: round_sigfigs(v, 4) for k, v in output.items()}</a>
        return output

    def reset_metrics(self):
        Reset metrics calculated by the model back to zero.</code></pre>