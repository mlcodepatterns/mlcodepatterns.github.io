<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    reward=reward,
                    discount=self.discount
                )
                <a id="change">reward -= state_value</a>

            else:
                next_state_value = tf.concat(values=(state_value[1:], (0.0,)), axis=0)
                zeros = tf.zeros_like(tensor=next_state_value)
                next_state_value = tf.where(condition=terminal, x=zeros, y=next_state_value)
                td_residual = reward + self.discount * next_state_value - state_value
                gae_discount = self.discount * self.gae_lambda
                reward = self.fn_discounted_cumulative_reward(
                    terminal=terminal,
                    reward=td_residual,
                    discount=gae_discount
                )

        <a id="change">return reward</a>

    def tf_pg_loss_per_instance(self, states, internals, actions, terminal, reward, next_states, next_internals, update):
        
        Creates the TensorFlow operations for calculating the (policy-gradient-specific) loss per batch</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 Normalise advantage
            mean, variance = tf.nn.moments(advantage, axes=[0], keep_dims=True)
            <a id="change">advantage = (advantage - mean) / (tf.sqrt(variance) + util.epsilon)</a>

            <a id="change">return advantage</a>

    def tf_pg_loss_per_instance(self, states, internals, actions, terminal, reward, next_states, next_internals, update):
        
        Creates the TensorFlow operations for calculating the (policy-gradient-specific) loss per batch</code></pre>