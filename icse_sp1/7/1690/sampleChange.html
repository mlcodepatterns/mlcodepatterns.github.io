<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 The iterative solution is necessitated by making
        &#47&#47 the implementation compatible with sequence-based models,
        &#47&#47 where the embedding indices are already two-dimensional.
        <a id="change">embedding = self.embeddings(indices * self._masks[0] % self.compressed_num_embeddings)</a>

        <a id="change">for mask in self._masks[1:]:
            embedding += self.embeddings(indices * mask % self.compressed_num_embeddings)

       </a> <a id="change">embedding /= self.num_hash_functions</a>

        return embedding
</code></pre><h3>After Change</h3><pre><code class='java'>
        See documentation on PyTorch ``nn.Embedding`` for details.
        

        <a id="change">(masks,
         masked_indices)</a> = self._initialize_caches(indices)

        torch.mul(
            indices.data.unsqueeze(indices.dim()).expand_as(masks),</code></pre>