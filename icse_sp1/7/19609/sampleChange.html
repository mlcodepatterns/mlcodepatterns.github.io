<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47     self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  &#47&#47 minimize(-exp_v) = maximize(exp_v)

    def learn(self, s, a, td):
        _, exp_v = self.sess.run(<a id="change">[self.train_op, self.exp_v]</a>, {self.s: [s], self.a: [a], self.td_error: td[0]})
        return exp_v

    def choose_action(self, s):</code></pre><h3>After Change</h3><pre><code class='java'>
    def learn(self, s, a, td):
            &#47&#47 _, exp_v = self.sess.run([self.train_op, self.exp_v], {self.s: [s], self.a: [a], self.td_error: td[0]})
        with tf.GradientTape() as tape:
            <a id="change">_logits = self.model([s]).outputs</a>
            &#47&#47 _probs = tf.nn.softmax(_logits)
            <a id="change">_exp_v = tl.rein.cross_entropy_reward_loss(logits=_logits, actions=[a], rewards=td[0])</a>
        grad = tape.gradient(_exp_v, self.model.weights)
        <a id="change">self.optimizer.apply_gradients(zip(grad, self.model.weights))</a>
        return _exp_v

    def choose_action(self, s):
            &#47&#47 probs = self.sess.run(self.acts_prob, {self.s: [s]})  &#47&#47 get probabilities of all actions</code></pre>