<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47 Loop over epochs. Save the model if the validation loss is the best
&#47&#47 we&quotve seen so far. Adjust the learning rate after each epoch.

<a id="change">best_val_loss = float("inf")</a>
epochs = 3 &#47&#47 The number of epochs
best_model = None

for epoch in range(1, epochs + 1):</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 equal to the length of the vocab object.
&#47&#47

ntokens = <a id="change">len(vocab.stoi)</a> &#47&#47 the size of vocabulary
emsize = 200 &#47&#47 embedding dimension
nhid = 200 &#47&#47 the dimension of the feedforward network model in nn.TransformerEncoder
<a id="change">nlayers = 2</a> &#47&#47 the number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead = 2 &#47&#47 the number of heads in the multiheadattention models
dropout = 0.2 &#47&#47 the dropout value
model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Run the model
&#47&#47 -------------
&#47&#47


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 `CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss&#47&#47torch.nn.CrossEntropyLoss&gt;`__
&#47&#47 is applied to track the loss and
&#47&#47 `SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd&#47&#47torch.optim.SGD&gt;`__
&#47&#47 implements stochastic gradient descent method as the optimizer. The initial
&#47&#47 learning rate is set to 5.0. `StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr&#47&#47torch.optim.lr_scheduler.StepLR&gt;`__ is
&#47&#47 applied to adjust the learn rate through epochs. During the
&#47&#47 training, we use
&#47&#47 `nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm&#47&#47torch.nn.utils.clip_grad_norm_&gt;`__
&#47&#47 function to scale all the gradient together to prevent exploding.
&#47&#47

criterion = nn.CrossEntropyLoss()
<a id="change">lr = 5.0</a> &#47&#47 learning rate
<a id="change">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</a>
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

import time
def train():</code></pre>