<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                tf.reduce_sum(powed_probs, 1, keepdims=True) + 1e-32)

        &#47&#47 set probs for no intents and action_listens to zero
        <a id="change">if ignore_mask is not None:
            probs = tf.concat([tf.where(ignore_mask,
                                        tf.zeros_like(probs[:, :-1]),
                                        probs[:, :-1]),
                               probs[:, -1:]], 1)
       </a> return probs, next_scores_state


def _compute_time_attention(attention_mechanism, attn_inputs, attention_state,</code></pre><h3>After Change</h3><pre><code class='java'>
                            scores[:, -1:]], 1)
        next_scores_state = scores

        <a id="change">if mask is not None:
            &#47&#47 apply mask to scores
            if self._shift_weight is not None:
                &#47&#47 rearrange scores to make them continuous for convolution
                scores = tf.map_fn(self._rearrange_fn,
                                   [scores, mask], dtype=scores.dtype)
            else:
                scores = tf.where(mask &gt; 0,
                                  scores, -self._inf * tf.ones_like(scores))

        &#47&#47 create probabilities for attention
       </a> if self._sparse_attention:
            probs = tf.contrib.sparsemax.sparsemax(scores)
        else:
            probs = tf.nn.softmax(scores)</code></pre>