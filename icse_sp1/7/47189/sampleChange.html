<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

def main_loop():
    generate multiple trajectories that reach the minimum batch_size
    for i_iter in <a id="change">count()</a>:
        memory = Memory()

        num_steps = 0
        reward_batch = 0
        num_episodes = 0

        while num_steps &lt; args.min_batch_size:
            state = env.reset()
            <a id="change">state = running_state(state)</a>
            reward_episode = 0

            for t in range(10000):
                state_var = Variable(Tensor(state).unsqueeze(0))
                action = policy_net.select_action(state_var)[0].cpu().numpy()
                action = int(action) if is_disc_action else action.astype(np.float64)
                next_state, reward, done, _ = env.step(action)
                reward_episode += reward
                <a id="change">next_state = running_state(next_state)</a>

                mask = 0 if done else 1

                memory.push(state, action, mask, next_state, reward)</code></pre><h3>After Change</h3><pre><code class='java'>

def main_loop():
    generate multiple trajectories that reach the minimum batch_size
    for i_iter in <a id="change">range(args.max_iter_num)</a>:
        memory = Memory()

        num_steps = 0
        reward_batch = 0
        num_episodes = 0

        while num_steps &lt; args.min_batch_size:
            state = env.reset()
            &#47&#47 state = running_state(state)
            reward_episode = 0

            for t in range(10000):
                state_var = Variable(Tensor(state).unsqueeze(0))
                action = policy_net.select_action(state_var)[0].cpu().numpy()
                action = int(action) if is_disc_action else action.astype(np.float64)
                next_state, reward, done, _ = env.step(action)
                reward_episode += reward
                &#47&#47 next_state = running_state(next_state)

                mask = 0 if done else 1

                memory.push(state, action, mask, next_state, reward)

                if args.render:
                    env.render()
                if done:
                    break

                state = next_state

            &#47&#47 log stats
            num_steps += (t+1)
            num_episodes += 1
            reward_batch += reward_episode

        reward_batch /= num_episodes
        batch = memory.sample()
        update_params(batch)

        if i_iter % args.log_interval == 0:
            print(&quotIter {}\tLast reward: {:.2f}\tAverage reward {:.2f}&quot.format(
                i_iter, reward_episode, reward_batch))

        if <a id="change">args.save_model_interval &gt; 0 and i</a>_iter % args.save_model_interval == 0:
            pickle.dump((policy_net, value_net), open(&quot../assets/learned_models/{}_a2c.p&quot.format(args.env_name), &quotwb&quot))

</code></pre>