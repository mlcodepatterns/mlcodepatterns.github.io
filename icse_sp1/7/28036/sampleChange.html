<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._labels_shape = (1,)                      &#47&#47 Shape of labels tensor excluding leading batch dimension
        self._labels_range = (0, self.num_classes-1)   &#47&#47 Possible labels&quot values inclusive
        if self.__dtype == &quotfloat16&quot and self.__have_float16_lrn:
            <a id="change">print("[WARNING] The data type is &quotfloat16&quot and I assume MXNET provides a float16 kernel for LRN layer. "
                  "If this model uses LRN and your MXNET version is outdated, you will get error. In this case, to "
                  "disable LRN layers in float16 regime, define the following variable &quotDLBS_MXNET_NO_FLOAT16_LRN&quot "
                  "(the value of this variable does not matter) i.e.: "
                  "-Pruntime.launcher=&quot\"DLBS_MXNET_NO_FLOAT16_LRN=1 \"&quot")</a>
        if self.__dtype == &quotfloat16&quot and not self.__have_float16_lrn:
            <a id="change">print("[WARNING] The data type is &quotfloat16&quot and you disable LRN layers. All calls to Model.maybe_lrn "
                  " will do nothing. If your MXNET version is up to date and provides LRN float16 kernel make sure "
                  "DLBS_MXNET_NO_FLOAT16_LRN environment variable is not defined. All this is relevant only if this "
                  "model uses LRN operators.")</a>

    @staticmethod
    def conv_shape(num_channels, spatial_dims, layout=&quotNCHW&quot):
         Return shape of a feature map tensor for convolutional models.</code></pre><h3>After Change</h3><pre><code class='java'>
                "in float16 regime, define the following variable &quotDLBS_MXNET_NO_FLOAT16_LRN&quot (the value of this "
                "variable does not matter) i.e.: -Pruntime.launcher=&quot\"DLBS_MXNET_NO_FLOAT16_LRN=1 \"&quot")
        if self.__dtype == &quotfloat16&quot and not self.__have_float16_lrn:
            <a id="change">logging.warning(
                "The data type is &quotfloat16&quot and you disable LRN layers. All calls to Model.maybe_lrn will do nothing. "
                "If your MXNET version is up to date and provides LRN float16 kernel make sure "
                "DLBS_MXNET_NO_FLOAT16_LRN environment variable is not defined. All this is relevant only if this "
                "model uses LRN operators.")</a>

    @staticmethod
    def conv_shape(num_channels, spatial_dims, layout=&quotNCHW&quot):
         Return shape of a feature map tensor for convolutional models.</code></pre>