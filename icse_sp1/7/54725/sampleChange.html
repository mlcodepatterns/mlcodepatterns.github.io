<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            assert_trained = net_util.gen_assert_trained(self.rnn_model)
        loss.backward(retain_graph=retain_graph)
        if self.clip_grad:
            <a id="change">logger.debug(f&quotClipping gradient: {self.clip_grad_val}&quot)</a>
            torch.nn.utils.clip_grad_norm_(self.parameters(), self.clip_grad_val)
        if global_net is None:
            self.optim.step()
        else:  &#47&#47 distributed training with global net</code></pre><h3>After Change</h3><pre><code class='java'>

    def training_step(self, x=None, y=None, loss=None, retain_graph=False, lr_t=None):
        &quot&quot&quotTakes a single training step: one forward and one backwards pass&quot&quot&quot
        <a id="change">if hasattr(self, &quotmodel_tails&quot) and x is not None:
            raise ValueError(&quotLoss computation from x,y not supported for multitails&quot)
       </a> self.lr_scheduler.step(epoch=lr_t)
        self.train()
        self.optim.zero_grad()
        if loss is None:</code></pre>