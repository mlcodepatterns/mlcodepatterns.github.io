<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    transforms = zip(sampled_dxs, sampled_dys, sampled_angles)

  adv_xs = []
  accs = <a id="change">[]</a>

  &#47&#47 Perform the transformation
  for (dx, dy, angle) in transforms:
    adv_xs.append(_apply_transformation(x, dx, dy, angle, batch_size))</code></pre><h3>After Change</h3><pre><code class='java'>
    sampled_dys = np.random.choice(dys, n_samples)
    sampled_angles = np.random.choice(angles, n_samples)
    transforms = zip(sampled_dxs, sampled_dys, sampled_angles)
  <a id="change">transformed_ims = parallel_apply_transformations(x, transforms, black_border_size)</a>

  def _compute_xent(x):
    preds = model.get_logits(x)
    return tf.nn.softmax_cross_entropy_with_logits_v2(
        labels=y, logits=preds)

  all_xents = tf.map_fn(
      _compute_xent,
      transformed_ims,
      parallel_iterations=1) &#47&#47 Must be 1 to avoid keras race conditions

  &#47&#47 Return the adv_x with worst accuracy

  &#47&#47 all_xents is n_total_samples x batch_size (SB)
  all_xents = tf.stack(all_xents) &#47&#47 SB

  &#47&#47 We want the worst case sample, with the largest xent_loss
  <a id="change">worst_sample_idx = tf.argmax(all_xents, axis=0)</a>  &#47&#47 B

  <a id="change">batch_size = tf.shape(x)[0]</a>
  keys = tf.stack([
      tf.range(batch_size, dtype=tf.int32),
      tf.cast(worst_sample_idx, tf.int32)
  ], axis=1)
  transformed_ims_bshwc = tf.einsum(&quotsbhwc-&gt;bshwc&quot, transformed_ims)
  <a id="change">after_lookup = tf.gather_nd(transformed_ims_bshwc, keys)</a>  &#47&#47 BHWC
  return after_lookup

</code></pre>