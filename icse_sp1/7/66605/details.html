<html><h3>b6af7a54569d20d53080bc09687732c8325b00a8,pyglmnet/pyglmnet.py,GLM,fit,#GLM#Any#Any#,664
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Initialize loss accumulators
        if callable(self.callback):
            <a id="change">self.loss_trace</a> = list()

        &#47&#47 Iterative updates
        for t in range(0, self.max_iter):
            if self.solver == &quotbatch-gradient&quot:
                grad = _grad_L2loss(self.distr,
                                    alpha, self.Tau,
                                    reg_lambda, X, y, self.eta,
                                    beta)
                if t &gt; 1:
                    if np.linalg.norm(grad) / np.linalg.norm(beta) &lt; tol / lr:
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
                beta = beta - self.learning_rate * grad

            elif self.solver == &quotcdfast&quot:
                beta_old = deepcopy(beta)
                beta, z = \
                    self._cdfast(X, y, z, ActiveSet, beta, reg_lambda)
                if t &gt; 1:
                    if ((np.linalg.norm(beta - beta_old) /
                         np.linalg.norm(beta_old) &lt; tol / lr)):
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
            &#47&#47 Apply proximal operator
            beta[1:] = self._prox(beta[1:], reg_lambda * alpha)

            &#47&#47 Update active set
            if self.solver == &quotcdfast&quot:
                ActiveSet[beta == 0] = 0
                ActiveSet[0] = 1.

            &#47&#47 Compute and save loss if callbacks are requested
            <a id="change">if callable(self.callback):
                self.loss_trace.append(self.callback(self.distr, alpha,
                                                     self.Tau, reg_lambda,
                                                     X, y, self.eta,
                                                     self.group, beta))

        &#47&#47 Update the estimated variables
       </a> self.beta0_ = beta[0]
        self.beta_ = beta[1:]
        self.ynull_ = np.mean(y)
        return self</code></pre><h3>After Change</h3><pre><code class='java'>
                beta[k], z = beta[k] - update, z - update * xk
        return beta, z

    def fit(<a id="change">self</a>, X, y):
        The fit function.

        Parameters
        ----------
        X : array
            The input data of shape (n_samples, n_features)

        y : array
            The target data

        Returns
        -------
        self : instance of GLM
            The fitted model.
        
        np.random.RandomState(self.random_state)
        lr = self.learning_rate

        &#47&#47 checks for group
        if self.group is not None:
            self.group = np.array(self.group)
            self.group.dtype = np.int64
            &#47&#47 shape check
            if self.group.shape[0] != X.shape[1]:
                raise ValueError(&quotgroup should be (n_features,)&quot)
            &#47&#47 int check
            if not np.all([isinstance(g, np.int64) for g in self.group]):
                raise ValueError(&quotall entries of group should be integers&quot)

        &#47&#47 type check for data matrix
        if not isinstance(X, np.ndarray):
            raise ValueError(&quotInput data should be of type ndarray (got %s).&quot
                             % type(X))

        n_features = X.shape[1]

        &#47&#47 Initialize parameters
        beta = np.zeros((n_features + 1,))
        if self.beta0_ is None and self.beta_ is None:
            beta[0] = 1 / (n_features + 1) * np.random.normal(0.0, 1.0, 1)
            beta[1:] = 1 / (n_features + 1) * \
                np.random.normal(0.0, 1.0, (n_features, ))
        else:
            beta[0] = self.beta0_
            beta[1:] = self.beta_

        logger.info(&quotLambda: %6.4f&quot % self.reg_lambda)

        tol = self.tol
        alpha = self.alpha
        reg_lambda = self.reg_lambda

        if self.solver == &quotcdfast&quot:
            ActiveSet = np.ones(n_features + 1)     &#47&#47 init active set
            z = beta[0] + np.dot(X, beta[1:])       &#47&#47 cache z

        &#47&#47 Iterative updates
        for t in range(0, self.max_iter):
            if self.solver == &quotbatch-gradient&quot:
                grad = _grad_L2loss(self.distr,
                                    alpha, self.Tau,
                                    reg_lambda, X, y, self.eta,
                                    beta)
                if t &gt; 1:
                    if np.linalg.norm(grad) / np.linalg.norm(beta) &lt; tol / lr:
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
                beta = beta - self.learning_rate * grad

            elif self.solver == &quotcdfast&quot:
                beta_old = deepcopy(beta)
                beta, z = \
                    self._cdfast(X, y, z, ActiveSet, beta, reg_lambda)
                if t &gt; 1:
                    if ((np.linalg.norm(beta - beta_old) /
                         np.linalg.norm(beta_old) &lt; tol / lr)):
                        msg = (&quot\tConverged in {0:d} iterations&quot.format(t))
                        logger.info(msg)
                        break
            &#47&#47 Apply proximal operator
            beta[1:] = self._prox(beta[1:], reg_lambda * alpha)

            &#47&#47 Update active set
            if self.solver == &quotcdfast&quot:
                ActiveSet[beta == 0] = 0
                ActiveSet[0] = 1.

            &#47&#47 Compute and save loss if callbacks are requested
            if callable(self.callback):
                <a id="change">self.callback(beta)</a>

        &#47&#47 Update the estimated variables
        self.beta0_ = beta[0]
        self.beta_ = beta[1:]</code></pre><img src="306966409.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/glm-tools/pyglmnet/commit/b6af7a54569d20d53080bc09687732c8325b00a8#diff-cc78bf166299339921d4c5dadf0499912041bb7b8415db1f800a54870a6fb141L662' target='_blank'>Link</a></div><div id='project'> Project Name: glm-tools/pyglmnet</div><div id='commit'> Commit Name: b6af7a54569d20d53080bc09687732c8325b00a8</div><div id='time'> Time: 2018-09-08</div><div id='author'> Author: mainakjas@gmail.com</div><div id='file'> File Name: pyglmnet/pyglmnet.py</div><div id='class'> Class Name: GLM</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/scipy/scipy/commit/a50ca1edd57935e1006cdb146d6c5bcf231c859f#diff-f4cfd19012a5ad40c462685c5dfaf5d6916b5334f7d19dd114376a01b0c86cc1L715' target='_blank'>Link</a></div><div id='project'> Project Name: scipy/scipy</div><div id='commit'> Commit Name: a50ca1edd57935e1006cdb146d6c5bcf231c859f</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: andyfaff@gmail.com</div><div id='file'> File Name: scipy/optimize/_differentialevolution.py</div><div id='class'> Class Name: DifferentialEvolutionSolver</div><div id='method'> Method Name: solve</div><BR><BR><div id='link'><a href='https://github.com/glm-tools/pyglmnet/commit/fca0d19f28eb612862376edc73e0d1ed4c397e9f#diff-cc78bf166299339921d4c5dadf0499912041bb7b8415db1f800a54870a6fb141L664' target='_blank'>Link</a></div><div id='project'> Project Name: glm-tools/pyglmnet</div><div id='commit'> Commit Name: fca0d19f28eb612862376edc73e0d1ed4c397e9f</div><div id='time'> Time: 2018-09-07</div><div id='author'> Author: mainakjas@gmail.com</div><div id='file'> File Name: pyglmnet/pyglmnet.py</div><div id='class'> Class Name: GLM</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/glm-tools/pyglmnet/commit/b6af7a54569d20d53080bc09687732c8325b00a8#diff-cc78bf166299339921d4c5dadf0499912041bb7b8415db1f800a54870a6fb141L664' target='_blank'>Link</a></div><div id='project'> Project Name: glm-tools/pyglmnet</div><div id='commit'> Commit Name: b6af7a54569d20d53080bc09687732c8325b00a8</div><div id='time'> Time: 2018-09-08</div><div id='author'> Author: mainakjas@gmail.com</div><div id='file'> File Name: pyglmnet/pyglmnet.py</div><div id='class'> Class Name: GLM</div><div id='method'> Method Name: fit</div><BR>