<html><h3>963710e3d38c9ad1d8b8cc1419a3bd1b3dddde1f,opennmt/utils/optim.py,,optimize,#Any#Any#Any#,90
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    decay_fn = None

  learning_rate = float(params["learning_rate"])
  <a id="change">clip_gradients = params.get("clip_gradients")</a>
  if clip_gradients is not None:
    clip_gradients = float(clip_gradients)

  optimizer_class = get_optimizer_class(params["optimizer"])</code></pre><h3>After Change</h3><pre><code class='java'>
    loss += regularization_penalty(regularization["type"], regularization["scale"])

  global_step = tf.train.get_or_create_global_step()
  <a id="change">with tf.variable_scope("optim"):
    &#47&#47 Learning rate.
    learning_rate = tf.get_variable(
        "learning_rate",
        [],
        trainable=False,
        initializer=tf.constant_initializer(float(params["learning_rate"])))
    if "decay_type" in params:
      decay_fn = learning_rate_decay_fn(
          params["decay_type"],
          params["decay_rate"],
          params["decay_steps"],
          decay_step_duration=params.get("decay_step_duration", 1),
          staircase=params.get("staircase", True),
          start_decay_steps=params.get("start_decay_steps", 0),
          minimum_learning_rate=params.get("minimum_learning_rate", 0))
      learning_rate = decay_fn(learning_rate, global_step)
    tf.summary.scalar("learning_rate", learning_rate)

    &#47&#47 Optimizer.
    optimizer_class = get_optimizer_class(params["optimizer"])
    optimizer_params = params.get("optimizer_params", {})
    if optimizer_class.__name__ == "AdafactorOptimizer":
      optimizer = optimizers.get_adafactor_optimizer_from_params(
          optimizer_class, optimizer_params, learning_rate=learning_rate)
    else:
      optimizer = optimizer_class(learning_rate, **optimizer_params)
    if mixed_precision:
      optimizer = optimizers.MixedPrecisionOptimizerWrapper(
          optimizer, loss_scale=get_loss_scale_from_params(params))

    &#47&#47 Gradients.
    gradients = optimizer.compute_gradients(loss, colocate_gradients_with_ops=True)
    _summarize_gradients_norm("global_norm/gradient_norm", gradients)
    if "clip_gradients" in params:
      gradients = _clip_gradients_by_norm(gradients, float(params["clip_gradients"]))
      _summarize_gradients_norm("global_norm/clipped_gradient_norm", gradients)

    return optimizer.apply_gradients(gradients, global_step=global_step)

</a>def regularization_penalty(regularization_type, scale, weights_list=None):
  Computes the weights regularization penalty.

  Args:</code></pre><img src="31141855.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-tf/commit/963710e3d38c9ad1d8b8cc1419a3bd1b3dddde1f#diff-ff657e253fb906e24176aa24effb3097b746169231a46b0016fb4daf4a86b895L102' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-tf</div><div id='commit'> Commit Name: 963710e3d38c9ad1d8b8cc1419a3bd1b3dddde1f</div><div id='time'> Time: 2018-11-09</div><div id='author'> Author: guillaumekln@users.noreply.github.com</div><div id='file'> File Name: opennmt/utils/optim.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: optimize</div><BR><BR><div id='link'><a href='https://github.com/analysiscenter/batchflow/commit/11c6bf1dbd051d087519ff771b39dac600c0d96d#diff-fc05731da9de3387518300d5bef20c3b17f0ba8aa1403173e064d0b06b2ddb7cL165' target='_blank'>Link</a></div><div id='project'> Project Name: analysiscenter/batchflow</div><div id='commit'> Commit Name: 11c6bf1dbd051d087519ff771b39dac600c0d96d</div><div id='time'> Time: 2019-07-29</div><div id='author'> Author: 7520522+a-arefina@users.noreply.github.com</div><div id='file'> File Name: batchflow/models/tf/encoder_decoder.py</div><div id='class'> Class Name: EncoderDecoder</div><div id='method'> Method Name: head</div><BR><BR><div id='link'><a href='https://github.com/dpressel/mead-baseline/commit/08a31864a8e7a633546790d8fed54455b914d96b#diff-73a1ff094a37a7894a6a6b4169b93ab04d67504e9db8d942a3c389afd27dcfb5L65' target='_blank'>Link</a></div><div id='project'> Project Name: dpressel/mead-baseline</div><div id='commit'> Commit Name: 08a31864a8e7a633546790d8fed54455b914d96b</div><div id='time'> Time: 2019-06-18</div><div id='author'> Author: dpressel@gmail.com</div><div id='file'> File Name: python/baseline/pytorch/embeddings.py</div><div id='class'> Class Name: CharConvEmbeddings</div><div id='method'> Method Name: __init__</div><BR>