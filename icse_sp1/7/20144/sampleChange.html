<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

      if loss is None:
        loss = self._loss_fn
      <a id="change">with tf.GradientTape() as tape:
        if len(inputs) == 1:
          inputs = inputs[0]
        outputs = self.model(inputs)
        if isinstance(outputs, tf.Tensor):
          outputs = [outputs]
        if self._loss_outputs is not None:
          outputs = [outputs[i] for i in self._loss_outputs]
        batch_loss = loss(outputs, labels, weights)
     </a> if variables is None:
        vars = self.model.trainable_variables
      else:
        vars = variables</code></pre><h3>After Change</h3><pre><code class='java'>
      loss = self._loss_fn
    var_key = None
    if variables is not None:
      var_key = <a id="change">tuple(v.experimental_ref() for v in variables)</a>

      &#47&#47 The optimizer creates internal variables the first time apply_gradients()
      &#47&#47 is called for a new set of variables.  If that happens inside a function
      &#47&#47 annotated with tf.function it throws an exception, so call it once here.

      zero_grads = [tf.zeros(v.shape) for v in variables]
      self._tf_optimizer.apply_gradients(zip(zero_grads, variables))
    if var_key not in self._gradient_fn_for_vars:
      self._gradient_fn_for_vars[var_key] = self._create_gradient_fn(variables)
    <a id="change">apply_gradient_for_batch = self._gradient_fn_for_vars[var_key]</a>
    time1 = time.time()

    &#47&#47 Main training loop.
</code></pre>