<html><h3>0bb6982bd06bf21de58e61f021626ade1c9b6101,ch14/04_train_ddpg.py,,,#,46
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                &#47&#47 train critic
                optimizer.zero_grad()
                q_v = net.critic(states_v, actions_v)
                q_last_v = <a id="change">tgt_net.target_model(last_states_v)[1]</a>
                q_last_v[dones_mask] = 0.0
                q_ref_v = rewards_v.unsqueeze(dim=-1) + q_last_v * GAMMA
                critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())
                critic_loss_v.backward()</code></pre><h3>After Change</h3><pre><code class='java'>
    test_env = gym.make(ENV_ID)

    act_net = model.DDPGActor(env.observation_space.shape[0], env.action_space.shape[0])
    <a id="change">crt_net = model.DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0])</a>
    if args.cuda:
        act_net.cuda()
        crt_net.cuda()
    print(act_net)
    print(crt_net)
    tgt_act_net = ptan.agent.TargetNet(act_net)
    tgt_crt_net = ptan.agent.TargetNet(crt_net)

    writer = SummaryWriter(comment="-ddpg_" + args.name)
    agent = model.AgentDDPG(act_net, cuda=args.cuda)
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)
    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)
    act_opt = optim.Adam(act_net.parameters(), lr=LEARNING_RATE)
    <a id="change">crt_opt = optim.Adam(crt_net.parameters(), lr=LEARNING_RATE)</a>

    frame_idx = 0
    best_reward = None
    with ptan.common.utils.RewardTracker(writer) as tracker:</code></pre><img src="177575025.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/0bb6982bd06bf21de58e61f021626ade1c9b6101#diff-85d16c683113b421686b267098136cac7f5e82f17b31ef926def5b039fd55506L55' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 0bb6982bd06bf21de58e61f021626ade1c9b6101</div><div id='time'> Time: 2018-02-04</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch14/04_train_ddpg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/fa961f2290c3144f1c5a5c9b8a484610ab835032#diff-e5c61e855c016aea37a65cc575c6454582ff202a0d4cb8ef38f8d18e42f3cdb0L60' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: fa961f2290c3144f1c5a5c9b8a484610ab835032</div><div id='time'> Time: 2019-05-14</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: tests/defences/test_pixel_defend.py</div><div id='class'> Class Name: TestPixelDefend</div><div id='method'> Method Name: test_one_channel</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/e2db3b3f1d3d23cd5bc1e295835e0f4b33e95447#diff-82556a6907b6c5ac8cfa2f65bd9fb14b506a7bd651d71d8bc93415de5f40928cL57' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: e2db3b3f1d3d23cd5bc1e295835e0f4b33e95447</div><div id='time'> Time: 2018-03-07</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: examples/cora_gcn.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>