<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    op5 = tf.group(*[ tf.assign(w,v) for w,v in zip(restored_vars, tmp_vars)])
                    with tf.get_default_graph().control_dependencies([op5]):
                        flin = gswap
                        <a id="change">flin = []</a>
                        for grad, jg in zip(gswap, Jgrads):
                            if jg is None:
                                print("JG NONE", grad)
                                flin += [grad]
                            else:
                                flin += [grad + jg * self._beta]
                            
                        <a id="change">step3 = zip(flin, var_list)</a>
                        op6 = self.optimizer.apply_gradients(step3, global_step=global_step, name=name)
                        with tf.get_default_graph().control_dependencies([op6]):
                            return tf.no_op()
</code></pre><h3>After Change</h3><pre><code class='java'>
                    with tf.get_default_graph().control_dependencies([op5]):

                        op6 = self.optimizer.apply_gradients(grads_and_vars, global_step=global_step, name=name)
                        <a id="change">with tf.get_default_graph().control_dependencies([op6]):
                            consensus_reg = 0.5 * sum(
                                    tf.reduce_sum(tf.square(g)) for g in all_grads[:len(d_vars)] if g is not None
                            )
                            Jgrads = tf.gradients(consensus_reg, d_vars) + [tf.zeros_like(g) for g in g_vars]
                            op7 = [tf.assign_sub(v, (jg * self._beta)) if jg is not None else tf.assign_sub(v,grad) for v,grad, jg in zip(var_list, all_grads, Jgrads)]
                            with tf.get_default_graph().control_dependencies(op7):
                                return tf.no_op()

  
 </a> def _apply_sparse(self, grad, var):
    raise NotImplementedError("Sparse gradient updates are not supported.")
  def variables(self):
      return super().variables() + self.optimizer.variables()</code></pre>