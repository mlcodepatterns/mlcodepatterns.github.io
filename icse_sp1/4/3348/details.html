<html><h3>447885e15243dd18d906e2e35ac34ec6dcf9a600,Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py,PolicyGradient,_build_net,#PolicyGradient#,129
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 build target_net
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&quots_&quot)
        <a id="change">with tf.variable_scope(&quottarget_net&quot):
            self.q_next = self._build_layers(self.s_, self.n_actions, trainable=False)

   </a> def _build_layers(self, inputs, action_size, trainable):
        layers_output = [inputs]
        for i, n_unit in enumerate(self.hidden_layers):
            with tf.variable_scope(&quotlayer%i&quot % i):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.fake_targets = tf.placeholder(tf.float32, [None, 1], name=&quotfake_targets&quot)  &#47&#47 fake targets
        self.advantages = tf.placeholder(tf.float32, [None, 1], name="advantages")  &#47&#47 advantages

        l1 = self._add_layer(&quothidden0&quot, self.x_inputs, self.n_features, 10, <a id="change">tf</a>.nn.relu)     &#47&#47 hidden layer 1
        self.prediction = self._add_layer(&quotoutput&quot, l1, 10, 1, tf.nn.sigmoid)  &#47&#47 predicting for action 0
        with tf.name_scope(&quotloss&quot):
            loglik = self.fake_targets*tf.log(self.prediction) + (1 - self.fake_targets)*tf.log(1-self.prediction)  &#47&#47
            self.loss = -<a id="change">tf.reduce_mean(loglik * self.advantages)</a>
        with tf.name_scope(&quottrain&quot):
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)

    def _add_layer(self, layer_name, inputs, in_size, out_size, activation_function=None):</code></pre><img src="22805640.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/MorvanZhou/tutorials/commit/447885e15243dd18d906e2e35ac34ec6dcf9a600#diff-0b950c40f343f7642bc3f76d52ac955592df4322a1c323d918cc8a630787ead7L100' target='_blank'>Link</a></div><div id='project'> Project Name: MorvanZhou/tutorials</div><div id='commit'> Commit Name: 447885e15243dd18d906e2e35ac34ec6dcf9a600</div><div id='time'> Time: 2016-12-30</div><div id='author'> Author: morvanzhou@hotmail.com</div><div id='file'> File Name: Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py</div><div id='class'> Class Name: PolicyGradient</div><div id='method'> Method Name: _build_net</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/0c361196698684acd135c3bae372c92692c1d5e0#diff-87ad50e157eb29abedfcc00e3683663fc1feb68653dc988ba2ab7a475858ec66L32' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 0c361196698684acd135c3bae372c92692c1d5e0</div><div id='time'> Time: 2017-02-14</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: tensorlayer/cost.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: cross_entropy</div><BR><BR><div id='link'><a href='https://github.com/zsdonghao/text-to-image/commit/74796ff02e9425ca336f595978fe6e7c422c0378#diff-87ad50e157eb29abedfcc00e3683663fc1feb68653dc988ba2ab7a475858ec66L32' target='_blank'>Link</a></div><div id='project'> Project Name: zsdonghao/text-to-image</div><div id='commit'> Commit Name: 74796ff02e9425ca336f595978fe6e7c422c0378</div><div id='time'> Time: 2017-04-11</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: tensorlayer/cost.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: cross_entropy</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/cleverhans/commit/06e9e1f8737c47869ae092c0e769f1e754d4de61#diff-af7280b91c7487fcffb26fac7b4444ddad24c6c49cf21056f4a45a3445fbe7bdL565' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/cleverhans</div><div id='commit'> Commit Name: 06e9e1f8737c47869ae092c0e769f1e754d4de61</div><div id='time'> Time: 2018-10-03</div><div id='author'> Author: windqaq@gmail.com</div><div id='file'> File Name: cleverhans/picklable_model.py</div><div id='class'> Class Name: PerImageStandardize</div><div id='method'> Method Name: fprop</div><BR><BR><div id='link'><a href='https://github.com/MorvanZhou/tutorials/commit/447885e15243dd18d906e2e35ac34ec6dcf9a600#diff-0b950c40f343f7642bc3f76d52ac955592df4322a1c323d918cc8a630787ead7L129' target='_blank'>Link</a></div><div id='project'> Project Name: MorvanZhou/tutorials</div><div id='commit'> Commit Name: 447885e15243dd18d906e2e35ac34ec6dcf9a600</div><div id='time'> Time: 2016-12-30</div><div id='author'> Author: morvanzhou@hotmail.com</div><div id='file'> File Name: Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py</div><div id='class'> Class Name: PolicyGradient</div><div id='method'> Method Name: _build_net</div><BR>