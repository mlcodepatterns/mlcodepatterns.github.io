<html><h3>6e0c869f5e17c734010fe7cc1b975ef178af9d1c,agents/dagger/train/train.py,,run,#Any#Any#Any#,45
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    init_op = tf.global_variables_initializer()
    pretrained_var_map = {}
    for v in <a id="change">tf.trainable_variables()</a>:
        found = False
        for bad_layer in ["fc6", "fc7", "fc8"]:
            if bad_layer in v.name:</code></pre><h3>After Change</h3><pre><code class='java'>
def run(resume_dir=None, data_dir=c.RECORDING_DIR, agent_name=None):
    &#47&#47 TODO: Clean up the net_name stuff
    if agent_name is None:
        <a id="change">net_name = net.ALEXNET_NAME</a>
    elif agent_name == &quotdagger_mobilenet_v2&quot:
        net_name = net.MOBILENET_V2_NAME
    else:
        raise NotImplementedError(&quot%r agent_name not associated with trunk net&quot % agent_name)
    os.makedirs(c.TENSORFLOW_OUT_DIR, exist_ok=True)
    if resume_dir is not None:
        date_str = resume_dir[resume_dir.rindex(&quot/&quot) + 1:resume_dir.rindex(&quot_&quot)]
    else:
        date_str = c.DATE_STR
    sess_train_dir = &quot%s/%s_train&quot % (c.TENSORFLOW_OUT_DIR, date_str)
    sess_eval_dir = &quot%s/%s_eval&quot % (c.TENSORFLOW_OUT_DIR, date_str)
    os.makedirs(sess_train_dir, exist_ok=True)
    os.makedirs(sess_eval_dir, exist_ok=True)

    &#47&#47 Decrease this to fit in your GPU&quots memory
    &#47&#47 If you increase, remember that it decreases accuracy https://arxiv.org/abs/1711.00489
    batch_size = 32

    x = tf.placeholder(tf.float32, (None,) + c.BASELINE_IMAGE_SHAPE)
    y = tf.placeholder(tf.float32, (None, c.NUM_TARGETS))
    log.info(&quotcreating model&quot)
    with tf.variable_scope("model"):
        global_step = tf.get_variable("global_step", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)

    if net_name == net.ALEXNET_NAME:
        eval_model_out, model_in, model_out = setup_alexnet(x, net_name)
    elif net_name == net.MOBILENET_V2_NAME:
        eval_model_out, model_in, model_out = setup_mobilenet_v2(net_name)
    else:
        raise NotImplementedError(&quot%r net not implemented&quot % net_name)

    l2_norm = tf.global_norm(tf.trainable_variables())
    loss = 0.5 * tf.reduce_sum(tf.square(model_out - y)) / tf.to_float(tf.shape(x)[0])
    tf.summary.scalar("model/loss", loss)
    tf.summary.scalar("model/l2_norm", l2_norm)
    total_loss = loss + 0.0005 * l2_norm
    tf.summary.scalar("model/total_loss", total_loss)
    starter_learning_rate = 2e-6

    &#47&#47 TODO: add polyak averaging.
    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step,
                                               decay_steps=73000, decay_rate=0.5, staircase=True)

    opt = tf.train.AdamOptimizer(learning_rate)
    tf.summary.scalar("model/learning_rate", learning_rate)
    grads_and_vars = opt.compute_gradients(total_loss)
    visualize_model(model_in, model_out, y)
    visualize_gradients(grads_and_vars)
    summary_op = tf.summary.merge_all()
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = opt.apply_gradients(grads_and_vars, global_step)

    init_op = tf.global_variables_initializer()

    <a id="change">if net_name == net.ALEXNET_NAME:
        init_fn = load_alexnet_pretrained(init_op)
    else:
        def init_fn(ses):
            log.info(&quotInitializing parameters.&quot)
            ses.run(init_op)

   </a> saver = tf.train.Saver()
    sv = tf.train.Supervisor(is_chief=True,
                             logdir=sess_train_dir,
                             summary_op=None,  &#47&#47 Automatic summaries don&quott work with placeholders.</code></pre><img src="31261672.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/deepdrive/deepdrive/commit/6e0c869f5e17c734010fe7cc1b975ef178af9d1c#diff-a6990461cb604763e33e96c3b59a1eaceb5fe264be4f5374eec0744c3d4abc28L46' target='_blank'>Link</a></div><div id='project'> Project Name: deepdrive/deepdrive</div><div id='commit'> Commit Name: 6e0c869f5e17c734010fe7cc1b975ef178af9d1c</div><div id='time'> Time: 2018-04-26</div><div id='author'> Author: cquiter@gmail.com</div><div id='file'> File Name: agents/dagger/train/train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: run</div><BR><BR><div id='link'><a href='https://github.com/asyml/texar/commit/1c499b27e225d0c3b8634347c1bb924e26842cd1#diff-9818cbbc6b54368d6dfa3c8b19e1e6936e0b0c9ab018053fc1f943dc547ef8e6L133' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: 1c499b27e225d0c3b8634347c1bb924e26842cd1</div><div id='time'> Time: 2017-09-16</div><div id='author'> Author: zhitinghu@gmail.com</div><div id='file'> File Name: txtgen/modules/decoders/rnn_decoder_base.py</div><div id='class'> Class Name: RNNDecoderBase</div><div id='method'> Method Name: _build</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/fe0950e68d35e29237d9d5ca82aabc8ff8a011ba#diff-c5f3f08e1e900c0c22a20aa29110aad58e7ee25c181663ce36196263686a5c67L171' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: fe0950e68d35e29237d9d5ca82aabc8ff8a011ba</div><div id='time'> Time: 2017-07-25</div><div id='author'> Author: wenqi.li@ucl.ac.uk</div><div id='file'> File Name: niftynet/engine/application_driver.py</div><div id='class'> Class Name: ApplicationDriver</div><div id='method'> Method Name: _create_graph</div><BR>