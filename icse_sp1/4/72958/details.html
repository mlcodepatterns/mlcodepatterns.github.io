<html><h3>0ac2b33e8c63304a50db7d2b484368299706b58b,slm_lab/agent/net/mlp.py,MLPNet,training_step,#MLPNet#Any#Any#Any#Any#Any#,130
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            assert_trained = net_util.gen_assert_trained(model)
        loss.backward(retain_graph=retain_graph)
        if self.clip_grad:
            <a id="change">logger.debug(f&quotClipping gradient: {self.clip_grad_val}&quot)</a>
            torch.nn.utils.clip_grad_norm_(self.parameters(), self.clip_grad_val)
        if global_net is None:
            self.optim.step()
        else:  &#47&#47 distributed training with global net</code></pre><h3>After Change</h3><pre><code class='java'>
        Takes a single training step: one forward and one backwards pass
        More most RL usage, we have custom, often complicated, loss functions. Compute its value and put it in a pytorch tensor then pass it in as loss
        &quot&quot&quot
        <a id="change">if hasattr(self, &quotmodel_tails&quot) and x is not None:
            raise ValueError(&quotLoss computation from x,y not supported for multitails&quot)
       </a> self.lr_scheduler.step(epoch=lr_t)
        self.train()
        self.optim.zero_grad()
        if loss is None:</code></pre><img src="330777589.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/kengz/SLM-Lab/commit/0ac2b33e8c63304a50db7d2b484368299706b58b#diff-320990264044727ba6864580bc7379bcde13a7ba9d68ccf57468857ee2a08076L121' target='_blank'>Link</a></div><div id='project'> Project Name: kengz/SLM-Lab</div><div id='commit'> Commit Name: 0ac2b33e8c63304a50db7d2b484368299706b58b</div><div id='time'> Time: 2018-11-14</div><div id='author'> Author: kengzwl@gmail.com</div><div id='file'> File Name: slm_lab/agent/net/mlp.py</div><div id='class'> Class Name: MLPNet</div><div id='method'> Method Name: training_step</div><BR><BR><div id='link'><a href='https://github.com/Qiskit/qiskit-aqua/commit/b5e847a5bf5f38ef30ebb2328c4d58ea28aa464a#diff-48cd7eb9abf22ff4b493a0c142ee0f52cebe61bc9e72f29c5a2d40d8a98a6652L64' target='_blank'>Link</a></div><div id='project'> Project Name: Qiskit/qiskit-aqua</div><div id='commit'> Commit Name: b5e847a5bf5f38ef30ebb2328c4d58ea28aa464a</div><div id='time'> Time: 2019-01-03</div><div id='author'> Author: dongreenberg2@gmail.com</div><div id='file'> File Name: qiskit_aqua/utils/circuit_cache.py</div><div id='class'> Class Name: CircuitCache</div><div id='method'> Method Name: cache_circuit</div><BR><BR><div id='link'><a href='https://github.com/kengz/SLM-Lab/commit/0ac2b33e8c63304a50db7d2b484368299706b58b#diff-54189061d60564151db3a95e988fca02848fed6bf25221c04ab5283d4d20e69fL164' target='_blank'>Link</a></div><div id='project'> Project Name: kengz/SLM-Lab</div><div id='commit'> Commit Name: 0ac2b33e8c63304a50db7d2b484368299706b58b</div><div id='time'> Time: 2018-11-14</div><div id='author'> Author: kengzwl@gmail.com</div><div id='file'> File Name: slm_lab/agent/net/recurrent.py</div><div id='class'> Class Name: RecurrentNet</div><div id='method'> Method Name: training_step</div><BR>