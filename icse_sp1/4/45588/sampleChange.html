<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
test_loss = evaluate(best_model, test_data)
print(&quot=&quot * 89)
print(&quot| End of training | test loss {:5.2f} | test ppl {:8.2f}&quot.format(
    test_loss, <a id="change">math.exp(test_loss)</a>))
print(&quot=&quot * 89)
</code></pre><h3>After Change</h3><pre><code class='java'>
nlayers = 2 &#47&#47 the number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead = 2 &#47&#47 the number of heads in the multiheadattention models
dropout = 0.2 &#47&#47 the dropout value
model = <a id="change">TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</a>


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Run the model
&#47&#47 -------------
&#47&#47


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 `CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss&#47&#47torch.nn.CrossEntropyLoss&gt;`__
&#47&#47 is applied to track the loss and
&#47&#47 `SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd&#47&#47torch.optim.SGD&gt;`__
&#47&#47 implements stochastic gradient descent method as the optimizer. The initial
&#47&#47 learning rate is set to 5.0. `StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr&#47&#47torch.optim.lr_scheduler.StepLR&gt;`__ is
&#47&#47 applied to adjust the learn rate through epochs. During the
&#47&#47 training, we use
&#47&#47 `nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm&#47&#47torch.nn.utils.clip_grad_norm_&gt;`__
&#47&#47 function to scale all the gradient together to prevent exploding.
&#47&#47

criterion = nn.CrossEntropyLoss()
lr = 5.0 &#47&#47 learning rate
<a id="change">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</a>
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

import time
def train():</code></pre>