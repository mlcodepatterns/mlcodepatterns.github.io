<html><h3>b70e026702c590618552ab857fc6661662ab72f2,allennlp/modules/matrix_attention/linear_matrix_attention.py,LinearMatrixAttention,forward,#LinearMatrixAttention#Any#Any#,67
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                matrix_2: torch.Tensor) -&gt; torch.Tensor:
        &#47&#47 TODO(mattg): Remove the need for this tiling.
        &#47&#47 https://github.com/allenai/allennlp/pull/1235&#47&#47issuecomment-391540133
        tiled_matrix_1 = <a id="change">matrix_1.unsqueeze(2).expand(matrix_1.size()[0],
                                                      matrix_1.size()[1],
                                                      matrix_2.size()[1],
                                                      matrix_1.size()[2])</a>
        tiled_matrix_2 = matrix_2.unsqueeze(1).expand(matrix_2.size()[0],
                                                      matrix_1.size()[1],
                                                      matrix_2.size()[1],
                                                      matrix_2.size()[2])

        combined_tensors = util.combine_tensors(self._combination, [tiled_matrix_1, tiled_matrix_2])
        <a id="change">dot_product = torch.matmul(combined_tensors, self._weight_vector)</a>
        return self._activation(dot_product + self._bias)
</code></pre><h3>After Change</h3><pre><code class='java'>
                matrix_1: torch.Tensor,
                matrix_2: torch.Tensor) -&gt; torch.Tensor:
        combined_tensors = util.combine_tensors_and_multiply(self._combination,
                                                             <a id="change">[matrix_1.unsqueeze(2), matrix_2.unsqueeze(1)]</a>,
                                                             self._weight_vector)
        return self._activation(combined_tensors + self._bias)
</code></pre><img src="218583718.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/b70e026702c590618552ab857fc6661662ab72f2#diff-ddaf73064e75629780c9fe98674eb9289242cde4c2d5ac19da6b739d6d8dae53L70' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: b70e026702c590618552ab857fc6661662ab72f2</div><div id='time'> Time: 2018-08-21</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: allennlp/modules/matrix_attention/linear_matrix_attention.py</div><div id='class'> Class Name: LinearMatrixAttention</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/asappresearch/sru/commit/faf3aa876462323f2fa721ebd633752d6489808f#diff-fbdb17c4f9a1012308449ed128e27fceac2cd837fa198a5afc3ef73ed9cd6d0cL536' target='_blank'>Link</a></div><div id='project'> Project Name: asappresearch/sru</div><div id='commit'> Commit Name: faf3aa876462323f2fa721ebd633752d6489808f</div><div id='time'> Time: 2020-09-18</div><div id='author'> Author: taolei@csail.mit.edu</div><div id='file'> File Name: sru/modules.py</div><div id='class'> Class Name: SRU</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/open-mmlab/mmdetection/commit/a6236b789b8f4e2e66c8379199f40ecef9afce06#diff-4bb3c4d14da32c29e21b1826a72bd9ae0df1d7f89d1b3842293a1d96ce2621ecL84' target='_blank'>Link</a></div><div id='project'> Project Name: open-mmlab/mmdetection</div><div id='commit'> Commit Name: a6236b789b8f4e2e66c8379199f40ecef9afce06</div><div id='time'> Time: 2020-04-21</div><div id='author'> Author: 40779233+ZwwWayne@users.noreply.github.com</div><div id='file'> File Name: mmdet/core/anchor/anchor_generator.py</div><div id='class'> Class Name: AnchorGenerator</div><div id='method'> Method Name: valid_flags</div><BR>