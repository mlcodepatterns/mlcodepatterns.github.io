<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            partition.append((start, end))
            start = end

        return <a id="change">{
            "state": [s["state"] for s in self._all_states],
            "param_groups": param_groups,
            "partition": partition,
            "local_state_dict": False,
        }</a>

    @staticmethod
    def rank_local_state_dict(rank: int, state_dict: dict) -&gt; dict:
        Returns the local_state_dict for a given rank.</code></pre><h3>After Change</h3><pre><code class='java'>
        

        if len(self._all_states) == 0:
            <a id="change">raise RuntimeError(
                "Optimizer state has not been consolidated on this rank. \
                Please call `consolidate_state_dict()` on all ranks beforehand if you meant to save the global state"
            )</a>

        &#47&#47 Unify the shard states and the state that pytorch would expect, given the model.
        &#47&#47 Indexation needs several redirections, since each shard only knows a limited scope of the model
        &#47&#47 - get the pytorch compliant parameter indexing</code></pre>