<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        optimizer = config.get(&quotoptimizer&quot)

        &#47&#47 TODO write summaries
        <a id="change">self.summary_writer = tf.summary.FileWriter(&quotlog&quot + "_%d" % task_index)</a>

        if not optimizer:
            self.optimizer = tf.train.AdamOptimizer(self.alpha)
        else:</code></pre><h3>After Change</h3><pre><code class='java'>

                self.global_prev_action_means = tf.placeholder(tf.float32, [None, self.action_count], name=&quotprev_actions&quot)

                <a id="change">if self.continuous:
                    self.global_policy = GaussianPolicy(self.global_network, self.session, self.global_state, self.random,
                                                 self.action_count, &quotgaussian_policy&quot)
                    self.global_prev_action_log_stds = tf.placeholder(tf.float32, [None, self.action_count])

                    self.global_prev_dist = dict(policy_output=self.global_prev_action_means,
                                          policy_log_std=self.global_prev_action_log_stds)

                else:
                    self.global_policy = CategoricalOneHotPolicy(self.global_network, self.session, self.global_state, self.random,
                                                          self.action_count, &quotcategorical_policy&quot)
                    self.global_prev_dist = dict(policy_output=self.global_prev_action_means)

                &#47&#47 Probability distribution used in the current policy
               </a> self.global_baseline_value_function = LinearValueFunction()

        self.optimizer = config.get(&quotoptimizer&quot)
        self.optimizer_args = config.get(&quotoptimizer_args&quot, [])</code></pre>