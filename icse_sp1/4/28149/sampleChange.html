<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if dtype == &quotfloat16&quot:
      &#47&#47 TODO(reedwm): Remove manually wrapping optimizer once mixed precision
      &#47&#47 can be enabled with a single line of code.
      <a id="change">if flags_dict["fp16_implementation"] == "graph_rewrite":
          optimizer = tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)
      else:
          optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
              optimizer, loss_scale=flags_core.get_loss_scale(flags_obj,
                                                              default_for_fp16=128))

   </a> if flags_obj.use_trivial_model:
      model = trivial_model.trivial_model(
          imagenet_preprocessing.NUM_CLASSES, dtype)
    else:</code></pre><h3>After Change</h3><pre><code class='java'>
      optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
          optimizer, loss_scale=flags_core.get_loss_scale(flags_obj,
                                                          default_for_fp16=128))
    <a id="change">pdb.set_trace()</a>
    if flags_obj.fp16_implementation == "graph_rewrite":
      &#47&#47 Note: when flags_obj["fp16_implementation"] == "graph_rewrite", 
      &#47&#47 dtype as determined by flags_core.get_tf_dtype(flags_obj) would be &quotfloat32&quot
      &#47&#47</code></pre>