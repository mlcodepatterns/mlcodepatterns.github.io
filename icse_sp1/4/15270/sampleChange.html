<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 So we sneak in the copy by going via torch.no_grad(), and then fix the gradients after the fact (without
    &#47&#47 modifying k, so the version doesn&quott increment) with _DistributeGradients.
    k = torch.empty(*f0.shape, tableau.length + 1, dtype=f0.dtype, device=f0.device)
    <a id="change">with torch.no_grad():
        k[..., 0] = f0  &#47&#47 sneak past the version checker
   </a> fs = [f0]
    for i, (alpha_i, beta_i) in enumerate(zip(tableau.alpha, tableau.beta)):
        ti = t0 + alpha_i * dt
        yi = y0 + k[..., :i + 1].matmul(beta_i * dt).view_as(f0)</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Little bit of black magic coming up.
    &#47&#47 We use an unchecked assign to put data into k without increments its _version counter, so that the backward
    &#47&#47 doesn&quott throw an error about in-place correctness. We know that it&quots actually correct.
    k = torch.empty(*f0.shape, <a id="change">len(tableau.alpha)</a> + 1, dtype=f0.dtype, device=f0.device)
    k = _UncheckedAssign.apply(k, f0, (..., 0))
    for i, (alpha_i, beta_i) in enumerate(zip(tableau.alpha, tableau.beta)):
        ti = t0 + alpha_i * dt</code></pre>