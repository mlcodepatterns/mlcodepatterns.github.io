<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

&#47&#47 Create a Trust Region Policy Optimization agent
agent = PPOAgent(
    <a id="change">Configuration(
    log_level=&quotinfo&quot,
    batch_size=4000,

    &#47&#47 max_kl_divergence=0.1,
    &#47&#47 cg_iterations=20,
    &#47&#47 cg_damping=0.001,
    &#47&#47 ls_max_backtracks=10,
    &#47&#47 ls_accept_ratio=0.9,
    &#47&#47 ls_override=False,

    learning_rate=0.001,
    entropy_penalty=0.01,
    epochs=5,
    optimizer_batch_size=512,
    loss_clipping=0.2,
    normalize_advantage=False,
    baseline=dict(
        type="mlp",
        sizes=[32, 32],
        epochs=1,
        update_batch_size=512,
        learning_rate=0.01
    ),
    states=env.states,
    actions=env.actions,
    network=layered_network_builder([
        dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),
        dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
    ])
)</a>)

&#47&#47 Create the runner
runner = Runner(agent=agent, environment=env)</code></pre><h3>After Change</h3><pre><code class='java'>
env = OpenAIGym(&quotCartPole-v0&quot)


<a id="change">config = Configuration(
    batch_size=4096,
    &#47&#47 Agent
    preprocessing=None,
    exploration=None,
    reward_preprocessing=None,
    &#47&#47 BatchAgent
    keep_last_timestep=True,
    &#47&#47 PPOAgent
    step_optimizer=dict(
        type=&quotadam&quot,
        learning_rate=1e-3
    ),
    optimization_steps=10,
    &#47&#47 Model
    scope=&quotppo&quot,
    discount=0.99,
    &#47&#47 DistributionModel
    distributions=None,  &#47&#47 not documented!!!
    entropy_regularization=0.01,
    &#47&#47 PGModel
    baseline_mode=None,
    baseline=None,
    baseline_optimizer=None,
    gae_lambda=None,
    normalize_rewards=False,
    &#47&#47 PGLRModel
    likelihood_ratio_clipping=0.2,
    &#47&#47 Logging
    log_level=&quotinfo&quot,
    &#47&#47 TensorFlow Summaries
    summary_logdir=None,
    summary_labels=[&quottotal-loss&quot],
    summary_frequency=1,
    &#47&#47 Distributed
    &#47&#47 TensorFlow distributed configuration
    cluster_spec=None,
    parameter_server=False,
    task_index=0,
    device=None,
    local_model=False,
    replica_model=False,
)</a>

&#47&#47 Network as list of layers
network_spec = [
    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),</code></pre>