<html><h3>b045bf9b22832610f256da0cda16c2c007158ba6,cleverhans/attacks.py,MomentumIterativeMethod,generate,#MomentumIterativeMethod#Any#,503
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        from . import loss as loss_module
        from . import utils_tf
        <a id="change">for i in range(self.nb_iter):
            &#47&#47 Compute loss
            logits = self.model.get_logits(adv_x)
            loss = loss_module.attack_softmax_cross_entropy(y, logits,
                                                            mean=False)
            if targeted:
                loss = -loss

            &#47&#47 Define gradient of loss wrt input
            grad, = tf.gradients(loss, adv_x)

            &#47&#47 Normalize current gradient and add it to the accumulated gradient
            red_ind = list(xrange(1, len(grad.get_shape())))
            avoid_zero_div = tf.cast(1e-12, grad.dtype)
            grad = grad / tf.maximum(avoid_zero_div,
                                     reduce_mean(tf.abs(grad),
                                                 red_ind, keepdims=True))
            momentum = self.decay_factor * momentum + grad

            if self.ord == np.inf:
                normalized_grad = tf.sign(momentum)
            elif self.ord == 1:
                norm = tf.maximum(avoid_zero_div,
                                  reduce_sum(tf.abs(momentum),
                                             red_ind, keepdims=True))
                normalized_grad = momentum / norm
            elif self.ord == 2:
                square = reduce_sum(tf.square(momentum),
                                    red_ind,
                                    keepdims=True)
                norm = tf.sqrt(tf.maximum(avoid_zero_div, square))
                normalized_grad = momentum / norm
            else:
                raise NotImplementedError("Only L-inf, L1 and L2 norms are "
                                          "currently implemented.")

            &#47&#47 Update and clip adversarial example in current iteration
            scaled_grad = self.eps_iter * normalized_grad
            adv_x = adv_x + scaled_grad
            adv_x = x + utils_tf.clip_eta(adv_x - x, self.ord, self.eps)

            if self.clip_min is not None and self.clip_max is not None:
                adv_x = tf.clip_by_value(adv_x, self.clip_min, self.clip_max)

            adv_x = tf.stop_gradient(adv_x)

       </a> return adv_x

    def parse_params(self, eps=0.3, eps_iter=0.06, nb_iter=10, y=None,
                     ord=np.inf, decay_factor=1.0,</code></pre><h3>After Change</h3><pre><code class='java'>
    from . import loss as loss_module
    from . import utils_tf

    <a id="change">cond = lambda i, _, __: tf.less(i, self.nb_iter)</a>

    def body(i, ax, m):
      preds = self.model.get_probs(adv_x)
      loss = utils_tf.model_loss(y, preds, mean=False)
      if targeted:
        loss = -loss

      &#47&#47 Define gradient of loss wrt input
      grad, = tf.gradients(loss, adv_x)

      &#47&#47 Normalize current gradient and add it to the accumulated gradient
      red_ind = list(xrange(1, len(grad.get_shape())))
      avoid_zero_div = tf.cast(1e-12, grad.dtype)
      grad = grad / tf.maximum(
          avoid_zero_div, reduce_mean(tf.abs(grad), red_ind, keepdims=True))
      momentum = self.decay_factor * momentum + grad

      if self.ord == np.inf:
        normalized_grad = tf.sign(momentum)
      elif self.ord == 1:
        norm = tf.maximum(avoid_zero_div,
                          reduce_sum(tf.abs(momentum), red_ind, keepdims=True))
        normalized_grad = momentum / norm
      elif self.ord == 2:
        square = reduce_sum(tf.square(momentum), red_ind, keepdims=True)
        norm = tf.sqrt(tf.maximum(avoid_zero_div, square))
        normalized_grad = momentum / norm
      else:
        raise NotImplementedError("Only L-inf, L1 and L2 norms are "
                                  "currently implemented.")

      &#47&#47 Update and clip adversarial example in current iteration
      scaled_grad = self.eps_iter * normalized_grad
      adv_x = adv_x + scaled_grad
      adv_x = x + utils_tf.clip_eta(adv_x - x, self.ord, self.eps)

      if self.clip_min is not None and self.clip_max is not None:
        adv_x = tf.clip_by_value(adv_x, self.clip_min, self.clip_max)

      adv_x = tf.stop_gradient(adv_x)

      return i + 1, adv_x, momentum

    <a id="change">_, adv_x, _ = tf.while_loop(
        cond, body, [tf.zeros([]), adv_x, momentum], back_prop=False)</a>

    return adv_x

  def parse_params(self,</code></pre><img src="175554167.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorflow/cleverhans/commit/b045bf9b22832610f256da0cda16c2c007158ba6#diff-d45ee9a62c0464cf8b5175a339c8724af4e1b565aa367d138369f36ec491f89fL533' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/cleverhans</div><div id='commit'> Commit Name: b045bf9b22832610f256da0cda16c2c007158ba6</div><div id='time'> Time: 2018-07-11</div><div id='author'> Author: aidan.n.gomez@gmail.com</div><div id='file'> File Name: cleverhans/attacks.py</div><div id='class'> Class Name: MomentumIterativeMethod</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/XifengGuo/CapsNet-Keras/commit/5cb43a9498315a16412cba20a59e6e76f9721b7b#diff-70158145f8c3017473e154d3e9318cd7ebab5469186eddb6bcbe943621b770a0L94' target='_blank'>Link</a></div><div id='project'> Project Name: XifengGuo/CapsNet-Keras</div><div id='commit'> Commit Name: 5cb43a9498315a16412cba20a59e6e76f9721b7b</div><div id='time'> Time: 2017-10-31</div><div id='author'> Author: guoxifeng1990@163.com</div><div id='file'> File Name: capsulelayers.py</div><div id='class'> Class Name: CapsuleLayer</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/cleverhans/commit/b045bf9b22832610f256da0cda16c2c007158ba6#diff-d45ee9a62c0464cf8b5175a339c8724af4e1b565aa367d138369f36ec491f89fL390' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/cleverhans</div><div id='commit'> Commit Name: b045bf9b22832610f256da0cda16c2c007158ba6</div><div id='time'> Time: 2018-07-11</div><div id='author'> Author: aidan.n.gomez@gmail.com</div><div id='file'> File Name: cleverhans/attacks.py</div><div id='class'> Class Name: BasicIterativeMethod</div><div id='method'> Method Name: generate</div><BR>