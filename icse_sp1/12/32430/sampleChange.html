<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for _ in runner.step_epochs():
            for cycle in range(self._steps_per_epoch):
                runner.step_path = runner.obtain_trajectories(runner.step_itr)
                <a id="change">last_return = self.train_once(runner.step_itr,
                                              runner.step_path)</a>
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    log_performance(runner.step_itr,
                                    obtain_evaluation_samples(
                                        self.policy, self._eval_env),
                                    discount=self._discount)
                runner.step_itr += 1

        <a id="change">return last_return</a>

    def train_once(self, itr, trajectories):
        Perform one step of policy optimization given one batch of samples.
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        if not self._eval_env:
            self._eval_env = runner.get_env_copy()
        <a id="change">last_returns = [float(&quotnan&quot)]</a>
        runner.enable_logging = False

        for _ in runner.step_epochs():
            for cycle in range(self._steps_per_epoch):
                runner.step_path = runner.obtain_trajectories(runner.step_itr)
                self.train_once(runner.step_itr, runner.step_path)
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    <a id="change">eval_samples = obtain_evaluation_samples(
                        self.policy, self._eval_env)</a>
                    <a id="change">last_returns = log_performance(runner.step_itr,
                                                   eval_samples,
                                                   discount=self._discount)</a>
                runner.step_itr += 1

        <a id="change">return np.mean(last_returns)</a>

    def train_once(self, itr, trajectories):
        Perform one step of policy optimization given one batch of samples.
</code></pre>