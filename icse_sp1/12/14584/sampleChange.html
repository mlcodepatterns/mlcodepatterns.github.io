<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    <a id="change">log_performance(runner.step_itr,
                                    obtain_evaluation_samples(
                                        self.policy, self._eval_env),
                                    discount=self._discount)</a>
                runner.step_itr += 1

        return last_return
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        if not self._eval_env:
            self._eval_env = runner.get_env_copy()
        <a id="change">last_returns = [float(&quotnan&quot)]</a>
        runner.enable_logging = False

        qf_losses = []
        for _ in runner.step_epochs():
            for cycle in range(self._steps_per_epoch):
                runner.step_path = runner.obtain_trajectories(runner.step_itr)
                qf_losses.extend(
                    self.train_once(runner.step_itr, runner.step_path))
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    <a id="change">eval_samples = obtain_evaluation_samples(
                        self.policy, self._eval_env)</a>
                    <a id="change">last_returns = log_performance(runner.step_itr,
                                                   eval_samples,
                                                   discount=self._discount)</a>
                runner.step_itr += 1
            tabular.record(&quotDQN/QFLossMean&quot, np.mean(qf_losses))
            tabular.record(&quotDQN/QFLossStd&quot, np.std(qf_losses))

        <a id="change">return np.mean(last_returns)</a>

    def train_once(self, itr, trajectories):
        Perform one step of policy optimization given one batch of samples.
</code></pre>