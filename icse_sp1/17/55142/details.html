<html><h3>c9528bf8c8d5e32f41f64d6d3c294cae19fc2982,train.py,,train,#Any#,32
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    crit = utils.LanguageModelCriterion()
    rl_crit = utils.RewardCriterion()

    <a id="change">optimizer = utils.build_optimizer(model.parameters(), opt)</a>
    &#47&#47 Load the optimizer
    if vars(opt).get(&quotstart_from&quot, None) is not None and os.path.isfile(os.path.join(opt.start_from,"optimizer.pth")):
        optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, &quotoptimizer.pth&quot)))

    while True:
        if update_lr_flag:
                &#47&#47 Assign the learning rate
            if epoch &gt; opt.learning_rate_decay_start and opt.learning_rate_decay_start &gt;= 0:
                frac = (epoch - opt.learning_rate_decay_start) // opt.learning_rate_decay_every
                decay_factor = opt.learning_rate_decay_rate  ** frac
                opt.current_lr = opt.learning_rate * decay_factor
            else:
                opt.current_lr = opt.learning_rate
            <a id="change">utils.set_lr(optimizer, opt.current_lr)</a> &#47&#47 set the decayed rate
            &#47&#47 Assign the scheduled sampling prob
            if epoch &gt; opt.scheduled_sampling_start and opt.scheduled_sampling_start &gt;= 0:
                frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Assure in training mode
    dp_model.train()

    if <a id="change">opt.label_smoothing</a> &gt; 0:
        crit = utils.LabelSmoothing(smoothing=opt.label_smoothing)
    else:
        crit = utils.LanguageModelCriterion()
    rl_crit = utils.RewardCriterion()

    <a id="change">if opt.noamopt:
        optimizer = utils.get_std_opt(model.model)
    else:
        optimizer = utils.build_optimizer(model.parameters(), opt)
    &#47&#47 Load the optimizer
   </a> if vars(opt).get(&quotstart_from&quot, None) is not None and os.path.isfile(os.path.join(opt.start_from,"optimizer.pth")):
        optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, &quotoptimizer.pth&quot)))

    while True:
        if epoch_done:
            <a id="change">if not opt.noamopt:
                &#47&#47 Assign the learning rate
                if epoch &gt; opt.learning_rate_decay_start and opt.learning_rate_decay_start &gt;= 0:
                    frac = (epoch - opt.learning_rate_decay_start) // opt.learning_rate_decay_every
                    decay_factor = opt.learning_rate_decay_rate  ** frac
                    opt.current_lr = opt.learning_rate * decay_factor
                else:
                    opt.current_lr = opt.learning_rate
                utils.set_lr(optimizer, opt.current_lr) &#47&#47 set the decayed rate
            &#47&#47 Assign the scheduled sampling prob
           </a> if epoch &gt; opt.scheduled_sampling_start and opt.scheduled_sampling_start &gt;= 0:
                frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every
                opt.ss_prob = min(opt.scheduled_sampling_increase_prob  * frac, opt.scheduled_sampling_max_prob)
                model.ss_prob = opt.ss_prob

            &#47&#47 If start self critical training
            if opt.self_critical_after != -1 and epoch &gt;= opt.self_critical_after:
                sc_flag = True
                init_scorer(opt.cached_tokens)
            else:
                sc_flag = False

            epoch_done = False
                
        start = time.time()
        &#47&#47 Load data from train split (0)
        data = loader.get_batch(&quottrain&quot)
        print(&quotRead data:&quot, time.time() - start)

        torch.cuda.synchronize()
        start = time.time()

        tmp = [data[&quotfc_feats&quot], data[&quotatt_feats&quot], data[&quotlabels&quot], data[&quotmasks&quot], data[&quotatt_masks&quot]]
        tmp = [_ if _ is None else torch.from_numpy(_).cuda() for _ in tmp]
        fc_feats, att_feats, labels, masks, att_masks = tmp
        
        optimizer.zero_grad()
        if not sc_flag:
            loss = crit(dp_model(fc_feats, att_feats, labels, att_masks), labels[:,1:], masks[:,1:])
        else:
            gen_result, sample_logprobs = dp_model(fc_feats, att_feats, att_masks, opt={&quotsample_max&quot:0}, mode=&quotsample&quot)
            reward = get_self_critical_reward(dp_model, fc_feats, att_feats, att_masks, data, gen_result, opt)
            loss = rl_crit(sample_logprobs, gen_result.data, torch.from_numpy(reward).float().cuda())

        loss.backward()
        utils.clip_gradient(optimizer, opt.grad_clip)
        optimizer.step()
        train_loss = loss.item()
        torch.cuda.synchronize()
        end = time.time()
        if not sc_flag:
            print("iter {} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}" \
                .format(iteration, epoch, train_loss, end - start))
        else:
            print("iter {} (epoch {}), avg_reward = {:.3f}, time/batch = {:.3f}" \
                .format(iteration, epoch, np.mean(reward[:,0]), end - start))

        &#47&#47 Update the iteration and epoch
        iteration += 1
        if data[&quotbounds&quot][&quotwrapped&quot]:
            epoch += 1
            epoch_done = True

        &#47&#47 Write the training loss summary
        if (iteration % opt.losses_log_every == 0):
            add_summary_value(tb_summary_writer, &quottrain_loss&quot, train_loss, iteration)
            <a id="change">if opt.noamopt:
                opt.current_lr = optimizer.rate()
           </a> add_summary_value(tb_summary_writer, &quotlearning_rate&quot, opt.current_lr, iteration)
            add_summary_value(tb_summary_writer, &quotscheduled_sampling_prob&quot, model.ss_prob, iteration)
            if sc_flag:
                add_summary_value(tb_summary_writer, &quotavg_reward&quot, np.mean(reward[:,0]), iteration)</code></pre><img src="253447931.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 15</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/c9528bf8c8d5e32f41f64d6d3c294cae19fc2982#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L71' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: c9528bf8c8d5e32f41f64d6d3c294cae19fc2982</div><div id='time'> Time: 2018-05-31</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/c9528bf8c8d5e32f41f64d6d3c294cae19fc2982#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L71' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: c9528bf8c8d5e32f41f64d6d3c294cae19fc2982</div><div id='time'> Time: 2018-05-31</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/ruotianluo/ImageCaptioning.pytorch/commit/a70d02452dd1346826462f769f775abbf6bc2100#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L72' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/ImageCaptioning.pytorch</div><div id='commit'> Commit Name: a70d02452dd1346826462f769f775abbf6bc2100</div><div id='time'> Time: 2019-04-09</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train</div><BR>