<html><h3>f72a9854c23f9101e75cf7c13fbf8232128a896a,tf_agents/agents/ddpg/examples/train_eval_rnn_dm.py,,train_eval,#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,65
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),
  ]

  with <a id="change">tf.contrib.summary.record_summaries_every_n_global_steps(
      summary_interval)</a>:
    if observations_whitelist is not None:
      env_wrappers = [
          functools.partial(
              wrappers.FlattenObservationsWrapper,
              observations_whitelist=[observations_whitelist])
      ]
    else:
      env_wrappers = []
    environment = suite_dm_control.load(
        env_name, task_name, env_wrappers=env_wrappers)

    tf_env = tf_py_environment.TFPyEnvironment(environment)
    eval_py_env = suite_dm_control.load(
        env_name, task_name, env_wrappers=env_wrappers)

    actor_net = actor_rnn_network.ActorRnnNetwork(
        tf_env.time_step_spec().observation,
        tf_env.action_spec(),
        input_fc_layer_params=actor_fc_layers,
        lstm_size=actor_lstm_size,
        output_fc_layer_params=actor_output_fc_layers)

    critic_net_input_specs = (tf_env.time_step_spec().observation,
                              tf_env.action_spec())

    critic_net = critic_rnn_network.CriticRnnNetwork(
        critic_net_input_specs,
        observation_fc_layer_params=critic_obs_fc_layers,
        action_fc_layer_params=critic_action_fc_layers,
        joint_fc_layer_params=critic_joint_fc_layers,
        lstm_size=critic_lstm_size,
        output_fc_layer_params=critic_output_fc_layers,
    )

    global_step = tf.compat.v1.train.get_or_create_global_step()
    tf_agent = ddpg_agent.DdpgAgent(
        tf_env.time_step_spec(),
        tf_env.action_spec(),
        actor_network=actor_net,
        critic_network=critic_net,
        actor_optimizer=tf.compat.v1.train.AdamOptimizer(
            learning_rate=actor_learning_rate),
        critic_optimizer=tf.compat.v1.train.AdamOptimizer(
            learning_rate=critic_learning_rate),
        ou_stddev=ou_stddev,
        ou_damping=ou_damping,
        target_update_tau=target_update_tau,
        target_update_period=target_update_period,
        dqda_clipping=dqda_clipping,
        gamma=gamma,
        reward_scale_factor=reward_scale_factor,
        gradient_clipping=gradient_clipping,
        debug_summaries=debug_summaries,
        summarize_grads_and_vars=summarize_grads_and_vars,
        train_step_counter=global_step)

    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        tf_agent.collect_data_spec,
        batch_size=tf_env.batch_size,
        max_length=replay_buffer_capacity)

    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)

    train_metrics = [
        tf_metrics.NumberOfEpisodes(),
        tf_metrics.EnvironmentSteps(),
        tf_metrics.AverageReturnMetric(),
        tf_metrics.AverageEpisodeLengthMetric(),
    ]

    collect_policy = tf_agent.collect_policy

    initial_collect_op = dynamic_episode_driver.DynamicEpisodeDriver(
        tf_env,
        collect_policy,
        observers=[replay_buffer.add_batch] + train_metrics,
        num_episodes=initial_collect_steps).run()

    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(
        tf_env,
        collect_policy,
        observers=[replay_buffer.add_batch] + train_metrics,
        num_episodes=collect_episodes_per_iteration).run()

    &#47&#47 Need extra step to generate transitions of train_sequence_length.
    &#47&#47 Dataset generates trajectories with shape [BxTx...]
    dataset = replay_buffer.as_dataset(
        num_parallel_calls=3,
        sample_batch_size=batch_size,
        num_steps=train_sequence_length + 1).prefetch(3)

    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
    trajectories, unused_info = iterator.get_next()
    train_op = tf_agent.train(experience=trajectories)

    train_checkpointer = common_utils.Checkpointer(
        ckpt_dir=train_dir,
        agent=tf_agent,
        global_step=global_step,
        metrics=metric_utils.MetricsGroup(train_metrics, &quottrain_metrics&quot))
    policy_checkpointer = common_utils.Checkpointer(
        ckpt_dir=os.path.join(train_dir, &quotpolicy&quot),
        policy=tf_agent.policy,
        global_step=global_step)
    rb_checkpointer = common_utils.Checkpointer(
        ckpt_dir=os.path.join(train_dir, &quotreplay_buffer&quot),
        max_to_keep=1,
        replay_buffer=replay_buffer)

    for train_metric in train_metrics:
      train_metric.tf_summaries(step_metrics=train_metrics[:2])
    summary_op = tf.contrib.summary.all_summary_ops()

    with eval_summary_writer.as_default(), \
         <a id="change">tf.contrib.summary.always_record_summaries()</a>:
      for eval_metric in eval_metrics:
        eval_metric.tf_summaries()
</code></pre><h3>After Change</h3><pre><code class='java'>
  ]

  global_step = tf.compat.v1.train.get_or_create_global_step()
  with <a id="change">tf.compat.v2.summary.record_if(
      lambda: tf.math.equal(global_step % summary_interval, 0))</a>:
    if observations_whitelist is not None:
      env_wrappers = [
          functools.partial(
              wrappers.FlattenObservationsWrapper,
              observations_whitelist=[observations_whitelist])
      ]
    else:
      env_wrappers = []
    environment = suite_dm_control.load(
        env_name, task_name, env_wrappers=env_wrappers)

    tf_env = tf_py_environment.TFPyEnvironment(environment)
    eval_py_env = suite_dm_control.load(
        env_name, task_name, env_wrappers=env_wrappers)

    actor_net = actor_rnn_network.ActorRnnNetwork(
        tf_env.time_step_spec().observation,
        tf_env.action_spec(),
        input_fc_layer_params=actor_fc_layers,
        lstm_size=actor_lstm_size,
        output_fc_layer_params=actor_output_fc_layers)

    critic_net_input_specs = (tf_env.time_step_spec().observation,
                              tf_env.action_spec())

    critic_net = critic_rnn_network.CriticRnnNetwork(
        critic_net_input_specs,
        observation_fc_layer_params=critic_obs_fc_layers,
        action_fc_layer_params=critic_action_fc_layers,
        joint_fc_layer_params=critic_joint_fc_layers,
        lstm_size=critic_lstm_size,
        output_fc_layer_params=critic_output_fc_layers,
    )

    tf_agent = ddpg_agent.DdpgAgent(
        tf_env.time_step_spec(),
        tf_env.action_spec(),
        actor_network=actor_net,
        critic_network=critic_net,
        actor_optimizer=tf.compat.v1.train.AdamOptimizer(
            learning_rate=actor_learning_rate),
        critic_optimizer=tf.compat.v1.train.AdamOptimizer(
            learning_rate=critic_learning_rate),
        ou_stddev=ou_stddev,
        ou_damping=ou_damping,
        target_update_tau=target_update_tau,
        target_update_period=target_update_period,
        dqda_clipping=dqda_clipping,
        gamma=gamma,
        reward_scale_factor=reward_scale_factor,
        gradient_clipping=gradient_clipping,
        debug_summaries=debug_summaries,
        summarize_grads_and_vars=summarize_grads_and_vars,
        train_step_counter=global_step)

    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        tf_agent.collect_data_spec,
        batch_size=tf_env.batch_size,
        max_length=replay_buffer_capacity)

    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)

    train_metrics = [
        tf_metrics.NumberOfEpisodes(),
        tf_metrics.EnvironmentSteps(),
        tf_metrics.AverageReturnMetric(),
        tf_metrics.AverageEpisodeLengthMetric(),
    ]

    collect_policy = tf_agent.collect_policy

    initial_collect_op = dynamic_episode_driver.DynamicEpisodeDriver(
        tf_env,
        collect_policy,
        observers=[replay_buffer.add_batch] + train_metrics,
        num_episodes=initial_collect_steps).run()

    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(
        tf_env,
        collect_policy,
        observers=[replay_buffer.add_batch] + train_metrics,
        num_episodes=collect_episodes_per_iteration).run()

    &#47&#47 Need extra step to generate transitions of train_sequence_length.
    &#47&#47 Dataset generates trajectories with shape [BxTx...]
    dataset = replay_buffer.as_dataset(
        num_parallel_calls=3,
        sample_batch_size=batch_size,
        num_steps=train_sequence_length + 1).prefetch(3)

    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
    trajectories, unused_info = iterator.get_next()
    train_op = tf_agent.train(experience=trajectories)

    train_checkpointer = common_utils.Checkpointer(
        ckpt_dir=train_dir,
        agent=tf_agent,
        global_step=global_step,
        metrics=metric_utils.MetricsGroup(train_metrics, &quottrain_metrics&quot))
    policy_checkpointer = common_utils.Checkpointer(
        ckpt_dir=os.path.join(train_dir, &quotpolicy&quot),
        policy=tf_agent.policy,
        global_step=global_step)
    rb_checkpointer = common_utils.Checkpointer(
        ckpt_dir=os.path.join(train_dir, &quotreplay_buffer&quot),
        max_to_keep=1,
        replay_buffer=replay_buffer)

    for train_metric in train_metrics:
      train_metric.tf_summaries(step_metrics=train_metrics[:2])
    summary_op = tf.contrib.summary.all_summary_ops()

    with eval_summary_writer.as_default(), \
         <a id="change">tf.compat.v2.summary.record_if(True)</a>:
      for eval_metric in eval_metrics:
        eval_metric.tf_summaries()
</code></pre><img src="203193948.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 10</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-8867b01c45db01e9003c38bc644e9121f637f6ff67263d094d84a366457fefc2L128' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/ddpg/examples/train_eval_rnn_dm.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-498f435a8c11bc81ad0ef101b5289f3e34de09718769a8ffc816f14186b0a71fL126' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/td3/examples/train_eval_rnn_dm.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-bf929dada108dbc7a68092025c4b6fae19b2f881ee333793ca769dd5bfb1783aL202' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/dqn/examples/train_eval_atari.py</div><div id='class'> Class Name: TrainEval</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-1c8a9f9db8afc3eb9fe330b217231ab4f831c9d01fff17787498168ef13184c3L144' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/ppo/examples/train_eval.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-450cd3786535cd66a7e35baa060ea773f92865c99aa81bd185c8278f6c2786a5L114' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/dqn/examples/train_eval_gym.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-ee3578d08aa207475c7de0def99f1eb0041ee4b8c773fad098f87b2f085f0b90L135' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/sac/examples/train_eval_mujoco.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-8867b01c45db01e9003c38bc644e9121f637f6ff67263d094d84a366457fefc2L128' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/ddpg/examples/train_eval_rnn_dm.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-a134a717da306deabd26fd511ecdcd976ffdde8c09a6b0147b807f1a6be610f5L101' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/reinforce/examples/train_eval_gym.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-7b5ef6231f6b1f96fa6f71281bc29a11e586111ab3f4ab00e6ae878e6b2301c7L121' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/td3/examples/train_eval_mujoco.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-363333daf3eaa814e0f4405e0795dcfd2347f6b53533c6fbbb8d77780017ffb5L115' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/dqn/examples/train_eval_rnn_gym.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/agents/commit/f72a9854c23f9101e75cf7c13fbf8232128a896a#diff-af3db8b74fc5b0a60e00f52b75c7aa9d20ae68037e48c1c7fff5ae71858430f1L123' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/agents</div><div id='commit'> Commit Name: f72a9854c23f9101e75cf7c13fbf8232128a896a</div><div id='time'> Time: 2019-02-27</div><div id='author'> Author: gjn@google.com</div><div id='file'> File Name: tf_agents/agents/ddpg/examples/train_eval_mujoco.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_eval</div><BR>