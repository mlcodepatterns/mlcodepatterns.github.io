<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        super(TransformerEncoder, self).__init__()
        self.d_model = d_model
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        <a id="change">self.ffn = FFN(d_model, pdrop, d_ff=d_ff, activation_type=activation_type)</a>
        self.ln1 = LayerNorm(d_model)
        self.ln2 = LayerNorm(d_model)
        self.dropout = nn.Dropout(pdrop)
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.d_model = d_model
        self.d_ff = d_ff if d_ff is not None else 4 * d_model
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        <a id="change">self.ffn = nn.Sequential(pytorch_linear(self.d_model, self.d_ff),
                                 pytorch_activation(activation_type),
                                 pytorch_linear(self.d_ff, self.d_model))</a>
        self.ln1 = <a id="change">nn.LayerNorm(self.d_model, eps=1e-12)</a>
        self.ln2 = <a id="change">nn.LayerNorm(self.d_model, eps=1e-12)</a>
        self.dropout = nn.Dropout(pdrop)

    def forward(self, x, mask=None):
        </code></pre>