<html><h3>69b5f880966e0f1290e47eb29b2f43674a5b49b0,art/attacks/deepfool.py,DeepFool,generate,#DeepFool#Any#,56
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        :rtype: `np.ndarray`
        
        self.set_params(**kwargs)
        <a id="change">clip_min</a>, <a id="change">clip_max</a> = self.classifier.clip_values
        x_adv = x.copy()
        preds = self.classifier.predict(x, logits=True)

        &#47&#47 Determine the class labels for which to compute the gradients
        use_grads_subset = self.nb_grads &lt; self.classifier.nb_classes
        if use_grads_subset:
            &#47&#47 TODO compute set of unique labels per batch
            grad_labels = np.argsort(-preds, axis=1)[:, :self.nb_grads]
            labels_set = np.unique(grad_labels)
        else:
            labels_set = np.arange(self.classifier.nb_classes)
        sorter = np.arange(len(labels_set))

        &#47&#47 Pick a small scalar to avoid division by 0
        tol = 10e-8

        &#47&#47 Compute perturbation with implicit batching
        for batch_id in range(int(np.ceil(x_adv.shape[0] / float(self.batch_size)))):
            batch_index_1, batch_index_2 = batch_id * self.batch_size, (batch_id + 1) * self.batch_size
            batch = x_adv[batch_index_1:batch_index_2]

            &#47&#47 Get predictions and gradients for batch
            f = preds[batch_index_1:batch_index_2]
            fk_hat = np.argmax(f, axis=1)
            if use_grads_subset:
                &#47&#47 Compute gradients only for top predicted classes
                grd = np.array([self.classifier.class_gradient(batch, logits=True, label=_) for _ in labels_set])
                grd = np.squeeze(np.swapaxes(grd, 0, 2), axis=0)
            else:
                &#47&#47 Compute gradients for all classes
                grd = self.classifier.class_gradient(batch, logits=True)

            &#47&#47 Get current predictions
            active_indices = np.arange(len(batch))
            current_step = 0
            while len(active_indices) != 0 and current_step &lt; self.max_iter:
                &#47&#47 Compute difference in predictions and gradients only for selected top predictions
                labels_indices = sorter[np.searchsorted(labels_set, fk_hat, sorter=sorter)]
                grad_diff = grd - grd[np.arange(len(grd)), labels_indices][:, None]
                f_diff = f[:, labels_set] - f[np.arange(len(f)), labels_indices][:, None]

                &#47&#47 Choose coordinate and compute perturbation
                norm = np.linalg.norm(grad_diff.reshape(len(grad_diff), len(labels_set), -1), axis=2) + tol
                value = np.abs(f_diff) / norm
                value[np.arange(len(value)), labels_indices] = np.inf
                l = np.argmin(value, axis=1)
                r = (abs(f_diff[np.arange(len(f_diff)), l]) / (pow(np.linalg.norm(grad_diff[np.arange(len(
                    grad_diff)), l].reshape(len(grad_diff), -1), axis=1), 2) + tol))[:, None, None, None] * \
                    grad_diff[np.arange(len(grad_diff)), l]

                &#47&#47 Add perturbation and clip result
                batch[active_indices] = np.clip(batch[active_indices] + r[active_indices], clip_min, clip_max)

                &#47&#47 Recompute prediction for new x
                f = self.classifier.predict(batch, logits=True)
                fk_i_hat = np.argmax(f, axis=1)

                &#47&#47 Recompute gradients for new x
                if use_grads_subset:
                    &#47&#47 Compute gradients only for (originally) top predicted classes
                    grd = np.array([self.classifier.class_gradient(batch, logits=True, label=_) for _ in labels_set])
                    grd = np.squeeze(np.swapaxes(grd, 0, 2), axis=0)
                else:
                    &#47&#47 Compute gradients for all classes
                    grd = self.classifier.class_gradient(batch, logits=True)

                &#47&#47 Stop if misclassification has been achieved
                active_indices = np.where(fk_i_hat == fk_hat)[0]

                current_step += 1

            &#47&#47 Apply overshoot parameter
            x_adv[batch_index_1:batch_index_2] = <a id="change">np.clip(x_adv[batch_index_1:batch_index_2] + (
                1 + self.epsilon) * (batch - x_adv[batch_index_1:batch_index_2]), clip_min, clip_max)</a>

        logger.info(&quotSuccess rate of DeepFool attack: %.2f%%&quot,
                    (np.sum(np.argmax(preds, axis=1) != np.argmax(self.classifier.predict(x_adv), axis=1)) /
                     x.shape[0]))</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 Apply overshoot parameter
            x_adv[batch_index_1:batch_index_2] = x_adv[batch_index_1:batch_index_2] + \
                (1 + self.epsilon) * (batch - x_adv[batch_index_1:batch_index_2])
            <a id="change">if hasattr(self.classifier, &quotclip_values&quot) and self.classifier.clip_values is not None:
                np.clip(x_adv[batch_index_1:batch_index_2], self.classifier.clip_values[0],
                        self.classifier.clip_values[1], out=x_adv[batch_index_1:batch_index_2])

       </a> logger.info(&quotSuccess rate of DeepFool attack: %.2f%%&quot,
                    (np.sum(np.argmax(preds, axis=1) != np.argmax(self.classifier.predict(x_adv), axis=1)) /
                     x.shape[0]))
</code></pre><img src="82068580.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 16</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/69b5f880966e0f1290e47eb29b2f43674a5b49b0#diff-8af1ecbe48ae0247d5bd969c0c84d97c1f38b5b121709d829bed824d7df42c8eL56' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 69b5f880966e0f1290e47eb29b2f43674a5b49b0</div><div id='time'> Time: 2019-04-30</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: art/attacks/deepfool.py</div><div id='class'> Class Name: DeepFool</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/69b5f880966e0f1290e47eb29b2f43674a5b49b0#diff-8af1ecbe48ae0247d5bd969c0c84d97c1f38b5b121709d829bed824d7df42c8eL56' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 69b5f880966e0f1290e47eb29b2f43674a5b49b0</div><div id='time'> Time: 2019-04-30</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: art/attacks/deepfool.py</div><div id='class'> Class Name: DeepFool</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/69b5f880966e0f1290e47eb29b2f43674a5b49b0#diff-13cd60163962c46977af5b34f31c47f1aa0e2b91becd8348fa3cc8b2e5a43989L157' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 69b5f880966e0f1290e47eb29b2f43674a5b49b0</div><div id='time'> Time: 2019-04-30</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: art/attacks/elastic_net.py</div><div id='class'> Class Name: ElasticNet</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/69b5f880966e0f1290e47eb29b2f43674a5b49b0#diff-dd9f970bf67a9705ff1cc8fc31ab9a4264fede938c045125456cbdda931a93f5L151' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 69b5f880966e0f1290e47eb29b2f43674a5b49b0</div><div id='time'> Time: 2019-04-30</div><div id='author'> Author: Maria-Irina.Nicolae@ibm.com</div><div id='file'> File Name: art/attacks/boundary.py</div><div id='class'> Class Name: BoundaryAttack</div><div id='method'> Method Name: _attack</div><BR>