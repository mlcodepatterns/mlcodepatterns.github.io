<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    observations = (np.array([[1, 2], [3, 4]], dtype=np.float32),
                    np.array([[1, 0, 0], [1, 1, 0]], dtype=np.int32))
    actions = np.array([0, 1], dtype=np.int32)
    rewards = <a id="change">np.array([[0.5, 6.0], [3.0, 4.0]], dtype=np.float32)</a>
    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(
        observations, rewards)
    action_step = _get_action_step(actions)
    experience = _get_experience(initial_step, action_step, final_step)</code></pre><h3>After Change</h3><pre><code class='java'>
  def testTrainAgentWithMaskAndConstraint(self):
    reward_net = DummyNet(self._observation_spec, self._action_spec)
    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)
    reward_spec = <a id="change">{
        &quotreward&quot: tensor_spec.TensorSpec(
            shape=(), dtype=tf.float32, name=&quotreward&quot),
        &quotconstraint&quot: tensor_spec.TensorSpec(
            shape=(), dtype=tf.float32, name=&quotconstraint&quot)
    }</a>
    observation_and_mask_spec = (tensor_spec.TensorSpec([2], tf.float32),
                                 tensor_spec.TensorSpec([3], tf.int32))
    time_step_spec = ts.time_step_spec(observation_and_mask_spec, reward_spec)

    constraint_net = DummyNet(self._observation_spec, self._action_spec)
    neural_constraint = constraints.NeuralConstraint(
        self._time_step_spec,
        self._action_spec,
        constraint_network=constraint_net)

    agent = greedy_agent.GreedyRewardPredictionAgent(
        time_step_spec,
        self._action_spec,
        reward_network=reward_net,
        optimizer=optimizer,
        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),
        constraints=[neural_constraint])
    observations = (np.array([[1, 2], [3, 4]], dtype=np.float32),
                    np.array([[1, 0, 0], [1, 1, 0]], dtype=np.int32))
    actions = np.array([0, 1], dtype=np.int32)
    rewards = <a id="change">{
        &quotreward&quot: np.array([0.5, 3.0], dtype=np.float32),
        &quotconstraint&quot: np.array([6.0, 4.0], dtype=np.float32)
    }</a>
    initial_step, final_step = (
        _get_initial_and_final_steps_action_mask_nested_rewards(
            observations, rewards))
    action_step = _get_action_step(actions)</code></pre>