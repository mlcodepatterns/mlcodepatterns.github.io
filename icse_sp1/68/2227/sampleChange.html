<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                      r=lr)

        vis = self.generate_visible_space()
        for ep in <a id="change">progress_bar(range(epochs + 1), desc="Epochs ",
                               total=epochs, disable=disable_progbar)</a>:
            
            random_permutation = torch.randperm(data.shape[0])

            shuffled_data           = data[random_permutation]   
            shuffled_character_data = character_data[random_permutation]

            batches = [shuffled_data[batch_start:(batch_start + batch_size)] 
                       for batch_start in range(0, len(data), batch_size)]

            char_batches = [shuffled_character_data[batch_start:(batch_start + batch_size)] 
                            for batch_start in range(0, len(data), batch_size)]

            if ep % log_every == 0:
                <a id="change">logZ = self.log_partition(vis)</a>
                <a id="change">nll = self.nll(data, logZ)</a>
                <a id="change">tqdm.write("{}: {}".format(ep, nll.item() / len(data)))</a>

            <a id="change">if ep == epochs:
                break

           </a> stddev = torch.tensor(
                [initial_gaussian_noise / ((1 + ep) ** gamma)],
                dtype=torch.double, device=self.device).sqrt()

            for batch in <a id="change">progress_bar(batches, desc="Batches",
                                      leave=False, disable=True)</a>:

                grads = self.compute_batch_gradients(k, batch, char_batches,
                                                     l1_reg, l2_reg,</code></pre><h3>After Change</h3><pre><code class='java'>

        vis = self.generate_visible_space()
        print (&quotGenerated visible space. Ready to begin training.&quot)
        <a id="change">fidelity_list = []</a>
        <a id="change">epoch_list = []</a>

        for ep in <a id="change">range(epochs+1)</a>:
            
            random_permutation = torch.randperm(data.shape[0])

            shuffled_data           = data[random_permutation]   
            shuffled_character_data = character_data[random_permutation]

            <a id="change">batches</a> = [shuffled_data[batch_start:(batch_start + batch_size)] 
                       for batch_start in range(0, len(data), batch_size)]

            char_batches = [shuffled_character_data[batch_start:(batch_start + batch_size)] 
                            for batch_start in range(0, len(data), batch_size)]

            if ep % log_every == 0:
                &#47&#47logZ = self.log_partition(vis)
                &#47&#47nll = self.nll(data, logZ)
                <a id="change">fidelity_ = self.fidelity(vis)</a>
                <a id="change">print (&quotEpoch = &quot,ep,&quot\nFidelity = &quot,fidelity_)</a>
                <a id="change">fidelity_list.append(fidelity_)</a>
                epoch_list.append(ep)

            <a id="change">if ep == epochs:
                fidelity_file = open(&quotfidelity_file.txt&quot, &quotw&quot)
                print (&quotFinished training. Saving results...&quot )               
                for i in range(len(fidelity_list)):
                    fidelity_file.write(&quot%.5f&quot % fidelity_list[i] + &quot %d\n&quot % epoch_list[i])
                break

           </a> stddev = torch.tensor(
                [initial_gaussian_noise / ((1 + ep) ** gamma)],
                dtype=torch.double, device=self.device).sqrt()

            for batch_index in <a id="change">range(len(batches))</a>:

                grads = self.compute_batch_gradients(k, <a id="change">batches[batch_index]</a>, <a id="change">char_batches[batch_index]</a>,
                                                     l1_reg, l2_reg,
                                                     stddev=stddev)
</code></pre>