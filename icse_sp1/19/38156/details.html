<html><h3>171e9e18a10f2daea090bc6f4815db41072d66b6,ch10/03_pong_a2c_rollouts.py,,,#,115
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    writer = SummaryWriter(comment="-pong-a2c-rollouts_" + args.name)
    set_seed(20, envs, cuda=args.cuda)

    <a id="change">net</a> = <a id="change">AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n)</a>
    <a id="change">if args.cuda:
        net.cuda()
   </a> print(net)

    agent = ptan.agent.ActorCriticAgent(net, apply_softmax=True, cuda=<a id="change">args</a>.cuda)
    exp_source = ptan.experience.ExperienceSourceRollouts(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    <a id="change">optimizer</a> = optim.RMSprop(net.parameters(), lr=LEARNING_RATE, eps=1e-5)

    step_idx = 0

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:
            for mb_states, mb_rewards, mb_actions, mb_values in exp_source:
                &#47&#47 handle new rewards
                new_rewards = exp_source.pop_total_rewards()
                if new_rewards:
                    if tracker.reward(np.mean(new_rewards), step_idx):
                        break

                optimizer.zero_grad()
                states_v = Variable(torch.from_numpy(mb_states))
                mb_adv = mb_rewards - mb_values
                adv_v = Variable(torch.from_numpy(mb_adv))
                actions_t = torch.from_numpy(mb_actions)
                vals_ref_v = Variable(torch.from_numpy(mb_rewards))
                if args.cuda:
                    states_v = states_v.cuda()
                    adv_v = adv_v.cuda()
                    actions_t = actions_t.cuda()
                    vals_ref_v = vals_ref_v.cuda()

                logits_v, value_v = net(states_v)
                loss_value_v = F.mse_loss(value_v, vals_ref_v)

                log_prob_v = F.log_softmax(logits_v, dim=1)
                <a id="change">log_prob_actions_v</a> = adv_v * log_prob_v[range(len(mb_states)), actions_t]
                loss_policy_v = -log_prob_actions_v.mean()

                prob_v = F.softmax(logits_v, dim=1)</code></pre><h3>After Change</h3><pre><code class='java'>
    writer = SummaryWriter(comment="-pong-a2c-rollouts_" + args.name)
    set_seed(20, envs, cuda=args.cuda)

    <a id="change">net</a> = <a id="change">AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)</a>
    print(net)

    agent = ptan.agent.ActorCriticAgent(net, apply_softmax=True, device=device)
    exp_source = ptan.experience.ExperienceSourceRollouts(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    <a id="change">optimizer</a> = optim.RMSprop(net.parameters(), lr=LEARNING_RATE, eps=1e-5)

    step_idx = 0

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:
            for mb_states, mb_rewards, mb_actions, mb_values in exp_source:
                &#47&#47 handle new rewards
                new_rewards = exp_source.pop_total_rewards()
                if new_rewards:
                    if tracker.reward(np.mean(new_rewards), step_idx):
                        break

                optimizer.zero_grad()
                states_v = torch.FloatTensor(mb_states).to(device)
                mb_adv = mb_rewards - mb_values
                adv_v = torch.FloatTensor(mb_adv).to(<a id="change">device</a>)
                actions_t = torch.LongTensor(mb_actions).to(<a id="change">device</a>)
                vals_ref_v = torch.FloatTensor(mb_rewards).to(<a id="change">device</a>)

                logits_v, value_v = net(states_v)
                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)

                log_prob_v = F.log_softmax(logits_v, dim=1)
                <a id="change">log_prob_actions_v</a> = adv_v * log_prob_v[range(len(mb_states)), actions_t]
                loss_policy_v = -log_prob_actions_v.mean()

                prob_v = F.softmax(logits_v, dim=1)</code></pre><img src="185079049.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 19</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/171e9e18a10f2daea090bc6f4815db41072d66b6#diff-3127b6eeb9f67b6225c6a2477817085516f6694a6a98b4f484633768f899b193L111' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 171e9e18a10f2daea090bc6f4815db41072d66b6</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch10/03_pong_a2c_rollouts.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/9e80c11073af48db2876fc943df9264a7ab0488e#diff-9a4e572a49b9dd490b46b26d6396a6965dc0b15d346ddd70c1c0546bf31e1959L60' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 9e80c11073af48db2876fc943df9264a7ab0488e</div><div id='time'> Time: 2018-04-29</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch11/01_a3c_data.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/171e9e18a10f2daea090bc6f4815db41072d66b6#diff-3cdbf42131fee9c9b0232f8ded7f4c62d779508cfde08f065c1470d18f179c79L105' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 171e9e18a10f2daea090bc6f4815db41072d66b6</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch10/02_pong_a2c.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>