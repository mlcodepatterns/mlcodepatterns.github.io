<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        filtered_max_q_vals = next_q_values * not_done_mask.float()

        <a id="change">if self.minibatch &lt; self.reward_burnin:
            target_q_values = reward
        else:
            target_q_values = reward + (discount_tensor * filtered_max_q_vals)

        &#47&#47 Get Q-value of action taken
       </a> current_state_action = rlt.StateAction(
            state=learning_input.state, action=learning_input.action
        )
        q_values = self.q_network(current_state_action).q_value
        self.all_action_scores = q_values.detach()

        value_loss = self.q_network_loss(q_values, target_q_values)
        self.loss = value_loss.detach()

        self.q_network_optimizer.zero_grad()
        value_loss.backward()
        self.q_network_optimizer.step()

        &#47&#47 TODO: Maybe soft_update should belong to the target network
        <a id="change">if self.minibatch &lt; self.reward_burnin:
            &#47&#47 Reward burnin: force target network
            self._soft_update(self.q_network, self.q_network_target, 1.0)
        else:
            &#47&#47 Use the soft update rule to update target network
            self._soft_update(self.q_network, self.q_network_target, self.tau)

        &#47&#47 get reward estimates
       </a> reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)
        self.reward_network_optimizer.zero_grad()
        reward_loss.backward()</code></pre><h3>After Change</h3><pre><code class='java'>
        self.q_network_optimizer.step()

        &#47&#47 Use the soft update rule to update target network
        <a id="change">self._soft_update(self.q_network, self.q_network_target, self.tau)</a>

        &#47&#47 get reward estimates
        reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)</code></pre>