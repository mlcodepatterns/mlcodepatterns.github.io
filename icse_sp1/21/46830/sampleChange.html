<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
def calc_loss(batch, net, tgt_net, gamma, cuda=False, double=True):
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)

    states_v = <a id="change">Variable(torch.from_numpy(states))</a>
    next_states_v = Variable(<a id="change">torch.from_numpy(next_states)</a>, volatile=True)
    actions_v = <a id="change">Variable(torch.from_numpy(actions))</a>
    rewards_v = <a id="change">Variable(torch.from_numpy(rewards))</a>
    done_mask = torch.ByteTensor(dones)
    <a id="change">if cuda:
        states_v = states_v.cuda()
        next_states_v = next_states_v.cuda()
        actions_v = actions_v.cuda()
        rewards_v = rewards_v.cuda()
        done_mask = done_mask.cuda()

   </a> state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    if double:
        next_state_actions = net(next_states_v).max(1)[1]
        next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)
    else:
        next_state_values = tgt_net(next_states_v).max(1)[0]
    next_state_values[done_mask] = 0.0
    <a id="change">next_state_values.volatile = False</a>

    expected_state_action_values = next_state_values * gamma + rewards_v
    return nn.MSELoss()(state_action_values, expected_state_action_values)
</code></pre><h3>After Change</h3><pre><code class='java'>
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)

    states_v = torch.tensor(states).to(device)
    next_states_v = <a id="change">torch</a>.tensor(next_states).to(device)
    actions_v = torch.tensor(actions).to(device)
    rewards_v = torch.tensor(rewards).to(device)
    done_mask = torch.ByteTensor(dones).to(device)

    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    if double:
        next_state_actions = net(next_states_v).max(1)[1]
        next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)
    else:
        next_state_values = tgt_net(next_states_v).max(1)[0]
    next_state_values[done_mask] = 0.0

    expected_state_action_values = <a id="change">next_state_values.detach()</a> * gamma + rewards_v
    return nn.MSELoss()(state_action_values, expected_state_action_values)

</code></pre>