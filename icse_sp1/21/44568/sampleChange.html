<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if self._baseline:
      value_estimation_loss = self.value_estimation_loss(
          value_preds, returns, num_episodes, weights)
      <a id="change">total_loss += value_estimation_loss</a>

    <a id="change">with tf.name_scope(&quotLosses/&quot):
      tf.compat.v2.summary.scalar(
          name=&quotpolicy_gradient_loss&quot,
          data=policy_gradient_loss,
          step=self.train_step_counter)
      tf.compat.v2.summary.scalar(
          name=&quotentropy_regularization_loss&quot,
          data=entropy_regularization_loss,
          step=self.train_step_counter)
      if self._baseline:
        tf.compat.v2.summary.scalar(
            name=&quotvalue_estimation_loss&quot,
            data=value_estimation_loss,
            step=self.train_step_counter)
      tf.compat.v2.summary.scalar(
          name=&quottotal_loss&quot, data=total_loss, step=self.train_step_counter)

   </a> return tf_agent.LossInfo(total_loss, ())

  def policy_gradient_loss(self,
                           actions_distribution,</code></pre><h3>After Change</h3><pre><code class='java'>
    entropy_regularization_loss = self.entropy_regularization_loss(
        actions_distribution, weights)

    <a id="change">network_regularization_loss = tf.nn.scale_regularization_loss(
        self._actor_network.losses)</a>

    total_loss = (policy_gradient_loss +
                  network_regularization_loss +
                  entropy_regularization_loss)

    <a id="change">losses_dict = {
        &quotpolicy_gradient_loss&quot: policy_gradient_loss,
        &quotpolicy_network_regularization_loss&quot: network_regularization_loss,
        &quotentropy_regularization_loss&quot: entropy_regularization_loss,
        &quotvalue_estimation_loss&quot: 0.0,
        &quotvalue_network_regularization_loss&quot: 0.0,
    }</a>

    value_estimation_loss = None
    if self._baseline:
      value_estimation_loss = self.value_estimation_loss(
          value_preds, returns, num_episodes, weights)
      value_network_regularization_loss = tf.nn.scale_regularization_loss(
          self._value_network.losses)
      total_loss += value_estimation_loss + value_network_regularization_loss
      losses_dict[&quotvalue_estimation_loss&quot] = value_estimation_loss
      losses_dict[&quotvalue_network_regularization_loss&quot] = (
          value_network_regularization_loss)

    loss_info_extra = ReinforceAgentLossInfo._make(losses_dict)

    losses_dict[&quottotal_loss&quot] = total_loss  &#47&#47 Total loss not in loss_info_extra.

    <a id="change">common.summarize_scalar_dict(losses_dict,
                                 self.train_step_counter,
                                 name_scope=&quotLosses/&quot)</a>

    return tf_agent.LossInfo(total_loss, loss_info_extra)

  def policy_gradient_loss(self,</code></pre>