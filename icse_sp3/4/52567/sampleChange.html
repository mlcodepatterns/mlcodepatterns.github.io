<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    &#47&#47 Define cost function based on which one was selected via set_loss_function
                    if self._loss_fn == &quotsoftmax cross entropy&quot:
                        sf_logits = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=xx, labels=tf.argmax(y, 1))
                    self._graph_ops[&quotcost&quot] = <a id="change">tf.add(tf.reduce_mean(tf.concat([sf_logits], axis=0)), l2_cost)</a>

                    &#47&#47 For classification problems, we will compute the training accuracy as well; this is also used
                    &#47&#47 for Tensorboard
                    self.__class_predictions = tf.argmax(tf.nn.softmax(xx), 1)</code></pre><h3>After Change</h3><pre><code class='java'>
            self._add_layers_to_graph()

            &#47&#47 Do the forward pass and training output calcs on possibly multiple GPUs
            <a id="change">device_costs = []</a>
            device_accuracies = []
            device_gradients = []
            device_variables = []
            for n, d in enumerate(self._get_device_list()):  &#47&#47 Build a graph on either a CPU or all of the GPUs
                with tf.device(d), tf.name_scope(&quottower_&quot + str(n)):
                    &#47&#47 Run the network operations
                    if self._has_moderation:
                        xx = self.forward_pass(x, deterministic=False, moderation_features=mod_w)
                    else:
                        xx = self.forward_pass(x, deterministic=False)

                    &#47&#47 Define regularization cost
                    self._log(&quotGraph: Calculating loss and gradients...&quot)
                    if self._reg_coeff is not None:
                        l2_cost = tf.squeeze(tf.reduce_sum(
                            [layer.regularization_coefficient * tf.nn.l2_loss(layer.weights) for layer in self._layers
                             if isinstance(layer, layers.fullyConnectedLayer)]))
                    else:
                        l2_cost = 0.0

                    &#47&#47 Define cost function based on which one was selected via set_loss_function
                    if self._loss_fn == &quotsoftmax cross entropy&quot:
                        sf_logits = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=xx, labels=tf.argmax(y, 1))
                    gpu_cost = tf.reduce_mean(tf.concat([sf_logits], axis=0)) + l2_cost
                    cost_sum = tf.reduce_sum(tf.concat([sf_logits], axis=0))
                    device_costs.append(cost_sum)
                    &#47&#47 self._graph_ops[&quotcost&quot] = tf.add(tf.reduce_mean(tf.concat([sf_logits], axis=0)), l2_cost)

                    &#47&#47 For classification problems, we will compute the training accuracy as well; this is also used
                    &#47&#47 for Tensorboard
                    self.__class_predictions = tf.argmax(tf.nn.softmax(xx), 1)
                    correct_predictions = tf.equal(self.__class_predictions, tf.argmax(y, 1))
                    accuracy_sum = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))
                    device_accuracies.append(accuracy_sum)
                    &#47&#47 self._graph_ops[&quotaccuracy&quot] = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))

                    &#47&#47 Set the optimizer and get the gradients from it
                    gradients, variables, global_grad_norm = self._graph_get_gradients(gpu_cost, optimizer)
                    device_gradients.append(gradients)
                    device_variables.append(variables)

            &#47&#47 Average the gradients from each GPU and apply them
            average_gradients = self._graph_average_gradients(device_gradients)
            opt_variables = device_variables[0]
            self._graph_ops[&quotoptimizer&quot] = self._graph_apply_gradients(average_gradients, opt_variables, optimizer)

            &#47&#47 Average the costs and accuracies from each GPU
            self._graph_ops[&quotcost&quot] = <a id="change">tf.reduce_sum(device_costs)</a> / self._batch_size + l2_cost
            self._graph_ops[&quotaccuracy&quot] = tf.reduce_sum(device_accuracies) / self._batch_size

            &#47&#47 Calculate test and validation accuracy (on a single device)</code></pre>