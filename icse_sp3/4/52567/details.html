<html><h3>15ef5d3b46eda15b4de4d8b008a13836c82bf149,deepplantphenomics/classification_model.py,ClassificationModel,_assemble_graph,#ClassificationModel#,44
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    &#47&#47 Define cost function based on which one was selected via set_loss_function
                    if self._loss_fn == &quotsoftmax cross entropy&quot:
                        sf_logits = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=xx, labels=tf.argmax(y, 1))
                    self._graph_ops[&quotcost&quot] = <a id="change">tf.add(tf.reduce_mean(tf.concat([sf_logits], axis=0)), l2_cost)</a>

                    &#47&#47 For classification problems, we will compute the training accuracy as well; this is also used
                    &#47&#47 for Tensorboard
                    self.__class_predictions = tf.argmax(tf.nn.softmax(xx), 1)</code></pre><h3>After Change</h3><pre><code class='java'>
            self._add_layers_to_graph()

            &#47&#47 Do the forward pass and training output calcs on possibly multiple GPUs
            <a id="change">device_costs = []</a>
            device_accuracies = []
            device_gradients = []
            device_variables = []
            for n, d in enumerate(self._get_device_list()):  &#47&#47 Build a graph on either a CPU or all of the GPUs
                with tf.device(d), tf.name_scope(&quottower_&quot + str(n)):
                    &#47&#47 Run the network operations
                    if self._has_moderation:
                        xx = self.forward_pass(x, deterministic=False, moderation_features=mod_w)
                    else:
                        xx = self.forward_pass(x, deterministic=False)

                    &#47&#47 Define regularization cost
                    self._log(&quotGraph: Calculating loss and gradients...&quot)
                    if self._reg_coeff is not None:
                        l2_cost = tf.squeeze(tf.reduce_sum(
                            [layer.regularization_coefficient * tf.nn.l2_loss(layer.weights) for layer in self._layers
                             if isinstance(layer, layers.fullyConnectedLayer)]))
                    else:
                        l2_cost = 0.0

                    &#47&#47 Define cost function based on which one was selected via set_loss_function
                    if self._loss_fn == &quotsoftmax cross entropy&quot:
                        sf_logits = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=xx, labels=tf.argmax(y, 1))
                    gpu_cost = tf.reduce_mean(tf.concat([sf_logits], axis=0)) + l2_cost
                    cost_sum = tf.reduce_sum(tf.concat([sf_logits], axis=0))
                    device_costs.append(cost_sum)
                    &#47&#47 self._graph_ops[&quotcost&quot] = tf.add(tf.reduce_mean(tf.concat([sf_logits], axis=0)), l2_cost)

                    &#47&#47 For classification problems, we will compute the training accuracy as well; this is also used
                    &#47&#47 for Tensorboard
                    self.__class_predictions = tf.argmax(tf.nn.softmax(xx), 1)
                    correct_predictions = tf.equal(self.__class_predictions, tf.argmax(y, 1))
                    accuracy_sum = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))
                    device_accuracies.append(accuracy_sum)
                    &#47&#47 self._graph_ops[&quotaccuracy&quot] = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))

                    &#47&#47 Set the optimizer and get the gradients from it
                    gradients, variables, global_grad_norm = self._graph_get_gradients(gpu_cost, optimizer)
                    device_gradients.append(gradients)
                    device_variables.append(variables)

            &#47&#47 Average the gradients from each GPU and apply them
            average_gradients = self._graph_average_gradients(device_gradients)
            opt_variables = device_variables[0]
            self._graph_ops[&quotoptimizer&quot] = self._graph_apply_gradients(average_gradients, opt_variables, optimizer)

            &#47&#47 Average the costs and accuracies from each GPU
            self._graph_ops[&quotcost&quot] = <a id="change">tf.reduce_sum(device_costs)</a> / self._batch_size + l2_cost
            self._graph_ops[&quotaccuracy&quot] = tf.reduce_sum(device_accuracies) / self._batch_size

            &#47&#47 Calculate test and validation accuracy (on a single device)</code></pre><img src="244404336.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/p2irc/deepplantphenomics/commit/15ef5d3b46eda15b4de4d8b008a13836c82bf149#diff-cdf034591d4b69b4f1802121b1646e8091b8de36c0ec2b4b9c01575996cd8db6L44' target='_blank'>Link</a></div><div id='project'> Project Name: p2irc/deepplantphenomics</div><div id='commit'> Commit Name: 15ef5d3b46eda15b4de4d8b008a13836c82bf149</div><div id='time'> Time: 2019-09-13</div><div id='author'> Author: dbl599@mail.usask.ca</div><div id='file'> File Name: deepplantphenomics/classification_model.py</div><div id='class'> Class Name: ClassificationModel</div><div id='method'> Method Name: _assemble_graph</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/7d9d506e77585e5600b45dc41da1a731a4b30722#diff-623cca10c4dcb5767a0bcf91b3d96bd97d705e4320e0f8fdb22922f809d5fc44L86' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: 7d9d506e77585e5600b45dc41da1a731a4b30722</div><div id='time'> Time: 2017-04-27</div><div id='author'> Author: z.eaton-rosen@ucl.ac.uk</div><div id='file'> File Name: nn/loss.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: sensitivity_specificity_loss</div><BR><BR><div id='link'><a href='https://github.com/jakeret/tf_unet/commit/67bd0ba416366c2b31006a5bf951ae9586135f7c#diff-2d4cf6811042fbb78628447ef934a4a24f24a39a01df98fdc7223c9c3b850d9fL62' target='_blank'>Link</a></div><div id='project'> Project Name: jakeret/tf_unet</div><div id='commit'> Commit Name: 67bd0ba416366c2b31006a5bf951ae9586135f7c</div><div id='time'> Time: 2018-06-25</div><div id='author'> Author: joel.akeret@gmail.com</div><div id='file'> File Name: tf_unet/layers.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pixel_wise_softmax</div><BR>