<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            trace_level=tf.RunOptions.FULL_TRACE
        )

    <a id="change">if is_chief:
        &#47&#47 Load pretrained weights needs to be called before defining the train
        &#47&#47 op. After it, variables for the optimizer are created.
        with tf.control_dependencies([tf.global_variables_initializer()]):
            with tf.control_dependencies([model.load_pretrained_weights()]):
                init_op = tf.no_op(name=&quotglobal_init_load_pretrained&quot)
    else:
        init_op = tf.no_op()

    &#47&#47 Create custom Scaffold to make sure we run our own init_op when model
    &#47&#47 is not restored from checkpoint.
   </a> summary_op = [model.summary]
    summaries = tf.summary.merge_all()
    if summaries is not None:
        summary_op.append(summaries)</code></pre><h3>After Change</h3><pre><code class='java'>

    scaffold = tf.train.Scaffold(
        &#47&#47 Initialize global variables.
        init_op=<a id="change">tf</a>.global_variables_initializer() if is_chief else <a id="change">tf.no_op()</a>,
        &#47&#47 Queue-related variables need a special initializer.
        local_init_op=tf.local_variables_initializer(),
        summary_op=summary_op,</code></pre>