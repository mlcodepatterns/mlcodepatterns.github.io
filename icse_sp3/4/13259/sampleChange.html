<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
  num_gpus = flags_core.get_num_gpus(flags_obj)
  multi_gpu = num_gpus &gt; 1

  <a id="change">if multi_gpu:
    &#47&#47 Validate that the batch size can be split into devices.
    distribution_utils.per_device_batch_size(flags_obj.batch_size, num_gpus)

    &#47&#47 There are two steps required if using multi-GPU: (1) wrap the model_fn,
    &#47&#47 and (2) wrap the optimizer. The first happens here, and (2) happens
    &#47&#47 in the model_fn itself when the optimizer is defined.
    model_function = tf.contrib.estimator.replicate_model_fn(
        model_fn, loss_reduction=tf.losses.Reduction.MEAN,
        devices=["/device:GPU:%d" % d for d in range(num_gpus)])

 </a> data_format = flags_obj.data_format
  if data_format is None:
    data_format = (&quotchannels_first&quot
                   if tf.test.is_built_with_cuda() else &quotchannels_last&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
  distribution_strategy = distribution_utils.get_distribution_strategy(
      flags_core.get_num_gpus(flags_obj), flags_obj.all_reduce_alg)

  run_config = <a id="change">tf.estimator.RunConfig(
      train_distribute=distribution_strategy, session_config=session_config)</a>

  data_format = flags_obj.data_format
  if data_format is None:
    data_format = (&quotchannels_first&quot</code></pre>