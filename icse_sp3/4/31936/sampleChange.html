<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
          outputs = [outputs[i] for i in self._loss_outputs]
        batch_loss = loss(outputs, labels, weights)
      if variables is None:
        <a id="change">vars = self.model.trainable_variables</a>
      else:
        vars = variables
      <a id="change">grads = tape.gradient(batch_loss, vars)</a>
      self._tf_optimizer.apply_gradients(zip(grads, vars))
      self._global_step.assign_add(1)
      current_step = self._global_step.numpy()
</code></pre><h3>After Change</h3><pre><code class='java'>
    if loss is None:
      loss = self._loss_fn
    var_key = None
    <a id="change">if variables is not None:
      var_key = tuple(v.experimental_ref() for v in variables)

      &#47&#47 The optimizer creates internal variables the first time apply_gradients()
      &#47&#47 is called for a new set of variables.  If that happens inside a function
      &#47&#47 annotated with tf.function it throws an exception, so call it once here.

      zero_grads = [tf.zeros(v.shape) for v in variables]
      self._tf_optimizer.apply_gradients(zip(zero_grads, variables))
   </a> if var_key not in self._gradient_fn_for_vars:
      self._gradient_fn_for_vars[var_key] = self._create_gradient_fn(variables)
    apply_gradient_for_batch = self._gradient_fn_for_vars[var_key]
    time1 = time.time()</code></pre>