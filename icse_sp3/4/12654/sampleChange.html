<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        tokens_batch = self.batch_input(tokens)
        self.prepare_vectorizers(tokens_batch)
        if self.preproc == "client":
            examples = <a id="change">self.vectorize(tokens_batch)</a>
        elif self.preproc == &quotserver&quot:
            &#47&#47 TODO: here we allow vectorizers even for preproc=server to get `word_lengths`.
            &#47&#47 vectorizers should not be available when preproc=server.
            featurized_examples = self.vectorize(tokens_batch)
            examples = {
                        &quottokens&quot: np.array([" ".join(x) for x in tokens_batch]),
                        self.model.lengths_key: featurized_examples[self.model.lengths_key]
            }

        <a id="change">outcomes_list = self.model.predict(examples)</a>
        return self.format_output(outcomes_list)

    def format_output(self, predicted):
        results = []</code></pre><h3>After Change</h3><pre><code class='java'>
        version = kwargs.get(&quotversion&quot)

        if backend not in {&quottf&quot}:
            raise ValueError("only Tensorfl<a id="change">ow is curren</a>tly supported for remote Services")
        import_user_module(&quotbaseline.{}.remote&quot.format(backend))
        exp_type = kwargs.get(&quotremote_type&quot)
        if exp_type is None:</code></pre>