<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 The next lines performs the following function, which is not supported by TorchScript
        &#47&#47 sorted_mega_batch = sorted(mega_batch,key=lambda element : len(element[argno]))
        mega_batch_class_list = <a id="change">[
            PytextTwoTowerEmbeddingModuleBatchSort(x, argno) for x in mega_batch
        ]</a>
        sorted_mega_batch_class_list = sorted(mega_batch_class_list)
        <a id="change">sorted_mega_batch = [x.be() for x in sorted_mega_batch_class_list]</a>

        &#47&#47 TBD: allow model server to specify batch size in goals dictionary
        max_bs: int = 10
        len_mb = len(mega_batch)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 The next lines sort all cross-request batch elements by the token length of right_.
        &#47&#47 Note that cross-request batch element can in turn be a client batch.
        mega_batch_key_list = [
            (max_tokens(<a id="change">self.right_tensorizer.tokenize(x[0], x[2])</a>), n)
            for (n, x) in enumerate(mega_batch)
        ]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [<a id="change">mega_batch[n]</a> <a id="change">for</a> (key, n) in sorted_mega_batch_key_list]

        &#47&#47 TBD: allow model server to specify batch size in goals dictionary
        max_bs: int = 10</code></pre>