<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

  with tf.device(&quot/cpu:0&quot):
    grads_and_vars = _average_gradients(tower_grads)
    <a id="change">if tower_summaries is not None:
      summaries.append(tower_summaries)

    &#47&#47 Modify the gradients for biases and last layer variables.
   </a> last_layers = model.get_extra_layer_scopes(
        FLAGS.last_layers_contain_logits_only)
    grad_mult = train_utils.get_model_gradient_multipliers(
        last_layers, FLAGS.last_layer_gradient_multiplier)</code></pre><h3>After Change</h3><pre><code class='java'>
        quant_delay=FLAGS.quantize_delay_step)

  for i in range(FLAGS.num_clones):
    <a id="change">with tf.device(&quot/gpu:%d&quot % i):
      name_scope = (&quotclone_%d&quot % i) if i else &quot&quot
      with tf.name_scope(name_scope) as scope:
        grads = optimizer.compute_gradients(tower_losses[i])
        tower_grads.append(grads)

 </a> with tf.device(&quot/cpu:0&quot):
    grads_and_vars = _average_gradients(tower_grads)

    &#47&#47 Modify the gradients for biases and last layer variables.</code></pre>