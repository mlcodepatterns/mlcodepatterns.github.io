<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            else:
                results = lda_instance.perplexity(data)

            <a id="change">logger.info(&quot&gt; evaluation result with metric "%s": %f&quot % (self.eval_metric, results))</a>

        self.send_results(params, results)

</code></pre><h3>After Change</h3><pre><code class='java'>
            lda_instance.fit(data)

            results = {}
            <a id="change">for metric in self.eval_metric:
                if metric == &quotcross_validation&quot: continue

                metric_opt = self.eval_metric_options.get(metric, {})

                if metric == &quotcao_juan_2009&quot:
                    topic_word_distrib = lda_instance.components_ / lda_instance.components_.sum(axis=1)[:, np.newaxis]
                    res = metric_cao_juan_2009(topic_word_distrib)
                elif metric == &quotarun_2010&quot:
                    topic_word_distrib = lda_instance.components_ / lda_instance.components_.sum(axis=1)[:, np.newaxis]
                    res = metric_arun_2010(topic_word_distrib, lda_instance.transform(data), data.sum(axis=1))
                else:  &#47&#47 default: perplexity
                    res = lda_instance.perplexity(data)

                logger.info(&quot&gt; evaluation result with metric "%s": %f&quot % (metric, res))
                results[metric] = res

       </a> self.send_results(params, results)


def evaluate_topic_models(varying_parameters, constant_parameters, data, metric=None, n_workers=None, n_folds=0,</code></pre>