<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        Returns:
            An numpy.ndarray containing the results.
        
        <a id="change">if constant.LIMIT_MEMORY:
            config = tf.ConfigProto()
            config.gpu_options.allow_growth = True
            sess = tf.Session(config=config)
            init = tf.global_variables_initializer()
            sess.run(init)
            backend.set_session(sess)
       </a> x_test = x_test.astype(&quotfloat32&quot) / 255
        model = self.load_searcher().load_best_model().produce_model()
        return self.y_encoder.inverse_transform(model.predict(x_test, ))
</code></pre><h3>After Change</h3><pre><code class='java'>
        if constant.LIMIT_MEMORY:
            &#47&#47 TODO: limit pytorch memory.
            pass
        <a id="change">test_data = self.data_transformer.transform_test(x_test)</a>
        <a id="change">test_loader = DataLoader(test_data, batch_size=constant.MAX_BATCH_SIZE, shuffle=True)</a>
        model = self.load_searcher().load_best_model().produce_model()
        model.eval()

        outputs = []
        with torch.no_grad():
            for index, inputs in <a id="change">enumerate(test_loader)</a>:
                outputs.append(model(inputs).numpy())
        output = reduce(lambda x, y: np.concatenate((x, y)), outputs)
        return self.y_encoder.inverse_transform(output)</code></pre>