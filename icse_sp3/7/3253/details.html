<html><h3>0a68639f4c33323274e7829b9349d0170dc6c8ea,train.py,,classifier,#,67
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    best_va = 0.0

    <a id="change">with tf.device(ARGS.train_device):
        global_step = tf.Variable(0, trainable=False, name=&quotglobal_step&quot)

        &#47&#47 Get images and labels
        with tf.device(&quot/cpu:0&quot):
            images, labels = DATASET.distorted_inputs(ARGS.batch_size)
        log_io(images)
        &#47&#47 Build a Graph that computes the logits predictions from the
        &#47&#47 inference model.
        is_training_, logits = MODEL.get(
            images,
            DATASET.num_classes(),
            train_phase=True,
            l2_penalty=ARGS.l2_penalty)

        &#47&#47 Calculate loss.
        loss = MODEL.loss(logits, labels)
        tf_log(tf.summary.scalar(&quotloss&quot, loss))

        &#47&#47 Create optimizer and log learning rate
        optimizer = build_optimizer(global_step)
        train_op = optimizer.minimize(
            loss,
            global_step=global_step,
            var_list=variables_to_train(ARGS.trainable_scopes))

        train_accuracy = utils.accuracy_op(logits, labels)
        &#47&#47 General validation summary
        accuracy_value_ = tf.placeholder(tf.float32, shape=())
        accuracy_summary = tf.summary.scalar(&quotaccuracy&quot, accuracy_value_)

        &#47&#47 read collection after that every op added its own
        &#47&#47 summaries in the train_summaries collection
        train_summaries = tf.summary.merge(
            tf.get_collection_ref(MODEL_SUMMARIES))

        &#47&#47 Build an initialization operation to run below.
        init = [
            tf.variables_initializer(tf.global_variables() +
                                     tf.local_variables()),
            tf.tables_initializer()
        ]

        &#47&#47 Start running operations on the Graph.
        with tf.Session(config=tf.ConfigProto(
                allow_soft_placement=True)) as sess:
            sess.run(init)

            &#47&#47 Start the queue runners with a coordinator
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(sess=sess, coord=coord)

            &#47&#47 Create the savers.
            train_saver, best_saver = build_train_savers([global_step])
            restore_or_restart(sess, global_step)
            train_log, validation_log = build_loggers(sess.graph)

            &#47&#47 Extract previous global step value
            old_gs = sess.run(global_step)

            &#47&#47 Restart from where we were
            for step in range(old_gs, MAX_STEPS):
                start_time = time.time()
                _, loss_value = sess.run(
                    [train_op, loss], feed_dict={is_training_: True})

                duration = time.time() - start_time

                if np.isnan(loss_value):
                    print(&quotModel diverged with loss = NaN&quot)
                    break

                &#47&#47 update logs every 10 iterations
                if step % 10 == 0:
                    examples_per_sec = ARGS.batch_size / duration
                    sec_per_batch = float(duration)

                    format_str = (&quot{}: step {}, loss = {:.4f} &quot
                                  &quot({:.1f} examples/sec; {:.3f} sec/batch)&quot)
                    print(
                        format_str.format(datetime.now(), step, loss_value,
                                          examples_per_sec, sec_per_batch))
                    &#47&#47 log train values
                    summary_lines = sess.run(
                        train_summaries, feed_dict={is_training_: True})
                    train_log.add_summary(summary_lines, global_step=step)

                &#47&#47 Save the model checkpoint at the end of every epoch
                &#47&#47 evaluate train and validation performance
                if (step &gt; 0 and
                        step % STEPS_PER_EPOCH == 0) or (step + 1) == MAX_STEPS:
                    checkpoint_path = os.path.join(LOG_DIR, &quotmodel.ckpt&quot)
                    train_saver.save(sess, checkpoint_path, global_step=step)

                    &#47&#47 validation accuracy
                    va_value = eval_model(LOG_DIR, InputType.validation)

                    summary_line = sess.run(
                        accuracy_summary, feed_dict={accuracy_value_: va_value})
                    validation_log.add_summary(summary_line, global_step=step)

                    &#47&#47 train accuracy
                    ta_value = sess.run(
                        train_accuracy, feed_dict={is_training_: False})
                    summary_line = sess.run(
                        accuracy_summary, feed_dict={accuracy_value_: ta_value})
                    train_log.add_summary(summary_line, global_step=step)

                    print(
                        &quot{} ({}): train accuracy = {:.3f} validation accuracy = {:.3f}&quot.
                        format(datetime.now(),
                               int(step / STEPS_PER_EPOCH), ta_value, va_value))
                    &#47&#47 save best model
                    if va_value &gt; best_va:
                        best_va = va_value
                        best_saver.save(
                            sess,
                            os.path.join(BEST_MODEL_DIR, &quotmodel.ckpt&quot),
                            global_step=step)
            &#47&#47 end of for

            validation_log.close()
            train_log.close()

            &#47&#47 When done, ask the threads to stop.
            coord.request_stop()
            &#47&#47 Wait for threads to finish.
            coord.join(threads)


</a>def autoencoder():
    Train the autoencoder and saves the best model:
    that&quots the model with the lower validation error.
    </code></pre><h3>After Change</h3><pre><code class='java'>

    best_va = 0.0

    <a id="change">with tf.Graph().as_default(), tf.device(ARGS.train_device):
        global_step = tf.Variable(0, trainable=False, name=&quotglobal_step&quot)

        &#47&#47 Get images and labels
        with tf.device(&quot/cpu:0&quot):
            images, labels = DATASET.distorted_inputs(ARGS.batch_size)
        log_io(images)
        &#47&#47 Build a Graph that computes the logits predictions from the
        &#47&#47 inference model.
        is_training_, logits = MODEL.get(
            images,
            DATASET.num_classes,
            train_phase=True,
            l2_penalty=ARGS.l2_penalty)

        &#47&#47 Calculate loss.
        loss = MODEL.loss(logits, labels)
        tf_log(tf.summary.scalar(&quotloss&quot, loss))

        &#47&#47 Create optimizer and log learning rate
        optimizer = build_optimizer(global_step)
        train_op = optimizer.minimize(
            loss,
            global_step=global_step,
            var_list=variables_to_train(ARGS.trainable_scopes))

        train_accuracy = utils.accuracy_op(logits, labels)
        &#47&#47 General validation summary
        accuracy_value_ = tf.placeholder(tf.float32, shape=())
        accuracy_summary = tf.summary.scalar(&quotaccuracy&quot, accuracy_value_)

        &#47&#47 read collection after that every op added its own
        &#47&#47 summaries in the train_summaries collection
        train_summaries = tf.summary.merge(
            tf.get_collection_ref(MODEL_SUMMARIES))

        &#47&#47 Build an initialization operation to run below.
        init = [
            tf.variables_initializer(tf.global_variables() +
                                     tf.local_variables()),
            tf.tables_initializer()
        ]

        &#47&#47 Start running operations on the Graph.
        with tf.Session(config=tf.ConfigProto(
                allow_soft_placement=True)) as sess:
            sess.run(init)

            &#47&#47 Start the queue runners with a coordinator
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(sess=sess, coord=coord)

            &#47&#47 Create the savers.
            train_saver, best_saver = build_train_savers([global_step])
            restore_or_restart(sess, global_step)
            train_log, validation_log = build_loggers(sess.graph)

            &#47&#47 Extract previous global step value
            old_gs = sess.run(global_step)

            &#47&#47 Restart from where we were
            for step in range(old_gs, MAX_STEPS):
                start_time = time.time()
                _, loss_value = sess.run(
                    [train_op, loss], feed_dict={is_training_: True})

                duration = time.time() - start_time

                if np.isnan(loss_value):
                    print(&quotModel diverged with loss = NaN&quot)
                    break

                &#47&#47 update logs every 10 iterations
                if step % 10 == 0:
                    examples_per_sec = ARGS.batch_size / duration
                    sec_per_batch = float(duration)

                    format_str = (&quot{}: step {}, loss = {:.4f} &quot
                                  &quot({:.1f} examples/sec; {:.3f} sec/batch)&quot)
                    print(
                        format_str.format(datetime.now(), step, loss_value,
                                          examples_per_sec, sec_per_batch))
                    &#47&#47 log train values
                    summary_lines = sess.run(
                        train_summaries, feed_dict={is_training_: True})
                    train_log.add_summary(summary_lines, global_step=step)

                &#47&#47 Save the model checkpoint at the end of every epoch
                &#47&#47 evaluate train and validation performance
                if (step &gt; 0 and
                        step % STEPS_PER_EPOCH == 0) or (step + 1) == MAX_STEPS:
                    checkpoint_path = os.path.join(LOG_DIR, &quotmodel.ckpt&quot)
                    train_saver.save(sess, checkpoint_path, global_step=step)

                    &#47&#47 validation accuracy
                    va_value = eval_model(LOG_DIR, InputType.validation)

                    summary_line = sess.run(
                        accuracy_summary, feed_dict={accuracy_value_: va_value})
                    validation_log.add_summary(summary_line, global_step=step)

                    &#47&#47 train accuracy
                    ta_value = sess.run(
                        train_accuracy, feed_dict={is_training_: False})
                    summary_line = sess.run(
                        accuracy_summary, feed_dict={accuracy_value_: ta_value})
                    train_log.add_summary(summary_line, global_step=step)

                    print(
                        &quot{} ({}): train accuracy = {:.3f} validation accuracy = {:.3f}&quot.
                        format(datetime.now(),
                               int(step / STEPS_PER_EPOCH), ta_value, va_value))
                    &#47&#47 save best model
                    if va_value &gt; best_va:
                        best_va = va_value
                        best_saver.save(
                            sess,
                            os.path.join(BEST_MODEL_DIR, &quotmodel.ckpt&quot),
                            global_step=step)
            &#47&#47 end of for

            validation_log.close()
            train_log.close()

            &#47&#47 When done, ask the threads to stop.
            coord.request_stop()
            &#47&#47 Wait for threads to finish.
            coord.join(threads)


</a>def autoencoder():
    Train the autoencoder and saves the best model:
    that&quots the model with the lower validation error.
    </code></pre><img src="22004797.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/galeone/dynamic-training-bench/commit/0a68639f4c33323274e7829b9349d0170dc6c8ea#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L74' target='_blank'>Link</a></div><div id='project'> Project Name: galeone/dynamic-training-bench</div><div id='commit'> Commit Name: 0a68639f4c33323274e7829b9349d0170dc6c8ea</div><div id='time'> Time: 2017-02-08</div><div id='author'> Author: nessuno@nerdz.eu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: classifier</div><BR><BR><div id='link'><a href='https://github.com/galeone/dynamic-training-bench/commit/0a68639f4c33323274e7829b9349d0170dc6c8ea#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L469' target='_blank'>Link</a></div><div id='project'> Project Name: galeone/dynamic-training-bench</div><div id='commit'> Commit Name: 0a68639f4c33323274e7829b9349d0170dc6c8ea</div><div id='time'> Time: 2017-02-08</div><div id='author'> Author: nessuno@nerdz.eu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: detector</div><BR><BR><div id='link'><a href='https://github.com/galeone/dynamic-training-bench/commit/0a68639f4c33323274e7829b9349d0170dc6c8ea#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L337' target='_blank'>Link</a></div><div id='project'> Project Name: galeone/dynamic-training-bench</div><div id='commit'> Commit Name: 0a68639f4c33323274e7829b9349d0170dc6c8ea</div><div id='time'> Time: 2017-02-08</div><div id='author'> Author: nessuno@nerdz.eu</div><div id='file'> File Name: train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: regressor</div><BR>