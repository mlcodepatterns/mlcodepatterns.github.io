<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        else:
            self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale, d_k=d_k)

        <a id="change">self.ffn = FFN(d_model, activation_type, d_ff, ffn_pdrop, name="ffn")</a>
        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)
        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)
        self.dropout = tf.keras.layers.Dropout(pdrop)
</code></pre><h3>After Change</h3><pre><code class='java'>
def tie_weight(weight, tie_shape):
    Higher order function to share weights between two layers.

    Tensorflow will take a cust<a id="change">om_getter inside of a variable scope.
    This method creates a getter that looks for a match in shapes. If they match,
    The weights are transposed and shared.

    
    def tie_getter(getter, name</a>, *args, **kwargs):
        if kwargs[&quotshape&quot] == tie_shape:
            return tf.transpose(weight)
        retur<a id="change">n getter("{}".format(name), *args, **kwargs)
 </a>   return tie_getter<a id="change">


def rnn_cell_w_dropout(hsz, pdrop, rnntype,</a> st=None, variational=False, training=False):

    Produce a single RNN cell with dropout
    :param hsz: (``int``) The number of hidden units per LSTM</code></pre>