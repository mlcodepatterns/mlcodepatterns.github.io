<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
env = OpenAIGym(&quotCartPole-v0&quot)


<a id="change">config = Configuration(
    batch_size=4096,
    &#47&#47 Agent
    preprocessing=None,
    exploration=None,
    reward_preprocessing=None,
    &#47&#47 BatchAgent
    keep_last_timestep=True,
    &#47&#47 PPOAgent
    step_optimizer=dict(
        type=&quotadam&quot,
        learning_rate=1e-3
    ),
    optimization_steps=10,
    &#47&#47 Model
    scope=&quotppo&quot,
    discount=0.99,
    &#47&#47 DistributionModel
    distributions=None,  &#47&#47 not documented!!!
    entropy_regularization=0.01,
    &#47&#47 PGModel
    baseline_mode=None,
    baseline=None,
    baseline_optimizer=None,
    gae_lambda=None,
    normalize_rewards=False,
    &#47&#47 PGLRModel
    likelihood_ratio_clipping=0.2,
    &#47&#47 Logging
    log_level=&quotinfo&quot,
    &#47&#47 TensorFlow Summaries
    summary_logdir=None,
    summary_labels=[&quottotal-loss&quot],
    summary_frequency=1,
    &#47&#47 Distributed
    &#47&#47 TensorFlow distributed configuration
    cluster_spec=None,
    parameter_server=False,
    task_index=0,
    device=None,
    local_model=False,
    replica_model</a>=False,
)

&#47&#47 Network as list of layers
network_spec = [
    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot),
    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
]

agent = <a id="change">PPOAgent(
    states_spec=env.states,
    actions_spec=env.actions,
    network_spec=network_spec,
    config=config
)</a>

&#47&#47 Create the runner
runner = Runner(agent=agent, environment=env)
</code></pre><h3>After Change</h3><pre><code class='java'>
    dict(type=&quotdense&quot, size=32, activation=&quottanh&quot)
]

agent = <a id="change">PPOAgent(
    states_spec=env.states,
    actions_spec=env.actions,
    network_spec=network_spec,
    batch_size=4096,
    &#47&#47 Agent
    preprocessing=None,
    exploration=None,
    reward_preprocessing=None,
    &#47&#47 BatchAgent
    keep_last_timestep=True,
    &#47&#47 PPOAgent
    step_optimizer=dict(
        type=&quotadam&quot,
        learning_rate=1e-3
    ),
    optimization_steps=10,
    &#47&#47 Model
    scope=&quotppo&quot,
    discount=0.99,
    &#47&#47 DistributionModel
    distributions_spec=None,
    entropy_regularization=0.01,
    &#47&#47 PGModel
    baseline_mode=None,
    baseline=None,
    baseline_optimizer=None,
    gae_lambda=None,
    normalize_rewards=False,
    &#47&#47 PGLRModel
    likelihood_ratio_clipping=0.2,
    summary_spec=None,
    distributed_spec=None
)</a>

&#47&#47 Create the runner
runner = Runner(agent=agent, environment=env)
</code></pre>