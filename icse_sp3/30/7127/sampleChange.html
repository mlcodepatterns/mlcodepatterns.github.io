<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        with tf.variable_scope(name, initializer=initializer) as vs:
            self.fw_cell = cell_fn(num_units=n_hidden, **cell_init_args)
            <a id="change">self.bw_cell</a> = <a id="change">cell_fn(num_units=n_hidden, **cell_init_args)</a>
            &#47&#47 Apply dropout
            if dropout:
                if type(dropout) in [tuple, list]:
                    in_keep_prob = dropout[0]
                    out_keep_prob = dropout[1]
                elif isinstance(dropout, float):
                    in_keep_prob, out_keep_prob = dropout, dropout
                else:
                    raise Exception("Invalid dropout type (must be a 2-D tuple of "
                                    "float)")
                try: &#47&#47 TF 1.0
                    DropoutWrapper_fn = tf.contrib.rnn.DropoutWrapper
                except:
                    DropoutWrapper_fn = tf.nn.rnn_cell.DropoutWrapper
                self.fw_cell = DropoutWrapper_fn(
                          self.fw_cell,
                          input_keep_prob=in_keep_prob,
                          output_keep_prob=out_keep_prob)
                <a id="change">self.bw_cell = DropoutWrapper_fn(
                          self.bw_cell,
                          input_keep_prob=in_keep_prob,
                          output_keep_prob=out_keep_prob)</a>
            &#47&#47 Apply multiple layers
            if n_layer &gt; 1:
                try: &#47&#47 TF1.0
                    MultiRNNCell_fn = tf.contrib.rnn.MultiRNNCell
                except:
                    MultiRNNCell_fn = tf.nn.rnn_cell.MultiRNNCell

                try:
                    self.fw_cell = MultiRNNCell_fn([self.fw_cell] * n_layer,
                                                          state_is_tuple=True)
                    <a id="change">self.bw_cell = MultiRNNCell_fn([self.bw_cell] * n_layer,
                                                          state_is_tuple=True)</a>
                except:
                    self.fw_cell = MultiRNNCell_fn([self.fw_cell] * n_layer)
                    self.bw_cell = MultiRNNCell_fn([self.bw_cell] * n_layer)

            &#47&#47 Initial state of RNN
            if fw_initial_state is None:
                self.fw_initial_state = self.fw_cell.zero_state(self.batch_size, dtype=tf.float32)
            else:
                self.fw_initial_state = fw_initial_state
            if bw_initial_state is None:
                <a id="change">self.bw_initial_state</a> = self.bw_cell.zero_state(self.batch_size, dtype=tf.float32)
            else:
                self.bw_initial_state = bw_initial_state
            &#47&#47 exit()</code></pre><h3>After Change</h3><pre><code class='java'>
            else:
                cell_creator = rnn_creator
            self.fw_cell = cell_creator()
            <a id="change">self.bw_cell = cell_creator()</a>

            &#47&#47 Apply multiple layers
            if n_layer &gt; 1:
                try: &#47&#47 TF1.0
                    MultiRNNCell_fn = tf.contrib.rnn.MultiRNNCell
                except:
                    MultiRNNCell_fn = tf.nn.rnn_cell.MultiRNNCell

                try:
                    self.fw_cell = MultiRNNCell_fn([cell_creator() for _ in range(n_layer)], state_is_tuple=True)
                    <a id="change">self.bw_cell = MultiRNNCell_fn([cell_creator() for _ in range(n_layer)], state_is_tuple=True)</a>
                except:
                    self.fw_cell = <a id="change">MultiRNNCell_fn([cell_creator() for _ in range(n_layer)])</a>
                    self.bw_cell = MultiRNNCell_fn([cell_creator() for _ in range(n_layer)])

            &#47&#47 Initial state of RNN
            if fw_initial_state is None:
                self.fw_initial_state = self.fw_cell.zero_state(self.batch_size, dtype=tf.float32)
            else:
                self.fw_initial_state = fw_initial_state
            if bw_initial_state is None:
                <a id="change">self.bw_initial_state</a> = self.bw_cell.zero_state(self.batch_size, dtype=tf.float32)
            else:
                self.bw_initial_state = bw_initial_state
            &#47&#47 exit()</code></pre>