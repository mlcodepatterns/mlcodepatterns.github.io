<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            with state_kept(self.q_function):
                next_qout = self.q_function(batch_next_state)

            <a id="change">with state_kept(self.target_q_function):
                target_next_qout = self.target_q_function(
                    batch_next_state)
           </a> next_q_max = F.reshape(target_next_qout.evaluate_actions(
                next_qout.greedy_actions), (batch_size,))

            batch_rewards = exp_batch[&quotreward&quot]</code></pre><h3>After Change</h3><pre><code class='java'>

class DoublePAL(pal.PAL):

    def _compute_y_and_t(<a id="change">self</a>, exp_batch):

        batch_state = exp_batch[&quotstate&quot]
        batch_size = len(exp_batch[&quotreward&quot])

        <a id="change">if self.recurrent:
            qout, _ = self.model.n_step_forward(
                batch_state, exp_batch[&quotrecurrent_state&quot],
                output_mode=&quotconcat&quot)
        else:
            qout = self.model(batch_state)

       </a> batch_actions = exp_batch[&quotaction&quot]
        batch_q = qout.evaluate_actions(batch_actions)

        &#47&#47 Compute target values

        with chainer.no_backprop_mode():
            batch_next_state = exp_batch[&quotnext_state&quot]
            <a id="change">if self.recurrent:
                next_qout, _ = self.model.n_step_forward(
                    batch_next_state, exp_batch[&quotnext_recurrent_state&quot],
                    output_mode=&quotconcat&quot)
                target_qout, _ = self.target_model.n_step_forward(
                    batch_state, exp_batch[&quotrecurrent_state&quot],
                    output_mode=&quotconcat&quot)
                target_next_qout, _ = self.target_model.n_step_forward(
                    batch_next_state, exp_batch[&quotnext_recurrent_state&quot],
                    output_mode=&quotconcat&quot)
            else:
                next_qout = self.model(batch_next_state)
                target_qout = self.target_model(batch_state)
                target_next_qout = self.target_model(batch_next_state)

           </a> next_q_max = F.reshape(target_next_qout.evaluate_actions(
                next_qout.greedy_actions), (batch_size,))

            batch_rewards = exp_batch[&quotreward&quot]</code></pre>