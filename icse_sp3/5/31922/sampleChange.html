<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      self.cell.cells[0] = tfa.seq2seq.AttentionWrapper(
          self.cell.cells[0],
          attention_mechanism,
          attention_layer_size=<a id="change">self</a>.cell.cells[0].output_size,
          initial_cell_state=initial_cell_state[0])
    else:
      self.cell = tfa.seq2seq.AttentionWrapper(</code></pre><h3>After Change</h3><pre><code class='java'>
        self.memory,
        memory_sequence_length=self.memory_sequence_length)
    decoder_state = self.cell.get_initial_state(batch_size=batch_size, dtype=dtype)
    <a id="change">if initial_state is not None:
      if self.first_layer_attention:
        cell_state = list(decoder_state)
        cell_state[0] = decoder_state[0].cell_state
        cell_state = self.bridge(initial_state, cell_state)
        cell_state[0] = decoder_state[0].clone(cell_state=cell_state[0])
        decoder_state = tuple(cell_state)
      else:
        cell_state = self.bridge(initial_state, decoder_state.cell_state)
        decoder_state = decoder_state.clone(cell_state=cell_state)
   </a> return decoder_state

  def step(self,
           inputs,</code></pre>