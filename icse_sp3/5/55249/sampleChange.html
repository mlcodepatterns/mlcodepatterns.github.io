<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

if __name__ == &quot__main__&quot:
    arg_parser = argparse.ArgumentParser(description=&quotDeepSpeech transcription&quot)
    <a id="change">arg_parser = add_inference_args(arg_parser)</a>
    arg_parser.add_argument(&quot--audio-path&quot,
                            default=&quotaudio.wav&quot,
                            help=&quotAudio file to predict on&quot)
    arg_parser.add_argument(&quot--offsets&quot,
                            dest=&quotoffsets&quot,
                            action=&quotstore_true&quot,
                            help=&quotReturns time offset information&quot)
    arg_parser = add_decoder_args(arg_parser)
    args = arg_parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")
    <a id="change">model = load_model(device, args.model_path, args.half)</a>

    decoder = load_decoder(decoder_type=args.decoder,
                           labels=model.labels,
                           lm_path=args.lm_path,
                           alpha=args.alpha,
                           beta=args.beta,
                           cutoff_top_n=args.cutoff_top_n,
                           cutoff_prob=args.cutoff_prob,
                           beam_width=args.beam_width,
                           lm_workers=args.lm_workers)

    <a id="change">spect_parser</a> = SpectrogramParser(audio_conf=model.audio_conf,
                                     normalize=True)

    decoded_output, decoded_offsets = transcribe(audio_path=args.audio_path,</code></pre><h3>After Change</h3><pre><code class='java'>
    transcribe(cfg=cfg)


<a id="change">if __name__ == &quot__main__&quot:
    hydra_main()</a>
</code></pre>