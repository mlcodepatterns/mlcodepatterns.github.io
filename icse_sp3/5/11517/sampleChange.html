<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
class TransformerEncoder(tf.keras.layers.Layer):

    def __init__(self, d_model, num_heads, pdrop, scale=True, activation=&quotrelu&quot, d_ff=None, name=None):
        <a id="change">super(TransformerEncoder, self).__init__(name=name)</a>
        if d_ff is None:
            d_ff = 4 * d_model
        self.ln1 = LayerNorm(name=&quotln_1&quot)
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        self.ffn = FFN(d_model, pdrop, activation_type, d_ff, name=&quotffn&quot)
        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.ln2 = <a id="change">tf.keras.layers.LayerNormalization(epsilon=1e-6)</a>
        self.dropout = tf.keras.layers.Dropout(pdrop)

    def forward(self, inputs):
        </code></pre>