<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    }
                    results = self.session.run(fetches, feed_dict)
                    if (n_updates % 1000) == 0:
                        <a id="change">self.writer.add_summary(results[-1], n_updates)</a>
                    n_updates += 1
                self.writer.flush()

            if self.config["save_model"]:</code></pre><h3>After Change</h3><pre><code class='java'>
        Run learning algorithm
        config = self.config
        n_updates = 0
        <a id="change">with self.writer.as_default():
            for _ in range(int(config["n_iter"])):
                &#47&#47 Collect trajectories until we get timesteps_per_batch total timesteps
                states, actions, advs, rs, _ = self.get_processed_trajectories()
                advs = np.array(advs)
                advs = (advs - advs.mean()) / advs.std()
                self.set_old_to_new()

                indices = np.arange(len(states))
                for _ in range(int(self.config["n_epochs"])):
                    np.random.shuffle(indices)
                    batch_size = int(self.config["batch_size"])
                    for j in range(0, len(states), batch_size):
                        batch_indices = indices[j:(j + batch_size)]
                        batch_states = np.array(states)[batch_indices]
                        batch_actions = np.array(actions)[batch_indices]
                        batch_advs = np.array(advs)[batch_indices]
                        batch_rs = np.array(rs)[batch_indices]
                        train_actor_loss, train_critic_loss, train_loss = self.train(batch_states,
                                                                                     batch_actions,
                                                                                     batch_advs,
                                                                                     batch_rs)
                        tf.summary.scalar("model/loss", train_loss, step=n_updates)
                        tf.summary.scalar("model/actor_loss", train_actor_loss, step=n_updates)
                        tf.summary.scalar("model/critic_loss", train_critic_loss, step=n_updates)
                        n_updates += 1

            if self.config["save_model"]:
                tf.saved_model.save(self.ac_net, os.path.join(self.monitor_path, "model"))


</a>class PPODiscrete(PPO):
    def build_networks(self) -&gt; ActorCriticNetwork:
        return ActorCriticNetworkDiscrete(
            self.env.action_space.n,</code></pre>