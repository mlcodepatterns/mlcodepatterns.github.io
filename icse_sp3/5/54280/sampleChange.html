<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        overflow = False
        try:
            <a id="change">if self.tpu and self.data_parallel_world_size &gt; 1:
                import torch_xla.core.xla_model as xm

                gradients = xm._fetch_gradients(self.optimizer.optimizer)
                xm.all_reduce(
                    "sum", gradients, scale=1.0 / self.data_parallel_world_size
                )

           </a> with torch.autograd.profiler.record_function("multiply-grads"):
                &#47&#47 multiply gradients by (&#47&#47 GPUs / sample_size) since DDP
                &#47&#47 already normalizes by the number of GPUs. Thus we get
                &#47&#47 (sum_of_gradients / sample_size).</code></pre><h3>After Change</h3><pre><code class='java'>
        try:
            with torch.autograd.profiler.record_function("reduce-grads"):
                self.optimizer.all_reduce_grads(self.model)
                <a id="change">if utils.has_parameters(self.criterion):
                    self.optimizer.all_reduce_grads(self.criterion)

           </a> with torch.autograd.profiler.record_function("multiply-grads"):
                &#47&#47 multiply gradients by (data_parallel_size / sample_size) since
                &#47&#47 DDP already normalizes by the number of data parallel workers.
                &#47&#47 Thus we get (sum_of_gradients / sample_size) at the end.</code></pre>