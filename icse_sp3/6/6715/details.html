<html><h3>0e3ecd7a2a4c544de6228ed466c29992f65def8e,cuda_functional.py,SRU_Compute_GPU,backward,#SRU_Compute_GPU#Any#Any#,539
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        init_ = x.new(ncols).zero_() if init is None else init
        grad_u = u.new(*u.size()).zero_()
        grad_wc = x.new(2, batch, d*bidir)
        grad_bias = <a id="change">x.new(2, batch, d*bidir)</a>
        grad_init = x.new(batch, d*bidir)

        &#47&#47  For DEBUG
        &#47&#47  size = (length, batch, x.size(-1)) if x.dim() == 3 else (batch, x.size(-1))
        &#47&#47  grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()

        &#47&#47  Normal use
        grad_x = x.new(*x.size()).zero_() if skip_type &gt; 0 and k_ == 3 else None

        if skip_type &gt; 0 and k_ == 3:
            x_ptr = x.contiguous()*scale_x if scale_x != 1 else x.contiguous()
            x_ptr = x_ptr.data_ptr()
        else:
            x_ptr = 0

        stream, _, _, bwd_func, bibwd_func = self.get_functions()
        FUNC = bwd_func if not self.bidirectional else bibwd_func
        FUNC(args=[
            u.contiguous().data_ptr(),
            x_ptr,
            weight_c.data_ptr(),
            bias.data_ptr(),
            init_.contiguous().data_ptr(),
            mask_h.data_ptr() if mask_h is not None else 0,
            mask_pad.data_ptr() if mask_pad is not None else 0,
            c.data_ptr(),
            grad_h.contiguous().data_ptr(),
            grad_last.contiguous().data_ptr(),
            length,
            batch,
            d,
            k_,
            grad_u.data_ptr(),
            grad_x.data_ptr() if skip_type &gt; 0 and k_ == 3 else 0,
            grad_wc.data_ptr(),
            grad_bias.data_ptr(),
            grad_init.data_ptr(),
            self.activation_type,
            skip_type],
            block=(thread_per_block, 1, 1),
            grid=(num_block, 1, 1),
            stream=stream
        )

        if skip_type &gt; 0 and k_ == 3 and scale_x != 1:
            grad_x.mul_(scale_x)
        <a id="change">return grad_u, grad_x, grad_wc.sum(1).view(-1), grad_bias.sum(1).view(-1), grad_init, None</a>
</code></pre><h3>After Change</h3><pre><code class='java'>

        init_ = x.new(ncols).zero_() if init is None else init
        grad_u = u.new(*u.size()).zero_()
        grad_wc = <a id="change">x.new(2*bidir*d).zero_()</a>
        grad_bias = x.new(2*bidir*d).zero_()
        grad_init = x.new(batch, d*bidir)
        grad_x = x.new(*x.size()).zero_() if skip_type &gt; 0 and k_ == 3 else None

        if skip_type &gt; 0 and k_ == 3:
            x_ptr = x.contiguous()*scale_x if scale_x != 1 else x.contiguous()
            x_ptr = x_ptr.data_ptr()
        else:
            x_ptr = 0

        stream, _, _, bwd_func, bibwd_func = self.get_functions()
        FUNC = bwd_func if not self.bidirectional else bibwd_func
        FUNC(args=[
            u.contiguous().data_ptr(),
            x_ptr,
            weight_c.data_ptr(),
            bias.data_ptr(),
            init_.contiguous().data_ptr(),
            mask_h.data_ptr() if mask_h is not None else 0,
            mask_pad.data_ptr() if mask_pad is not None else 0,
            c.data_ptr(),
            grad_h.contiguous().data_ptr(),
            grad_last.contiguous().data_ptr(),
            length,
            batch,
            d,
            k_,
            grad_u.data_ptr(),
            grad_x.data_ptr() if skip_type &gt; 0 and k_ == 3 else 0,
            grad_wc.data_ptr(),
            grad_bias.data_ptr(),
            grad_init.data_ptr(),
            self.activation_type,
            skip_type],
            block=(thread_per_block, 1, 1),
            grid=(num_block, 1, 1),
            stream=stream
        )

        if skip_type &gt; 0 and k_ == 3 and scale_x != 1:
            grad_x.mul_(scale_x)
        <a id="change">return grad_u, grad_x, grad_wc, grad_bias, grad_init, None</a>
</code></pre><img src="42441767.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/asappresearch/sru/commit/0e3ecd7a2a4c544de6228ed466c29992f65def8e#diff-95f669209186ffa7f21cc90c4980088715073444852ea8e5e87e06c4e371d009L540' target='_blank'>Link</a></div><div id='project'> Project Name: asappresearch/sru</div><div id='commit'> Commit Name: 0e3ecd7a2a4c544de6228ed466c29992f65def8e</div><div id='time'> Time: 2018-07-25</div><div id='author'> Author: hp@asapp.com</div><div id='file'> File Name: cuda_functional.py</div><div id='class'> Class Name: SRU_Compute_GPU</div><div id='method'> Method Name: backward</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/flownet2-pytorch/commit/dafdc9b5cb8fa4c65285aad22b1429549d06d71a#diff-c93e510728956605e2d656ff11edff477b64a79bf0dcfef323efd31c834d696aL27' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/flownet2-pytorch</div><div id='commit'> Commit Name: dafdc9b5cb8fa4c65285aad22b1429549d06d71a</div><div id='time'> Time: 2018-02-04</div><div id='author'> Author: chenkaidev@gmail.com</div><div id='file'> File Name: networks/resample2d_package/functions/resample2d.py</div><div id='class'> Class Name: Resample2dFunction</div><div id='method'> Method Name: backward</div><BR><BR><div id='link'><a href='https://github.com/NVIDIA/flownet2-pytorch/commit/dafdc9b5cb8fa4c65285aad22b1429549d06d71a#diff-c23fc8ce5f70e32dd3085e9b5aeb45d37ea34fce12d266eea0310af3e1d2b98bL26' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/flownet2-pytorch</div><div id='commit'> Commit Name: dafdc9b5cb8fa4c65285aad22b1429549d06d71a</div><div id='time'> Time: 2018-02-04</div><div id='author'> Author: chenkaidev@gmail.com</div><div id='file'> File Name: networks/channelnorm_package/functions/channelnorm.py</div><div id='class'> Class Name: ChannelNormFunction</div><div id='method'> Method Name: backward</div><BR>