<html><h3>8d52bd0b09152b02e0a5504d33593d0c290b88c7,ch14/06_train_d4pg.py,,,#,47
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                q_v = crt_net(states_v, actions_v)
                last_act_v = tgt_act_net.target_model(last_states_v)
                q_last_v = tgt_crt_net.target_model(last_states_v, last_act_v)
                <a id="change">q_last_v[dones_mask]</a> = 0.0
                q_ref_v = rewards_v.unsqueeze(dim=-1) + q_last_v * (GAMMA ** REWARD_STEPS)
                critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())
                critic_loss_v.backward()</code></pre><h3>After Change</h3><pre><code class='java'>
                crt_opt.zero_grad()
                crt_distr_v = crt_net(states_v, actions_v)
                last_act_v = tgt_act_net.target_model(last_states_v)
                last_distr_v = <a id="change">F.softmax(tgt_crt_net.target_model(last_states_v, last_act_v))</a>
                <a id="change">proj_distr_v = distr_projection(last_distr_v, rewards_v, dones_mask,
                                                gamma=GAMMA**REWARD_STEPS, cuda=args.cuda)</a>
                <a id="change">prob_dist_v = -F.log_softmax(crt_distr_v) * proj_distr_v</a>
                critic_loss_v = prob_dist_v.sum(dim=1).mean()
                critic_loss_v.backward()
                crt_opt.step()
                tb_tracker.track("loss_critic", critic_loss_v, frame_idx)</code></pre><img src="133876667.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/8d52bd0b09152b02e0a5504d33593d0c290b88c7#diff-077b2765340b93d1e3aee125f5d00492ee6c49b7e72cb8a6164ef1bd5de4144fL69' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 8d52bd0b09152b02e0a5504d33593d0c290b88c7</div><div id='time'> Time: 2018-02-05</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch14/06_train_d4pg.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/catalyst-team/catalyst/commit/c27dbde9ccec2920f3825538aff07e8533e086ba#diff-74dc86de78a0451867038185ca255cd987ac4b63e10eb008535d16a1b141a2d2L74' target='_blank'>Link</a></div><div id='project'> Project Name: catalyst-team/catalyst</div><div id='commit'> Commit Name: c27dbde9ccec2920f3825538aff07e8533e086ba</div><div id='time'> Time: 2019-07-24</div><div id='author'> Author: scitator@gmail.com</div><div id='file'> File Name: catalyst/rl/offpolicy/algorithms/dqn.py</div><div id='class'> Class Name: DQN</div><div id='method'> Method Name: _categorical_loss</div><BR>