<html><h3>36101ab4095065a4196ff4f6437e94f0d91df4e9,official/nlp/modeling/layers/attention_test.py,CachedAttentionTest,test_masked_attention,#CachedAttentionTest#,201
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 one element.
    mask_data = np.random.randint(
        2, size=(batch_size, from_seq_length, from_seq_length))
    masked_output_data, cache = layer(<a id="change">[from_data, from_data]</a>, mask_data, cache)
    self.assertEqual(masked_output_data.shape, (3, 4, 8))
    self.assertEqual(cache["value"].shape, (3, 4, 2, 2))

    &#47&#47 Tests inputs without cache.
    masked_output_data, cache = <a id="change">layer([from_data, from_data, mask_data])</a>
    self.assertEqual(masked_output_data.shape, (3, 4, 8))
    self.assertIsNone(cache)

  def test_padded_decode(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    layer = attention.CachedAttention(num_heads=num_heads, key_size=head_size)

    &#47&#47 Generate data for the input (non-mask) tensors.
    <a id="change">from_data</a> = tf.zeros((batch_size, from_seq_length, 8), dtype=np.float32)
    &#47&#47 Invoke the data with a random set of mask data. This should mask at least
    &#47&#47 one element.
    mask_data = np.random.randint(
        2, size=(batch_size, from_seq_length, from_seq_length))
    masked_output_data, cache = layer(
        query=from_data, value=from_data, attention_mask=mask_data, cache=cache)
    self.assertEqual(masked_output_data.shape, (3, 4, 8))
    self.assertEqual(cache["value"].shape, (3, 4, 2, 2))

    &#47&#47 Tests inputs without cache.
    masked_output_data, cache = <a id="change">layer(
        query=from_data, value=from_data, attention_mask=mask_data)</a>
    self.assertEqual(masked_output_data.shape, (3, 4, 8))
    self.assertIsNone(cache)

  def test_padded_decode(self):</code></pre><img src="170455208.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/36101ab4095065a4196ff4f6437e94f0d91df4e9#diff-beb24e4a36a2b4ada2a3e4f3dfe80a2e3895f0e11cdbaa5e004017de7131e7abL214' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: 36101ab4095065a4196ff4f6437e94f0d91df4e9</div><div id='time'> Time: 2020-07-21</div><div id='author'> Author: hongkuny@google.com</div><div id='file'> File Name: official/nlp/modeling/layers/attention_test.py</div><div id='class'> Class Name: CachedAttentionTest</div><div id='method'> Method Name: test_masked_attention</div><BR><BR><div id='link'><a href='https://github.com/stellargraph/stellargraph/commit/d7e12cb9dece9bfc7108d29ddd4614c86ecb70bf#diff-fb0d90fa5859fdd93ddf634ea25b14afa1c9d2d3ae2dc7cf410baddd4a48e8ffL91' target='_blank'>Link</a></div><div id='project'> Project Name: stellargraph/stellargraph</div><div id='commit'> Commit Name: d7e12cb9dece9bfc7108d29ddd4614c86ecb70bf</div><div id='time'> Time: 2020-04-30</div><div id='author'> Author: Huon.Wilson@data61.csiro.au</div><div id='file'> File Name: tests/layer/test_sort_pooling.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: test_mask</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/570d9a2b06fd6269c930d7fddf38bc60b212ebee#diff-beb24e4a36a2b4ada2a3e4f3dfe80a2e3895f0e11cdbaa5e004017de7131e7abL214' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: 570d9a2b06fd6269c930d7fddf38bc60b212ebee</div><div id='time'> Time: 2020-07-21</div><div id='author'> Author: hongkuny@google.com</div><div id='file'> File Name: official/nlp/modeling/layers/attention_test.py</div><div id='class'> Class Name: CachedAttentionTest</div><div id='method'> Method Name: test_masked_attention</div><BR>