<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
import tflearn.datasets.oxflower17 as oxflower17
def block35(net, scale=1.0, activation=&quotrelu&quot):
    tower_conv = conv_2d(net, 32, 1, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot, name=&quotConv2d_1x1&quot)
    tower_conv1_0 = <a id="change">conv_2d(net, 32, 1, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot,name=&quotConv2d_0a_1x1&quot)</a>
    tower_conv1_1 = conv_2d(tower_conv1_0, 32, 3, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot,name=&quotConv2d_0b_3x3&quot)
    tower_conv2_0 = <a id="change">conv_2d(net, 32, 1, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot, name=&quotConv2d_0a_1x1&quot)</a>
    tower_conv2_1 = <a id="change">conv_2d(tower_conv2_0, 48,3, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot, name=&quotConv2d_0b_3x3&quot)</a>
    tower_conv2_2 = conv_2d(tower_conv2_1, 64,3, normalizer_fn=&quotbatch_normalization&quot, activation=&quotrelu&quot, name=&quotConv2d_0c_3x3&quot)
    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode=&quotconcat&quot, axis=3)
    tower_out = <a id="change">conv_2d(tower_mixed, net.get_shape()[3], 1, normalizer_fn=&quotbatch_normalization&quot, activation=None, name=&quotConv2d_1x1&quot)</a>
    net += scale * tower_out
    if activation:
        if isinstance(activation, str):
            net = activations.get(activation)(net)</code></pre><h3>After Change</h3><pre><code class='java'>

def block35(net, scale=1.0, activation="relu"):
    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name=&quotConv2d_1x1&quot)))
    tower_conv1_0 = <a id="change">relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name=&quotConv2d_0a_1x1&quot)))</a>
    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name=&quotConv2d_0b_3x3&quot)))
    tower_conv2_0 = <a id="change">relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name=&quotConv2d_0a_1x1&quot)))</a>
    tower_conv2_1 = <a id="change">relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name=&quotConv2d_0b_3x3&quot)))</a>
    tower_conv2_2 = <a id="change">relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name=&quotConv2d_0c_3x3&quot)))</a>
    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode=&quotconcat&quot, axis=3)
    tower_out = <a id="change">relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name=&quotConv2d_1x1&quot)))</a>
    net += scale * tower_out
    if activation:
        if isinstance(activation, str):
            net = activations.get(activation)(net)</code></pre>