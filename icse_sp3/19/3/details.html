<html><h3>a18d8941f663eea55488781c804e6305a36f1b58,ml/rl/training/parametric_dqn_trainer.py,ParametricDQNTrainer,train,#ParametricDQNTrainer#Any#,57
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                training_batch = training_batch.as_parametric_sarsa_training_batch()

        learning_input = training_batch.training_input
        <a id="change">self.minibatch</a> += 1

        reward = learning_input.reward
        not_done_mask = learning_input.not_terminal

        discount_tensor = torch.full_like(reward, self.gamma)
        if self.use_seq_num_diff_as_time_diff:
            assert self.multi_steps is None
            discount_tensor = torch.pow(self.gamma, learning_input.time_diff.float())
        if self.multi_steps is not None:
            discount_tensor = torch.pow(self.gamma, learning_input.step.float())

        if self.maxq_learning:
            all_next_q_values, all_next_q_values_target = self.get_detached_q_values(
                learning_input.tiled_next_state, learning_input.possible_next_actions
            )
            &#47&#47 Compute max a&quot Q(s&quot, a&quot) over all possible actions using target network
            next_q_values, _ = self.get_max_q_values_with_target(
                all_next_q_values.q_value,
                all_next_q_values_target.q_value,
                learning_input.possible_next_actions_mask.float(),
            )
        else:
            &#47&#47 SARSA (Use the target network)
            _, next_q_values = self.get_detached_q_values(
                learning_input.next_state, learning_input.next_action
            )
            next_q_values = next_q_values.q_value

        filtered_max_q_vals = next_q_values * not_done_mask.float()

        <a id="change">if self.minibatch &lt; self.reward_burnin:
            target_q_values = reward
        else:
            target_q_values = reward + (discount_tensor * filtered_max_q_vals)

        &#47&#47 Get Q-value of action taken
       </a> current_state_action = rlt.StateAction(
            state=learning_input.state, action=learning_input.action
        )
        q_values = self.q_network(current_state_action).q_value
        self.all_action_scores = q_values.detach()

        value_loss = self.q_network_loss(q_values, target_q_values)
        self.loss = value_loss.detach()

        self.q_network_optimizer.zero_grad()
        value_loss.backward()
        self.q_network_optimizer.step()

        &#47&#47 TODO: Maybe soft_update should belong to the target network
        <a id="change">if self.minibatch &lt; self.reward_burnin:
            &#47&#47 Reward burnin: force target network
            self._soft_update(self.q_network, self.q_network_target, 1.0)
        else:
            &#47&#47 Use the soft update rule to update target network
            self._soft_update(self.q_network, self.q_network_target, self.tau)

        &#47&#47 get reward estimates
       </a> reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)
        self.reward_network_optimizer.zero_grad()
        reward_loss.backward()</code></pre><h3>After Change</h3><pre><code class='java'>
        self.q_network_optimizer.step()

        &#47&#47 Use the soft update rule to update target network
        <a id="change">self._soft_update(self.q_network, self.q_network_target, self.tau)</a>

        &#47&#47 get reward estimates
        reward_estimates = self.reward_network(current_state_action).q_value
        reward_loss = F.mse_loss(reward_estimates, reward)</code></pre><img src="13210.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 17</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/a18d8941f663eea55488781c804e6305a36f1b58#diff-a421c1688e7de52470d40f812d85accc1903ba475daa8bd7034891dbc49631f4L57' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: a18d8941f663eea55488781c804e6305a36f1b58</div><div id='time'> Time: 2019-04-18</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/parametric_dqn_trainer.py</div><div id='class'> Class Name: ParametricDQNTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/a18d8941f663eea55488781c804e6305a36f1b58#diff-1458fd4d72c999f6eec93ce7d3414a04bc3864600342529de8965ee95e44dbc6L100' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: a18d8941f663eea55488781c804e6305a36f1b58</div><div id='time'> Time: 2019-04-18</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/sac_trainer.py</div><div id='class'> Class Name: SACTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/a18d8941f663eea55488781c804e6305a36f1b58#diff-a421c1688e7de52470d40f812d85accc1903ba475daa8bd7034891dbc49631f4L57' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: a18d8941f663eea55488781c804e6305a36f1b58</div><div id='time'> Time: 2019-04-18</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/parametric_dqn_trainer.py</div><div id='class'> Class Name: ParametricDQNTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/a18d8941f663eea55488781c804e6305a36f1b58#diff-1862d8218744d2795ce4d7951c475fc42a511d11322ed30c9a45d50d42480511L120' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: a18d8941f663eea55488781c804e6305a36f1b58</div><div id='time'> Time: 2019-04-18</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/ddpg_trainer.py</div><div id='class'> Class Name: DDPGTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/a18d8941f663eea55488781c804e6305a36f1b58#diff-1e05823efcf5119bf771fc16f182df9cf4b8aac20b722908d7557012b69896d4L100' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: a18d8941f663eea55488781c804e6305a36f1b58</div><div id='time'> Time: 2019-04-18</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/dqn_trainer.py</div><div id='class'> Class Name: DQNTrainer</div><div id='method'> Method Name: train</div><BR>