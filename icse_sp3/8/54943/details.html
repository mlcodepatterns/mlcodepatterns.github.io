<html><h3>cfeef8cb854a24371f0ef5d7d9c68f23675c97ee,a2_multi_head_attention.py,,multi_head_attention_for_sentence_vectorized,#Any#,168
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    embed_size=d_model
    Embedding = tf.get_variable("Embedding_", shape=[vocab_size, embed_size],initializer=initializer)
    input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name="input_x")
    <a id="change">input_x_=tf.reshape(input_x,(batch_size*sequence_length,))</a> &#47&#47[batch_size*sequence_length]
    embedded_words = tf.nn.embedding_lookup(Embedding, input_x_) &#47&#47[batch_size*sequence_length,embed_size]
    with tf.variable_scope("query_at_each_sentence"+str(layer_number)):
        Q = embedded_words  &#47&#47 [batch_size*sequence_length,embed_size]</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47vectorized implementation of multi head attention for sentences with batch
def multi_head_attention_for_sentence_vectorized(layer_number):
    print("started...")
    <a id="change">start = time.time()</a>
    &#47&#47 1.set parameter
    d_model = 512
    d_k = 64
    d_v = 64
    sequence_length = 1000
    h = 8
    batch_size=128
    initializer = tf.random_normal_initializer(stddev=0.1)
    &#47&#47 2.set Q,K,V
    vocab_size=1000
    embed_size=d_model
    type=&quotdecoder&quot
    Embedding = tf.get_variable("Embedding_", shape=[vocab_size, embed_size],initializer=initializer)
    input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name="input_x")
    embedded_words = tf.nn.embedding_lookup(Embedding, input_x) &#47&#47[batch_size,sequence_length,embed_size]
    mask=get_mask(batch_size,sequence_length) &#47&#47tf.ones((batch_size,sequence_length))*-1e8  &#47&#47[batch,sequence_length]
    with tf.variable_scope("query_at_each_sentence"+str(layer_number)):
        Q = embedded_words  &#47&#47 [batch_size*sequence_length,embed_size]
        K_s=embedded_words &#47&#47[batch_size*sequence_length,embed_size]
        V_s=tf.get_variable("V_s_original_", shape=embedded_words.get_shape().as_list(),initializer=initializer) &#47&#47[batch_size,sequence_length,embed_size]
        &#47&#47 3.call method to get result
        multi_head_attention_class = MultiHeadAttention(Q, K_s, V_s, d_model, d_k, d_v, sequence_length, h,type=&quotdecoder&quot,mask=mask)
        encoder_output=multi_head_attention_class.multi_head_attention_fn() &#47&#47shape:[sequence_length,d_model]
        encoder_output=tf.reshape(encoder_output,shape=(batch_size,sequence_length,d_model))
    <a id="change">end = time.time()</a>
    print("input_x:",input_x)
    print("encoder_output:",encoder_output,";time_spent:",<a id="change">(end-start)</a>)

def get_mask(batch_size,sequence_length):
    lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0)</code></pre><img src="253851681.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/brightmart/text_classification/commit/cfeef8cb854a24371f0ef5d7d9c68f23675c97ee#diff-aca991fbd0c834002f9bc3a96e84df5ff3e6de0e45cc7e4404deb0fc382902acL121' target='_blank'>Link</a></div><div id='project'> Project Name: brightmart/text_classification</div><div id='commit'> Commit Name: cfeef8cb854a24371f0ef5d7d9c68f23675c97ee</div><div id='time'> Time: 2017-07-06</div><div id='author'> Author: brightmart@hotmail.com</div><div id='file'> File Name: a2_multi_head_attention.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: multi_head_attention_for_sentence_vectorized</div><BR><BR><div id='link'><a href='https://github.com/brightmart/text_classification/commit/cfeef8cb854a24371f0ef5d7d9c68f23675c97ee#diff-aca991fbd0c834002f9bc3a96e84df5ff3e6de0e45cc7e4404deb0fc382902acL168' target='_blank'>Link</a></div><div id='project'> Project Name: brightmart/text_classification</div><div id='commit'> Commit Name: cfeef8cb854a24371f0ef5d7d9c68f23675c97ee</div><div id='time'> Time: 2017-07-06</div><div id='author'> Author: brightmart@hotmail.com</div><div id='file'> File Name: a2_multi_head_attention.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: multi_head_attention_for_sentence_vectorized</div><BR><BR><div id='link'><a href='https://github.com/ilastik/ilastik/commit/fe073644f6a8f37e9ce57df903bf12b560690fc3#diff-4fb4d106c653e8c615ba468d15377eb7d4e5c7b70097360c122e589a3edbf9bfL255' target='_blank'>Link</a></div><div id='project'> Project Name: ilastik/ilastik</div><div id='commit'> Commit Name: fe073644f6a8f37e9ce57df903bf12b560690fc3</div><div id='time'> Time: 2012-09-14</div><div id='author'> Author: christoph.straehle@iwr.uni-heidelberg.de</div><div id='file'> File Name: lazyflow/operators/obsolete/classifierOperators.py</div><div id='class'> Class Name: OpPredictRandomForest</div><div id='method'> Method Name: execute</div><BR><BR><div id='link'><a href='https://github.com/brightmart/text_classification/commit/cfeef8cb854a24371f0ef5d7d9c68f23675c97ee#diff-49ae38eabf4fb612be0c092bc5ceb2e77fce8459b535fa07b36f92669a404f5bL42' target='_blank'>Link</a></div><div id='project'> Project Name: brightmart/text_classification</div><div id='commit'> Commit Name: cfeef8cb854a24371f0ef5d7d9c68f23675c97ee</div><div id='time'> Time: 2017-07-06</div><div id='author'> Author: brightmart@hotmail.com</div><div id='file'> File Name: a2_attention_between_enc_dec.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: test</div><BR>