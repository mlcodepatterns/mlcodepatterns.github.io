<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        callbacks.on_train_begin()

        nb_batches = int(math.ceil(inputs[0].size(0) / batch_size))
        <a id="change">for epoch_idx in range(nb_epoch):
            epoch_logs = {
                &quotnb_batches&quot: nb_batches,
                &quotnb_epoch&quot: nb_epoch,
                &quothas_validation_data&quot: has_validation_data
            }
            callbacks.on_epoch_begin(epoch_idx, epoch_logs)

            for batch_idx in range(nb_batches):
                batch_logs = {&quotbatch_idx&quot: batch_idx}  
                callbacks.on_batch_begin(batch_idx, batch_logs) 

                input_batch = [Variable(x[batch_idx*batch_size:(batch_idx+1)*batch_size]) for x in inputs]
                if has_target:
                    target_batch = [Variable(y[batch_idx*batch_size:(batch_idx+1)*batch_size]) for y in targets]

                if cuda_device &gt; -1:
                    input_batch = [ins.cuda(cuda_device) for ins in input_batch]
                    if has_target:
                        target_batch = [targs.cuda(cuda_device) for targs in target_batch]

                batch_logs[&quotbatch_samples&quot] = len(input_batch[0])

                &#47&#47&#47&#47 ZERO GRAD AND FORWARD PASS
                self._optimizer.zero_grad()
                outputs = self(*input_batch)

                if not isinstance(outputs, (list,tuple)):
                    outputs = [outputs]
                if has_target:
                    loss = self._loss_fns[0](outputs[0], target_batch[0])
                    for loss_idx in range(1,nb_targets):
                        if self._has_multiple_loss_fns:
                            loss += self._loss_fns[loss_idx](outputs[loss_idx], target_batch[loss_idx])
                        else:
                            loss += self._loss_fns[0](outputs[loss_idx], target_batch[loss_idx])
                else:
                    loss = self._loss_fns[0](outputs[0])
                    for loss_idx in range(1,nb_targets):
                        if self._has_multiple_loss_fns:
                            loss += self._loss_fns[loss_idx](outputs[loss_idx])
                        else:
                            loss += self._loss_fns[0](outputs[loss_idx])
                
                if self._has_regularizers:
                    regularizer_loss = regularizers(self)
                    loss += regularizer_loss
                    batch_logs[&quotregularizer_loss&quot] = regularizer_loss

                if self._has_constraints and constraints.has_lagrangian:
                    constraint_loss = constraints(self)
                    loss += constraint_loss
                    batch_logs[&quotconstraint_loss&quot] = constraint_loss

                batch_logs[&quotloss&quot] = loss.data[0]

                if self._has_metrics:
                    metric_logs = metrics(outputs[0], target_batch[0])
                    batch_logs.update(metric_logs)

                &#47&#47 BACKWARD PASS AND OPTIMIZER STEP
                loss.backward()
                self._optimizer.step()

                callbacks.on_batch_end(batch_idx, batch_logs)
                if self._has_constraints:
                    constraints.on_batch_end(batch_idx)

            if has_validation_data:
                val_loss = self.evaluate(*validation_data, 
                                         batch_size=batch_size,
                                         cuda_device=cuda_device)
                if self._has_metrics:
                    val_loss, val_metric_logs = val_loss
                    epoch_logs.update(val_metric_logs)
                epoch_logs[&quotval_loss&quot] = val_loss
                self.history.batch_metrics[&quotval_loss&quot] = val_loss
            
            &#47&#47 END OF EPOCH
            epoch_logs.update(self.history.batch_metrics)
            if self._has_metrics:
                epoch_logs.update(metric_logs)

            callbacks.on_epoch_end(epoch_idx, epoch_logs)

            if self._has_constraints:
                constraints.on_epoch_end(epoch_idx)
            if self._has_metrics:
                metrics.reset()
            if self._stop_training:
                break

       </a> callbacks.on_train_end()

    def fit_loader(self, 
                   loader, </code></pre><h3>After Change</h3><pre><code class='java'>
            metrics = MetricsModule(self._metrics)

        &#47&#47&#47&#47 create callbacks
        <a id="change">with TQDM() as pbar:
            progressbar = []
            if verbose &gt; 0:
                progressbar = [pbar]
            callbacks = CallbackModule(self._callbacks + progressbar)
            callbacks.set_model(self)

            callbacks.on_train_begin()

            nb_batches = int(math.ceil(inputs[0].size(0) / batch_size))
            for epoch_idx in range(nb_epoch):
                epoch_logs = {
                    &quotnb_batches&quot: nb_batches,
                    &quotnb_epoch&quot: nb_epoch,
                    &quothas_validation_data&quot: has_validation_data
                }
                callbacks.on_epoch_begin(epoch_idx, epoch_logs)

                for batch_idx in range(nb_batches):
                    batch_logs = {&quotbatch_idx&quot: batch_idx}  
                    callbacks.on_batch_begin(batch_idx, batch_logs) 

                    input_batch = [Variable(x[batch_idx*batch_size:(batch_idx+1)*batch_size]) for x in inputs]
                    if has_target:
                        target_batch = [Variable(y[batch_idx*batch_size:(batch_idx+1)*batch_size]) for y in targets]

                    if cuda_device &gt; -1:
                        input_batch = [ins.cuda(cuda_device) for ins in input_batch]
                        if has_target:
                            target_batch = [targs.cuda(cuda_device) for targs in target_batch]

                    batch_logs[&quotbatch_samples&quot] = len(input_batch[0])

                    &#47&#47&#47&#47 ZERO GRAD AND FORWARD PASS
                    self._optimizer.zero_grad()
                    outputs = self(*input_batch)

                    if not isinstance(outputs, (list,tuple)):
                        outputs = [outputs]
                    if has_target:
                        loss = self._loss_fns[0](outputs[0], target_batch[0])
                        for loss_idx in range(1,nb_targets):
                            if self._has_multiple_loss_fns:
                                loss += self._loss_fns[loss_idx](outputs[loss_idx], target_batch[loss_idx])
                            else:
                                loss += self._loss_fns[0](outputs[loss_idx], target_batch[loss_idx])
                    else:
                        loss = self._loss_fns[0](outputs[0])
                        for loss_idx in range(1,nb_targets):
                            if self._has_multiple_loss_fns:
                                loss += self._loss_fns[loss_idx](outputs[loss_idx])
                            else:
                                loss += self._loss_fns[0](outputs[loss_idx])
                    
                    if self._has_regularizers:
                        regularizer_loss = regularizers(self)
                        loss += regularizer_loss
                        batch_logs[&quotregularizer_loss&quot] = regularizer_loss

                    if self._has_constraints and constraints.has_lagrangian:
                        constraint_loss = constraints(self)
                        loss += constraint_loss
                        batch_logs[&quotconstraint_loss&quot] = constraint_loss

                    batch_logs[&quotloss&quot] = loss.data[0]

                    if self._has_metrics:
                        metric_logs = metrics(outputs[0], target_batch[0])
                        batch_logs.update(metric_logs)

                    &#47&#47 BACKWARD PASS AND OPTIMIZER STEP
                    loss.backward()
                    self._optimizer.step()

                    callbacks.on_batch_end(batch_idx, batch_logs)
                    if self._has_constraints:
                        constraints.on_batch_end(batch_idx)

                if has_validation_data:
                    val_loss = self.evaluate(*validation_data, 
                                             batch_size=batch_size,
                                             cuda_device=cuda_device)
                    if self._has_metrics:
                        val_loss, val_metric_logs = val_loss
                        epoch_logs.update(val_metric_logs)
                    epoch_logs[&quotval_loss&quot] = val_loss
                    self.history.batch_metrics[&quotval_loss&quot] = val_loss
                
                &#47&#47 END OF EPOCH
                epoch_logs.update(self.history.batch_metrics)
                if self._has_metrics:
                    epoch_logs.update(metric_logs)

                callbacks.on_epoch_end(epoch_idx, epoch_logs)

                if self._has_constraints:
                    constraints.on_epoch_end(epoch_idx)
                if self._has_metrics:
                    metrics.reset()
                if self._stop_training:
                    break

            callbacks.on_train_end()

   </a> def fit_loader(self, 
                   loader, 
                   val_loader=None, 
                   nb_epoch=100,</code></pre>