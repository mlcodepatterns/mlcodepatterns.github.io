<html><h3>570d9a2b06fd6269c930d7fddf38bc60b212ebee,official/nlp/modeling/layers/attention.py,CachedAttention,call,#CachedAttention#Any#Any#Any#Any#,487
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
           attention_mask=None,
           cache=None,
           decode_loop_step=None):
    <a id="change">from_tensor = inputs[0]</a>
    <a id="change">to_tensor = inputs[1]</a>

    &#47&#47 Scalar dimensions referenced here:
    &#47&#47   B = batch size (number of sequences)
    &#47&#47   F = `from_tensor` sequence length
    &#47&#47   T = `to_tensor` sequence length
    &#47&#47   N = `num_attention_heads`
    &#47&#47   H = `size_per_head`
    &#47&#47 `query_tensor` = [B, F, N ,H]
    query_tensor = self._query_dense(from_tensor)

    &#47&#47 `key_tensor` = [B, T, N, H]
    key_tensor = self._key_dense(to_tensor)

    &#47&#47 `value_tensor` = [B, T, N, H]
    value_tensor = self._value_dense(to_tensor)

    if cache:
      key_tensor, value_tensor = self._update_cache(key_tensor, value_tensor,
                                                    cache, decode_loop_step)

    &#47&#47 Take the dot product between "query" and "key" to get the raw
    &#47&#47 attention scores.
    <a id="change">attention_scores = tf.einsum(self._dot_product_equation, key_tensor,
                                 query_tensor)</a>
    <a id="change">attention_scores = tf.multiply(attention_scores,
                                   1.0 / math.sqrt(float(self._key_size)))</a>

    &#47&#47 Normalize the attention scores to probabilities.
    &#47&#47 `attention_scores` = [B, N, F, T]
    attention_scores = self._masked_softmax(attention_scores, attention_mask)

    &#47&#47 This is actually dropping out entire tokens to attend to, which might
    &#47&#47 seem a bit unusual, but is taken from the original Transformer paper.
    attention_scores = self._dropout_layer(attention_scores)
    &#47&#47 `context_layer` = [B, F, N, H]
    <a id="change">attention_output = tf.einsum(self._combine_equation, attention_scores,
                                 value_tensor)</a>
    attention_output = self._output_dense(attention_output)
    if self._return_attention_scores:
      return attention_output, attention_scores, cache
    return attention_output, cache</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 `value` = [B, S, N, H]
    value = self._value_dense(value)

    <a id="change">attention_output</a>, attention_scores = self.compute_attention(
        query, key, value, attention_mask)
    attention_output = self._output_dense(attention_output)
</code></pre><img src="333651809.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/570d9a2b06fd6269c930d7fddf38bc60b212ebee#diff-768369de61e88e252a9df1743b577d4142f656e2338573a96dde6be73b708e63L408' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: 570d9a2b06fd6269c930d7fddf38bc60b212ebee</div><div id='time'> Time: 2020-07-21</div><div id='author'> Author: hongkuny@google.com</div><div id='file'> File Name: official/nlp/modeling/layers/attention.py</div><div id='class'> Class Name: CachedAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/jfkirk/tensorrec/commit/ac35acad4f4311aef39aea039902cd01775eb70e#diff-6854abd25299e4f3f634fd4a9539e53db160d4444e6e81016b0f15fb16e6c097L84' target='_blank'>Link</a></div><div id='project'> Project Name: jfkirk/tensorrec</div><div id='commit'> Commit Name: ac35acad4f4311aef39aea039902cd01775eb70e</div><div id='time'> Time: 2018-03-01</div><div id='author'> Author: jcauterucciojr@gmail.com</div><div id='file'> File Name: tensorrec/eval.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: ndcg_at_k</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/models/commit/36101ab4095065a4196ff4f6437e94f0d91df4e9#diff-768369de61e88e252a9df1743b577d4142f656e2338573a96dde6be73b708e63L487' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/models</div><div id='commit'> Commit Name: 36101ab4095065a4196ff4f6437e94f0d91df4e9</div><div id='time'> Time: 2020-07-21</div><div id='author'> Author: hongkuny@google.com</div><div id='file'> File Name: official/nlp/modeling/layers/attention.py</div><div id='class'> Class Name: CachedAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/jfkirk/tensorrec/commit/466ef0befceaad2be3244efd724c76c90869b17d#diff-6854abd25299e4f3f634fd4a9539e53db160d4444e6e81016b0f15fb16e6c097L84' target='_blank'>Link</a></div><div id='project'> Project Name: jfkirk/tensorrec</div><div id='commit'> Commit Name: 466ef0befceaad2be3244efd724c76c90869b17d</div><div id='time'> Time: 2018-03-02</div><div id='author'> Author: jcauterucciojr@gmail.com</div><div id='file'> File Name: tensorrec/eval.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: ndcg_at_k</div><BR>