<html><h3>65af003602d38cd1f324e0bcfce40c7f0987eec0,main.py,,main,#,41
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    rollouts.obs[0].copy_(obs)

    &#47&#47 These variables are used to compute average rewards for all processes.
    episode_rewards = <a id="change">torch.zeros([args.num_processes, 1])</a>
    final_rewards = torch.zeros([args.num_processes, 1])

    rollouts.to(device)

    start = time.time()
    for j in range(num_updates):
        for step in range(args.num_steps):
            &#47&#47 Sample actions
            with torch.no_grad():
                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(
                        rollouts.obs[step],
                        rollouts.recurrent_hidden_states[step],
                        rollouts.masks[step])

            &#47&#47 Obser reward and next obs
            obs, reward, done, info = envs.step(action)
            <a id="change">episode_rewards += reward</a>

            &#47&#47 If done then clean the history of observations.
            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])
            final_rewards *= masks</code></pre><h3>After Change</h3><pre><code class='java'>
            obs, reward, done, infos = envs.step(action)

            for info in infos:
                if &quotepisode&quot in <a id="change">info.keys()</a>:
                    episode_rewards.append(info[&quotepisode&quot][&quotr&quot])

            &#47&#47 If done then clean the history of observations.</code></pre><img src="187639119.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/commit/65af003602d38cd1f324e0bcfce40c7f0987eec0#diff-b10564ab7d2c520cdd0243874879fb0a782862c3c902ab535faabe57d5a505e1L84' target='_blank'>Link</a></div><div id='project'> Project Name: ikostrikov/pytorch-a2c-ppo-acktr</div><div id='commit'> Commit Name: 65af003602d38cd1f324e0bcfce40c7f0987eec0</div><div id='time'> Time: 2018-09-05</div><div id='author'> Author: ikostrikov@gmail.com</div><div id='file'> File Name: main.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: main</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/1113670b794b23c39cf23a9a532f866d08b63fdb#diff-db2ca863dc0d67bc37f78e1725f8fbf27fb734f577f82f2a900ba1616ef603daL9' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 1113670b794b23c39cf23a9a532f866d08b63fdb</div><div id='time'> Time: 2018-06-30</div><div id='author'> Author: kristian.hartikainen@gmail.com</div><div id='file'> File Name: softlearning/replay_buffers/simple_replay_buffer.py</div><div id='class'> Class Name: SimpleReplayBuffer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/idaholab/raven/commit/078b2512f26bfb20aa3ecbb6a065008bf7d45753#diff-1f67df234ed14284da5318a97a1f93d779b88678a4af17046473a0e09b0e9a00L309' target='_blank'>Link</a></div><div id='project'> Project Name: idaholab/raven</div><div id='commit'> Commit Name: 078b2512f26bfb20aa3ecbb6a065008bf7d45753</div><div id='time'> Time: 2020-06-30</div><div id='author'> Author: mohammad.abdo@inl.gov</div><div id='file'> File Name: framework/Optimizers/GeneticAlgorithm.py</div><div id='class'> Class Name: GeneticAlgorithm</div><div id='method'> Method Name: _useRealization</div><BR>