<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                yield batch

            cur_dataset.examples = None
            <a id="change">gc.collect()</a>
            del cur_dataset
            gc.collect()

</code></pre><h3>After Change</h3><pre><code class='java'>
        paths = self._paths
        if self.is_train and self.repeat:
            &#47&#47 Cycle through the shards indefinitely.
            <a id="change">paths = cycle(paths)</a>
        for path in paths:
            for batch in self._iter_dataset(path):
                yield batch
                num_batches += 1
        if self.is_train and not self.repeat and \
           num_batches % self.num_batches_multiple != 0:
            &#47&#47 When the dataset is not repeated, we might need to ensure that
            &#47&#47 the number of returned batches is the multiple of a given value.
            &#47&#47 This is important for multi GPU training to ensure that all
            &#47&#47 workers have the same number of batches to process.
            for path in paths:
                <a id="change">for batch in self._iter_dataset(path):
                    yield batch
                    num_batches += 1
                    if num_batches % self.num_batches_multiple == 0:
                        return


</a>def max_tok_len(new, count, sofar):
    
    In token batching scheme, the number of sequences is limited
    such that the total number of src/tgt tokens (including padding)</code></pre>