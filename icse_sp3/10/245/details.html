<html><h3>5b48f9a9c097d26d395873044ceaa1a0b886682a,solutionbox/code_free_ml/mltoolbox/code_free_ml/analyze.py,,run_cloud_analysis,#Any#Any#Any#Any#Any#,161
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

  numerical_vocab_stats = {}

  <a id="change">for col_schema in schema:
    col_name = col_schema[&quotname&quot]
    col_type = col_schema[&quottype&quot].lower()
    transform = features[col_name][&quottransform&quot]

    &#47&#47 Map the target transfrom into one_hot or identity.
    if transform == constant.TARGET_TRANSFORM:
      if col_type == constant.STRING_SCHEMA:
        transform = constant.ONE_HOT_TRANSFORM
      elif col_type in constant.NUMERIC_SCHEMA:
        transform = constant.IDENTITY_TRANSFORM
      else:
        raise ValueError(&quotUnknown schema type&quot)

    if transform in (constant.TEXT_TRANSFORMS + constant.CATEGORICAL_TRANSFORMS):
      if transform in constant.TEXT_TRANSFORMS:
        &#47&#47 Split strings on space, then extract labels and how many rows each
        &#47&#47 token is in. This is done by making two temp tables:
        &#47&#47   SplitTable: each text row is made into an array of strings. The
        &#47&#47       array may contain repeat tokens
        &#47&#47   TokenTable: SplitTable with repeated tokens removed per row.
        &#47&#47 Then to flatten the arrays, TokenTable has to be joined with itself.
        &#47&#47 See the sections &quotFlattening Arrays&quot and &quotFiltering Arrays&quot at
        &#47&#47 https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays
        sql = (&quotWITH SplitTable AS &quot
               &quot         (SELECT SPLIT({name}, \&quot \&quot) as token_array FROM {table}), &quot
               &quot     TokenTable AS &quot
               &quot         (SELECT ARRAY(SELECT DISTINCT x &quot
               &quot                       FROM UNNEST(token_array) AS x) AS unique_tokens_per_row &quot
               &quot          FROM SplitTable) &quot
               &quotSELECT token, COUNT(token) as token_count &quot
               &quotFROM TokenTable &quot
               &quotCROSS JOIN UNNEST(TokenTable.unique_tokens_per_row) as token &quot
               &quotWHERE LENGTH(token) &gt; 0 &quot
               &quotGROUP BY token &quot
               &quotORDER BY token_count DESC, token ASC&quot).format(name=col_name,
                                                              table=table_name)
      else:
        &#47&#47 Extract label and frequency
        sql = (&quotSELECT {name} as token, count(*) as count &quot
               &quotFROM {table} &quot
               &quotWHERE {name} IS NOT NULL &quot
               &quotGROUP BY {name} &quot
               &quotORDER BY count DESC, token ASC&quot).format(name=col_name,
                                                        table=table_name)

      df = _execute_sql(sql, table)

      &#47&#47 Save the vocab
      string_buff = six.StringIO()
      df.to_csv(string_buff, index=False, header=False)
      file_io.write_string_to_file(
          os.path.join(output_dir, constant.VOCAB_ANALYSIS_FILE % col_name),
          string_buff.getvalue())
      numerical_vocab_stats[col_name] = {&quotvocab_size&quot: len(df)}

      &#47&#47 free memeory
      del string_buff
      del df
    elif transform in constant.NUMERIC_TRANSFORMS:
      &#47&#47 get min/max/average
      sql = (&quotSELECT max({name}) as max_value, min({name}) as min_value, &quot
             &quotavg({name}) as avg_value from {table}&quot).format(name=col_name,
                                                             table=table_name)
      df = _execute_sql(sql, table)
      numerical_vocab_stats[col_name] = {&quotmin&quot: df.iloc[0][&quotmin_value&quot],
                                         &quotmax&quot: df.iloc[0][&quotmax_value&quot],
                                         &quotmean&quot: df.iloc[0][&quotavg_value&quot]}
    elif transform == constant.IMAGE_TRANSFORM:
      pass
    elif transform == constant.KEY_TRANSFORM:
      pass
    else:
      raise ValueError(&quotUnknown transform %s&quot % transform)

  &#47&#47 get num examples
 </a> sql = &quotSELECT count(*) as num_examples from {table}&quot.format(table=table_name)
  df = _execute_sql(sql, table)
  num_examples = df.iloc[0][&quotnum_examples&quot]
</code></pre><h3>After Change</h3><pre><code class='java'>

  &#47&#47 Make a copy of inverted_features and update the target transform to be
  &#47&#47 identity or one hot depending on the schema.
  <a id="change">inverted_features_target = copy.deepcopy(inverted_features)</a>
  <a id="change">for name, transform_set in six.iteritems(inverted_features_target):
    if transform_set == set([constant.TARGET_TRANSFORM]):
      target_schema = next(col[&quottype&quot].lower() for col in schema if col[&quotname&quot] == name)
      if target_schema in constant.NUMERIC_SCHEMA:
        inverted_features_target[name] = {constant.IDENTITY_TRANSFORM}
      else:
        inverted_features_target[name] = {constant.ONE_HOT_TRANSFORM}

 </a> numerical_vocab_stats = {}
  <a id="change">for col_name, transform_set in six.iteritems(inverted_features_target):
    &#47&#47 All transforms in transform_set require the same analysis. So look
    &#47&#47 at the first transform.
    transform_name = next(iter(transform_set))
    if (transform_name in constant.CATEGORICAL_TRANSFORMS or
       transform_name in constant.TEXT_TRANSFORMS):
      if transform_name in constant.TEXT_TRANSFORMS:
        &#47&#47 Split strings on space, then extract labels and how many rows each
        &#47&#47 token is in. This is done by making two temp tables:
        &#47&#47   SplitTable: each text row is made into an array of strings. The
        &#47&#47       array may contain repeat tokens
        &#47&#47   TokenTable: SplitTable with repeated tokens removed per row.
        &#47&#47 Then to flatten the arrays, TokenTable has to be joined with itself.
        &#47&#47 See the sections &quotFlattening Arrays&quot and &quotFiltering Arrays&quot at
        &#47&#47 https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays
        sql = (&quotWITH SplitTable AS &quot
               &quot         (SELECT SPLIT({name}, \&quot \&quot) as token_array FROM {table}), &quot
               &quot     TokenTable AS &quot
               &quot         (SELECT ARRAY(SELECT DISTINCT x &quot
               &quot                       FROM UNNEST(token_array) AS x) AS unique_tokens_per_row &quot
               &quot          FROM SplitTable) &quot
               &quotSELECT token, COUNT(token) as token_count &quot
               &quotFROM TokenTable &quot
               &quotCROSS JOIN UNNEST(TokenTable.unique_tokens_per_row) as token &quot
               &quotWHERE LENGTH(token) &gt; 0 &quot
               &quotGROUP BY token &quot
               &quotORDER BY token_count DESC, token ASC&quot).format(name=col_name,
                                                              table=table_name)
      else:
        &#47&#47 Extract label and frequency
        sql = (&quotSELECT {name} as token, count(*) as count &quot
               &quotFROM {table} &quot
               &quotWHERE {name} IS NOT NULL &quot
               &quotGROUP BY {name} &quot
               &quotORDER BY count DESC, token ASC&quot).format(name=col_name,
                                                        table=table_name)

      df = _execute_sql(sql, table)

      &#47&#47 Save the vocab
      string_buff = six.StringIO()
      df.to_csv(string_buff, index=False, header=False)
      file_io.write_string_to_file(
          os.path.join(output_dir, constant.VOCAB_ANALYSIS_FILE % col_name),
          string_buff.getvalue())
      numerical_vocab_stats[col_name] = {&quotvocab_size&quot: len(df)}

      &#47&#47 free memeory
      del string_buff
      del df
    elif transform_name in constant.NUMERIC_TRANSFORMS:
      &#47&#47 get min/max/average
      sql = (&quotSELECT max({name}) as max_value, min({name}) as min_value, &quot
             &quotavg({name}) as avg_value from {table}&quot).format(name=col_name,
                                                             table=table_name)
      df = _execute_sql(sql, table)
      numerical_vocab_stats[col_name] = {&quotmin&quot: df.iloc[0][&quotmin_value&quot],
                                         &quotmax&quot: df.iloc[0][&quotmax_value&quot],
                                         &quotmean&quot: df.iloc[0][&quotavg_value&quot]}

  &#47&#47 get num examples
 </a> sql = &quotSELECT count(*) as num_examples from {table}&quot.format(table=table_name)
  df = _execute_sql(sql, table)
  num_examples = df.iloc[0][&quotnum_examples&quot]
</code></pre><img src="3182880.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/googledatalab/pydatalab/commit/5b48f9a9c097d26d395873044ceaa1a0b886682a#diff-b28df4a0dbd5cb118109e04660bf7d1f3e094b526fe4b73d77c8538d24f912fdL204' target='_blank'>Link</a></div><div id='project'> Project Name: googledatalab/pydatalab</div><div id='commit'> Commit Name: 5b48f9a9c097d26d395873044ceaa1a0b886682a</div><div id='time'> Time: 2017-06-14</div><div id='author'> Author: brandondutra@google.com</div><div id='file'> File Name: solutionbox/code_free_ml/mltoolbox/code_free_ml/analyze.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: run_cloud_analysis</div><BR><BR><div id='link'><a href='https://github.com/drckf/paysage/commit/2921cade4187f926f89b35a4f56a9cb54fa2e2f3#diff-c787359c6009fe7ebbb4bebb5baf99461a20285d3e05f468043c103b21922a83L207' target='_blank'>Link</a></div><div id='project'> Project Name: drckf/paysage</div><div id='commit'> Commit Name: 2921cade4187f926f89b35a4f56a9cb54fa2e2f3</div><div id='time'> Time: 2017-03-17</div><div id='author'> Author: charleskennethfisher@gmail.com</div><div id='file'> File Name: paysage/optimizers.py</div><div id='class'> Class Name: RMSProp</div><div id='method'> Method Name: update</div><BR><BR><div id='link'><a href='https://github.com/googledatalab/pydatalab/commit/5b48f9a9c097d26d395873044ceaa1a0b886682a#diff-b28df4a0dbd5cb118109e04660bf7d1f3e094b526fe4b73d77c8538d24f912fdL311' target='_blank'>Link</a></div><div id='project'> Project Name: googledatalab/pydatalab</div><div id='commit'> Commit Name: 5b48f9a9c097d26d395873044ceaa1a0b886682a</div><div id='time'> Time: 2017-06-14</div><div id='author'> Author: brandondutra@google.com</div><div id='file'> File Name: solutionbox/code_free_ml/mltoolbox/code_free_ml/analyze.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: run_local_analysis</div><BR>