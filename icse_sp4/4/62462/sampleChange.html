<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      &#47&#47 TODO(petebu): Consider the case when all updates are None.
      if update is not None:
        optimizer_utils.check_same_dtype(update, parameter)
        <a id="change">m, v = self._get_or_create_moments(parameter)</a>
        learning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)
        beta1 = tf.cast(self.beta1, update.dtype.base_dtype)
        beta2 = tf.cast(self.beta2, update.dtype.base_dtype)
        epsilon = tf.cast(self.epsilon, update.dtype.base_dtype)</code></pre><h3>After Change</h3><pre><code class='java'>
        lengths, or have inconsistent types.
    
    optimizer_utils.check_updates_parameters(updates, parameters)
    <a id="change">self._initialize(parameters)</a>
    self.step.assign_add(1)
    for update, parameter, m, v in zip(updates, parameters, self.m, self.v):
      &#47&#47 TODO(petebu): Add support for sparse tensors.
      &#47&#47 TODO(petebu): Consider caching learning_rate cast.</code></pre>