<html><h3>4dd96193aaa69485ef183778f053fcfcbef457a2,sac/algos/base.py,RLAlgorithm,_train,#RLAlgorithm#Any#Any#Any#,72
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            gt.reset()
            gt.set_def_unique(False)

            <a id="change">for epoch in gt.timed_for(range(self._n_epochs + 1),
                                      save_itrs=True):
                logger.push_prefix(&quotEpoch &#47&#47%d | &quot % epoch)

                num_steps = int(np.ceil(self._epoch_length/self._control_interval))
                for t in range(num_steps):
                    iteration = t + epoch * self._epoch_length

                    action, _ = policy.get_action(observation)

                    for i in range(self._control_interval):
                        next_ob, reward, terminal, info = env.step(action)

                        path_length += 1
                        path_return += reward

                        if terminal or path_length &gt;= self._max_path_length:
                            break

                    self._pool.add_sample(
                        observation,
                        action,
                        reward,
                        terminal,
                        next_ob,
                    )

                    if terminal or path_length &gt;= self._max_path_length:
                        observation = env.reset()
                        policy.reset()
                        path_length = 0
                        max_path_return = max(max_path_return, path_return)
                        last_path_return = path_return

                        path_return = 0
                        n_episodes += 1

                    else:
                        observation = next_ob
                    gt.stamp(&quotsample&quot)

                    if self._pool.size &gt;= self._min_pool_size:
                        for i in range(self._n_train_repeat):
                            batch = self._pool.random_batch(self._batch_size)
                            self._do_training(iteration, batch)

                    gt.stamp(&quottrain&quot)

                self._evaluate(epoch)

                params = self.get_snapshot(epoch)
                logger.save_itr_params(epoch, params)
                times_itrs = gt.get_times().stamps.itrs

                eval_time = times_itrs[&quoteval&quot][-1] if epoch &gt; 1 else 0
                total_time = gt.get_times().total
                logger.record_tabular(&quottime-train&quot, times_itrs[&quottrain&quot][-1])
                logger.record_tabular(&quottime-eval&quot, eval_time)
                logger.record_tabular(&quottime-sample&quot, times_itrs[&quotsample&quot][-1])
                logger.record_tabular(&quottime-total&quot, total_time)
                logger.record_tabular(&quotepoch&quot, epoch)
                logger.record_tabular(&quotepisodes&quot, n_episodes)
                logger.record_tabular(&quotmax-path-return&quot, max_path_return)
                logger.record_tabular(&quotlast-path-return&quot, last_path_return)
                logger.record_tabular(&quotpool-size&quot, self._pool.size)

                logger.dump_tabular(with_prefix=False)
                logger.pop_prefix()

                gt.stamp(&quoteval&quot)

           </a> env.terminate()

    def _evaluate(self, epoch):
        Perform evaluation for the current policy.</code></pre><h3>After Change</h3><pre><code class='java'>
            gt.reset()
            gt.set_def_unique(False)

            <a id="change">for epoch in gt.timed_for(range(self._n_epochs + 1),
                                      save_itrs=True):
                logger.push_prefix(&quotEpoch &#47&#47%d | &quot % epoch)

                num_steps = int(np.ceil(self._epoch_length/self._control_interval))
                for t in range(num_steps):
                    iteration = t + epoch * self._epoch_length

                    action, _ = policy.get_action(observation)

                    for i in range(self._control_interval):
                        next_ob, reward, terminal, info = env.step(action)

                        path_length += 1
                        path_return += reward

                        if terminal or path_length &gt;= self._max_path_length:
                            break

                    self._pool.add_sample(
                        observation,
                        action,
                        reward,
                        terminal,
                        next_ob,
                    )

                    if terminal or path_length &gt;= self._max_path_length:
                        observation = env.reset()
                        policy.reset()
                        path_length = 0
                        max_path_return = max(max_path_return, path_return)
                        last_path_return = path_return

                        path_return = 0
                        n_episodes += 1

                    else:
                        observation = next_ob
                    gt.stamp(&quotsample&quot)

                    for i in range(self._n_train_repeat):
                        self._do_training(
                            itr=t + epoch * self._epoch_length,
                            batch=self.sampler.random_batch())
                    gt.stamp(&quottrain&quot)

                self._evaluate(epoch)

                params = self.get_snapshot(epoch)
                logger.save_itr_params(epoch, params)
                times_itrs = gt.get_times().stamps.itrs

                eval_time = times_itrs[&quoteval&quot][-1] if epoch &gt; 1 else 0
                total_time = gt.get_times().total
                logger.record_tabular(&quottime-train&quot, times_itrs[&quottrain&quot][-1])
                logger.record_tabular(&quottime-eval&quot, eval_time)
                logger.record_tabular(&quottime-sample&quot, times_itrs[&quotsample&quot][-1])
                logger.record_tabular(&quottime-total&quot, total_time)
                logger.record_tabular(&quotepoch&quot, epoch)

                self.sampler.log_diagnostics()

                logger.dump_tabular(with_prefix=False)
                logger.pop_prefix()

                gt.stamp(&quoteval&quot)

           </a> self.sampler.terminate()

    def _evaluate(self, epoch):
        Perform evaluation for the current policy.</code></pre><img src="3220465.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/4dd96193aaa69485ef183778f053fcfcbef457a2#diff-70592caf539d6d11e11da7482e30bba889831f5a79b8187eb5e7a22f72469fa9L63' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 4dd96193aaa69485ef183778f053fcfcbef457a2</div><div id='time'> Time: 2018-05-22</div><div id='author'> Author: azhou42@berkeley.edu</div><div id='file'> File Name: sac/algos/base.py</div><div id='class'> Class Name: RLAlgorithm</div><div id='method'> Method Name: _train</div><BR><BR><div id='link'><a href='https://github.com/CyberZHG/keras-bert/commit/05896d40da5307b85cd7f82c5981586f528996af#diff-89ef5eb015b9b05c36034be360520ce8046b04ba5bf8f5d7a5f84a21563ebbeaL77' target='_blank'>Link</a></div><div id='project'> Project Name: CyberZHG/keras-bert</div><div id='commit'> Commit Name: 05896d40da5307b85cd7f82c5981586f528996af</div><div id='time'> Time: 2018-10-25</div><div id='author'> Author: CyberZHG@gmail.com</div><div id='file'> File Name: tests/layers/test_feed_forward.py</div><div id='class'> Class Name: TestAttention</div><div id='method'> Method Name: test_fit</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/445387698dad8bcdccb53b7708aba564c1a5c319#diff-d0aecab0bd58616190c092b13c93719070f8518f9d99cfc8b128f2adfbc4b25cL160' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 445387698dad8bcdccb53b7708aba564c1a5c319</div><div id='time'> Time: 2019-02-01</div><div id='author'> Author: henryzhangbh@outlook.com</div><div id='file'> File Name: examples/development/main.py</div><div id='class'> Class Name: ExperimentRunner</div><div id='method'> Method Name: _restore</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/5d350985ce3425d93eb0ddae565d14060e811bd6#diff-956fb9db83e3eaad4838411e36081b232b9fe6d72e0cf1ec1850da76a3f982d7L12' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 5d350985ce3425d93eb0ddae565d14060e811bd6</div><div id='time'> Time: 2018-11-23</div><div id='author'> Author: hartikainen@berkeley.edu</div><div id='file'> File Name: softlearning/misc/nn.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: feedforward_model</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/fe2fe5fc5eefe738d44833eba95d8ae246c196db#diff-d0aecab0bd58616190c092b13c93719070f8518f9d99cfc8b128f2adfbc4b25cL147' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: fe2fe5fc5eefe738d44833eba95d8ae246c196db</div><div id='time'> Time: 2019-02-01</div><div id='author'> Author: hartikainen@berkeley.edu</div><div id='file'> File Name: examples/development/main.py</div><div id='class'> Class Name: ExperimentRunner</div><div id='method'> Method Name: _restore</div><BR>