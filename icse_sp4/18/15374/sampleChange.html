<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
  return DPGaussianOptimizerClass

&#47&#47 Compatibility with tf 1 and 2 APIs
<a id="change">try:
  AdagradOptimizer = tf.compat.v1.train.AdagradOptimizer
  AdamOptimizer = tf.compat.v1.train.AdamOptimizer
  GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer
except:  &#47&#47 pylint: disable=bare-except
  AdagradOptimizer = tf.optimizers.Adagrad
  AdamOptimizer = tf.optimizers.Adam
  GradientDescentOptimizer = tf.optimizers.SGD  &#47&#47 pylint: disable=invalid-name

</a>DPAdagradOptimizer = make_optimizer_class(AdagradOptimizer)
DPAdamOptimizer = make_optimizer_class(AdamOptimizer)
DPGradientDescentOptimizer = make_optimizer_class(GradientDescentOptimizer)
</code></pre><h3>After Change</h3><pre><code class='java'>

  return DPGaussianOptimizerClass

<a id="change">if LooseVersion(tf.__version__) &lt; LooseVersion(&quot2.0.0&quot):
  AdagradOptimizer = tf.train.AdagradOptimizer
  AdamOptimizer = tf.train.AdamOptimizer
  GradientDescentOptimizer = tf.train.GradientDescentOptimizer
else:
  AdagradOptimizer = tf.optimizers.Adagrad
  AdamOptimizer = tf.optimizers.Adam
  GradientDescentOptimizer = tf.optimizers.SGD  &#47&#47 pylint: disable=invalid-name

</a>DPAdagradOptimizer = make_optimizer_class(AdagradOptimizer)
DPAdamOptimizer = make_optimizer_class(AdamOptimizer)
DPGradientDescentOptimizer = make_optimizer_class(GradientDescentOptimizer)
</code></pre>