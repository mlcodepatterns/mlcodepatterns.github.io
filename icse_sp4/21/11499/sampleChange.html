<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                return x

        m = M().eval()
        <a id="change">mp = prepare_fx(m, {&quot&quot: torch.quantization.default_qconfig})</a>
        mp(torch.randn(4, 4))
        &#47&#47 TODO(future PR): prevent the need for copying here, we can copy the
        &#47&#47 modules but should reuse the underlying tensors
        <a id="change">mp_copy = copy.deepcopy(mp)</a>
        <a id="change">mq = convert_fx(mp_copy)</a>

        <a id="change">mp_shadows_mq = add_shadow_loggers(&quotfp32_prepared&quot, mp, &quotint8&quot, mq, OutputLogger)</a>

        &#47&#47 TODO(before land): test both scripted and non-scripted
        mp_shadows_mq = torch.jit.script(mp_shadows_mq)

        &#47&#47 calibrate
        input_fp32 = torch.randn(4, 4)
        mp_shadows_mq(input_fp32)

        &#47&#47 check activation result correctness
        <a id="change">act_compare_dict = extract_shadow_logger_info(
            mp_shadows_mq, OutputLogger)</a>
        self.assertTrue(len(act_compare_dict) == 2)
        self.assert_ns_compare_dict_valid(act_compare_dict)

    @skipIfNoFBGEMM</code></pre><h3>After Change</h3><pre><code class='java'>
                return x

        m = M().eval()
        <a id="change">self._test_match_shadow_activations(
            m, (torch.randn(4, 4),), results_len=2)</a>

    @skipIfNoFBGEMM
    def test_add_shadow_loggers_multiple_dtype_casts(self):
        </code></pre>