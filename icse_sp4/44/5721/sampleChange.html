<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    for i in range(args.max_iter):
        &#47&#47 Save parameters
        if i % args.model_save_interval == 0:
            <a id="change">nn.save_parameters(os.path.join(
                args.model_save_path, &quotparam_%06d.h5&quot % i))</a>

        &#47&#47 Validation
        if i % args.val_interval == 0 and i != 0:
</code></pre><h3>After Change</h3><pre><code class='java'>
    v_e = F.mean(F.top_n_error(v_pred2, v_model.label))

    &#47&#47 Save_nnp_Epoch0
    <a id="change">contents = save_nnp({&quotx&quot: v_model.image}, {
                        &quoty&quot: v_model.pred}, args.batch_size)</a>
    <a id="change">save.save(os.path.join(args.model_save_path,
                           &quotImagenet_result_epoch0.nnp&quot), contents)</a>

    &#47&#47 Create Solver.
    solver = S.Momentum(args.learning_rate, 0.9)
    solver.set_parameters(nn.get_parameters())

    <a id="change">start_point = 0</a>
    <a id="change">if args.checkpoint is not None:
        &#47&#47 load weights and solver state info from specified checkpoint file.
        start_point = load_checkpoint(args.checkpoint, solver)

    &#47&#47 Create monitor.
   </a> import nnabla.monitor as M
    monitor = M.Monitor(args.monitor_path)
    monitor_loss = M.MonitorSeries("Training loss", monitor, interval=10)
    monitor_err = M.MonitorSeries("Training error", monitor, interval=10)
    monitor_vloss = M.MonitorSeries("Validation loss", monitor, interval=10)
    monitor_verr = M.MonitorSeries("Validation error", monitor, interval=10)
    monitor_time = M.MonitorTimeElapsed("Training time", monitor, interval=10)
    monitor_vtime = M.MonitorTimeElapsed(
        "Validation time", monitor, interval=10)

    &#47&#47 Training loop.
    for i in range(start_point, args.max_iter):
        &#47&#47 Save parameters
        if i % args.model_save_interval == 0:
            &#47&#47 save checkpoint file
            <a id="change">save_checkpoint(args.model_save_path, i, solver)</a>

        &#47&#47 Validation
        if i % args.val_interval == 0 and i != 0:

            &#47&#47 Clear all intermediate memory to save memory.
            &#47&#47 t_model.loss.clear_recursive()

            l = 0.0
            e = 0.0
            for j in range(args.val_iter):
                images, labels = vdata.next()
                v_model.image.d = images
                v_model.label.d = labels
                v_model.image.data.cast(np.uint8, ctx)
                v_model.label.data.cast(np.int32, ctx)
                v_model.loss.forward(clear_buffer=True)
                v_e.forward(clear_buffer=True)
                l += v_model.loss.d
                e += v_e.d
            monitor_vloss.add(i, l / args.val_iter)
            monitor_verr.add(i, e / args.val_iter)
            monitor_vtime.add(i)

            &#47&#47 Clear all intermediate memory to save memory.
            &#47&#47 v_model.loss.clear_recursive()

        &#47&#47 Training
        l = 0.0
        e = 0.0
        solver.zero_grad()

        def accumulate_error(l, e, t_model, t_e):
            l += t_model.loss.d
            e += t_e.d
            return l, e

        &#47&#47 Gradient accumulation loop
        for j in range(args.accum_grad):
            images, labels = data.next()
            t_model.image.d = images
            t_model.label.d = labels
            t_model.image.data.cast(np.uint8, ctx)
            t_model.label.data.cast(np.int32, ctx)
            t_model.loss.forward(clear_no_need_grad=True)
            t_model.loss.backward(clear_buffer=True)  &#47&#47 Accumulating gradients
            t_e.forward(clear_buffer=True)
            l, e = accumulate_error(l, e, t_model, t_e)

        solver.weight_decay(args.weight_decay)
        solver.update()

        monitor_loss.add(i, l / args.accum_grad)
        monitor_err.add(i, e / args.accum_grad)
        monitor_time.add(i)

        &#47&#47 Learning rate decay at scheduled iter
        if i in args.learning_rate_decay_at:
            solver.set_learning_rate(solver.learning_rate() * 0.1)
    nn.save_parameters(os.path.join(args.model_save_path,
                                    &quotparam_%06d.h5&quot % args.max_iter))

    &#47&#47 Save_nnp
    <a id="change">contents = save_nnp({&quotx&quot: v_model.image}, {
                        &quoty&quot: v_model.pred}, args.batch_size)</a>
    <a id="change">save.save(os.path.join(args.model_save_path,
                           &quotImagenet_result.nnp&quot), contents)</a>


if __name__ == &quot__main__&quot:
    train()</code></pre>