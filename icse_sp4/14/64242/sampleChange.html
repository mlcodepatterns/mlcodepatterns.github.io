<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    model.eval()
    test_loss = 0
    correct = 0
    <a id="change">for data, target in test_loader:
        if use_cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)

        &#47&#47 sum up batch loss
        test_loss += F.nll_loss(output, target, size_average=False).data[0]
        &#47&#47 get the index of the max log-probability
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

   </a> test_loss /= len(test_loader.dataset)
    print(&quot\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&quot
          .format(test_loss, correct, len(test_loader.dataset),
                  100. * correct / len(test_loader.dataset)))</code></pre><h3>After Change</h3><pre><code class='java'>


def test():
    <a id="change">with torch.no_grad():
        model.eval()
        test_loss = 0
        correct = 0
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)

            &#47&#47 sum up batch loss
            test_loss += F.nll_loss(output, target, size_average=False).item()
            &#47&#47 get the index of the max log-probability
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

        test_loss /= len(test_loader.dataset)
        print(&quot\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&quot
              .format(test_loss, correct, len(test_loader.dataset),
                      100. * correct / len(test_loader.dataset)))

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 Visualizing the STN results
&#47&#47 ---------------------------
&#47&#47
&#47&#47 Now, we will inspect the results of our learned visual attention
&#47&#47 mechanism.
&#47&#47
&#47&#47 We define a small helper function in order to visualize the
&#47&#47 transformations while training.


</a>def convert_image_np(inp):
    Convert a Tensor to numpy image.
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])</code></pre>