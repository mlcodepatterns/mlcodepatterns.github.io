<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      self.assertEqual(mlp_transpose.layers[i].output_size,
                       mlp.layers[-1 - i].input_shape[1])

    <a id="change">data = np.random.rand(self.batch_size, self.input_size)</a>
    init = tf.global_variables_initializer()
    <a id="change">with self.test_session() as sess:
      sess.run(init)
      sess.run(mlp_transposed_output, feed_dict={input_to_mlp: data})

   </a> variables = mlp_transpose.get_variables()

    if use_bias:
      self.assertEqual(len(variables), len(self.output_sizes) * 2)</code></pre><h3>After Change</h3><pre><code class='java'>
    self.assertEqual(mlp_transpose.use_bias, mlp.use_bias)
    self.assertEqual(mlp_transpose.activate_final, mlp.activate_final)

    <a id="change">if not tf.executing_eagerly():
      if activate_final:
        self.assertEqual(mlp_transposed_output.op.type, "Relu")
      elif use_bias:
        self.assertEqual(mlp_transposed_output.op.type, "Add")
      else:
        self.assertEqual(mlp_transposed_output.op.type, "MatMul")

   </a> for i in range(0, len(mlp.layers)):
      self.assertEqual(mlp_transpose.layers[i].output_size,
                       mlp.layers[-1 - i].input_shape[1])
</code></pre>