<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                                   name=&quotq_acted&quot)

            &#47&#47 we calculate the loss as the mean squared error between actual observed rewards and expected rewards
            loss = <a id="change">tf.reduce_mean(tf.square(self.q_targets - q_values_actions_taken), name=&quotloss&quot)</a>

            if self.gradient_clipping is not None:
                grads_and_vars = self.optimizer.compute_gradients(loss)
                <a id="change">for idx, (grad, var) in enumerate(grads_and_vars):
                    if grad is not None:
                        grads_and_vars[idx] = (tf.clip_by_norm(grad, self.gradient_clipping), var)
               </a> self.optimize_op = self.optimizer.apply_gradients(grads_and_vars)

            self.optimize_op = self.optimizer.minimize(loss)
</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 if gradient clipping is used, calculate the huber loss
            if self.config.clip_gradients:
                <a id="change">huber_loss = tf.select(tf.abs(delta) &lt; 1.0, 0.5 * tf.square(delta), tf.abs(delta) - 0.5)</a>
                <a id="change">self.loss = tf.reduce_mean(huber_loss, name=&quotloss&quot)</a>
            else:
                self.loss = tf.reduce_mean(tf.square(delta), name=&quotloss&quot)

            self.optimize_op = self.optimizer.minimize(loss)</code></pre>