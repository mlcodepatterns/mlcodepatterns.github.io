<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    op5 = tf.group(*[ tf.assign(w,v) for w,v in zip(restored_vars, tmp_vars)])
                    with tf.get_default_graph().control_dependencies([op5]):
                        flin = gswap
                        <a id="change">flin = []</a>
                        for grad, jg in zip(gswap, Jgrads):
                            if jg is None:
                                print("JG NONE", grad)
                                <a id="change">flin += [grad]</a>
                            else:
                                flin += <a id="change">[grad + jg * self._beta]</a>
                            
                        <a id="change">step3 = zip(flin, var_list)</a>
                        op6 = self.optimizer.apply_gradients(step3, global_step=global_step, name=name)
                        with tf.get_default_graph().control_dependencies([op6]):
                            return tf.no_op()
</code></pre><h3>After Change</h3><pre><code class='java'>
                            consensus_reg = 0.5 * sum(
                                    tf.reduce_sum(tf.square(g)) for g in all_grads[:len(d_vars)] if g is not None
                            )
                            Jgrads = tf.gradients(consensus_reg, d_vars) + [<a id="change">tf.zeros_like(g)</a> for g in g_vars]
                            <a id="change">op7 = [tf.assign_sub(v, (jg * self._beta)) if jg is not None else tf.assign_sub(v,grad) for v,grad, jg in zip(var_list, all_grads, Jgrads)]</a>
                            with tf.get_default_graph().control_dependencies(op7):
                                return tf.no_op()

  </code></pre>