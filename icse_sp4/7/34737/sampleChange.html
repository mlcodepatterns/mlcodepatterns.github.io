<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 These are the metrics we&quotll pass up the way, and their new names
        train_metrics = {"train_loss", "ups", "wps", "gnorm", "clip"}
        <a id="change">valid_metrics = {"valid_loss"}</a>

        metrics = train_metrics if self.is_training else valid_metrics

        m = {k: self.trainer.meters[k].avg for k in metrics}

        &#47&#47 additionally output perplexity. note that fairseq models use base 2
        &#47&#47 in cross_entropy:
        &#47&#47 github.com/pytorch/fairseq/blob/master/fairseq/criterions/cross_entropy.py&#47&#47L55
        if "train_loss" in m:
            m["train_ppl"] = np.exp2(m["train_loss"])
        <a id="change">if "valid_loss" in m:
            m["ppl"] = np.exp2(m["valid_loss"])

       </a> for k, v in m.items():
            &#47&#47 clean up: rounds to sigfigs and converts tensors to floats
            m[k] = round_sigfigs(v, 4)
</code></pre><h3>After Change</h3><pre><code class='java'>
                output[k] = self.trainer.meters[k].avg

        &#47&#47 for display purposes
        <a id="change">output = {k: round_sigfigs(v, 4) for k, v in output.items()}</a>
        return output

    def reset_metrics(self):
        Reset metrics calculated by the model back to zero.</code></pre>