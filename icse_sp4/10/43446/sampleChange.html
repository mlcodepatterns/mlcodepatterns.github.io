<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        n = self.n

        x = rnd.randn(n)
        <a id="change">y = rnd.randn(n)</a>
        z = rnd.randn(n)

        &#47&#47 The signature of the cost function now implies that we are on the
        &#47&#47 product manifold, so we mimic the behavior of solvers by calling the
        &#47&#47 cost function with a single argument: a tuple containing a tuple (x,
        &#47&#47 y) and a single vector z.
        self.assertAlmostEqual(np.sum(x ** 2 + y + z ** 3), cost(((x, y), z)))

        egrad = cost.compute_gradient()
        g = egrad(((x, y), z))

        &#47&#47 We defined the cost function signature to treat the first two
        &#47&#47 arguments as one parameter, so a call to the gradient must produce
        &#47&#47 two elements.
        self.assertIsInstance(g, (list, tuple))
        self.assertEqual(len(g), 2)
        g_xy, g_z = g
        self.assertIsInstance(g_xy, (list, tuple))
        self.assertEqual(len(g_xy), 2)
        self.assertIsInstance(g_z, np.ndarray)

        &#47&#47 Verify correctness of the gradient.
        np_testing.assert_allclose(g_xy[0], 2 * x)
        np_testing.assert_allclose(g_xy[1], 1)
        np_testing.assert_allclose(g_z, 3 * z ** 2)

        &#47&#47 Test the Hessian.
        <a id="change">u = rnd.randn(n)</a>
        v = rnd.randn(n)
        w = rnd.randn(n)

        ehess = cost.compute_hessian()</code></pre><h3>After Change</h3><pre><code class='java'>
        np_testing.assert_allclose(g_z, 3 * z ** 2)

        &#47&#47 Test the Hessian.
        <a id="change">u, v, w = [rnd.randn(n) for _ in range(3)]</a>

        ehess = cost.compute_hessian_vector_product()
        h = ehess(x, y, z, u, v, w)
</code></pre>