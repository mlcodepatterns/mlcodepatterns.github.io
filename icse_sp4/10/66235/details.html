<html><h3>9bdbb11cf27060e7847a87dcdf691dd6b96ce6df,rlkit/data_management/obs_dict_replay_buffer.py,ObsDictRelabelingBuffer,random_batch,#ObsDictRelabelingBuffer#Any#,171
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                num_rollout_goals:last_env_goal_idx] = \
                    env_goals[goal_key]
        if num_future_goals &gt; 0:
            <a id="change">future_obs_idxs = []</a>
            for i in indices[-num_future_goals:]:
                possible_future_obs_idxs = self._idx_to_future_obs_idx[i]
                &#47&#47 This is generally faster than random.choice. Makes you wonder what
                &#47&#47 random.choice is doing
                num_options = len(possible_future_obs_idxs)
                next_obs_i = int(np.random.randint(0, num_options))
                future_obs_idxs.append(possible_future_obs_idxs[next_obs_i])
            <a id="change">future_obs_idxs = np.array(future_obs_idxs)</a>
            <a id="change">resampled_goals[-num_future_goals:]</a> = self._next_obs[
                self.achieved_goal_key
            ][future_obs_idxs]
            for goal_key in self.goal_keys:
                new_obs_dict[goal_key][-num_future_goals:] = \
                    self._next_obs[goal_key][future_obs_idxs]
                new_next_obs_dict[goal_key][-num_future_goals:] = \
                    self._next_obs[goal_key][future_obs_idxs]

        new_obs_dict[self.desired_goal_key] = resampled_goals
        new_next_obs_dict[self.desired_goal_key] = resampled_goals
        new_obs_dict = postprocess_obs_dict(new_obs_dict)
        new_next_obs_dict = postprocess_obs_dict(new_next_obs_dict)
        &#47&#47 resampled_goals must be postprocessed as well
        resampled_goals = new_next_obs_dict[self.desired_goal_key]

        new_actions = self._actions[indices]
        
        For example, the environments in this repo have batch-wise
        implementations of computing rewards:

        https://github.com/vitchyr/multiworld
        

        if hasattr(self.env, &quotcompute_rewards&quot):
            new_rewards = self.env.compute_rewards(
                new_actions,
                new_next_obs_dict,
            )
        else:  &#47&#47 Assuming it&quots a (possibly wrapped) gym GoalEnv
            new_rewards = np.ones((batch_size, 1))
            for i in range(batch_size):
                <a id="change">new_rewards[i]</a> = self.env.compute_reward(
                    new_next_obs_dict[self.achieved_goal_key][i],
                    new_next_obs_dict[self.desired_goal_key][i],
                    None</code></pre><h3>After Change</h3><pre><code class='java'>
        if num_future_goals &gt; 0:
            &#47&#47&#47&#47 better future obs sample algorithm
            future_indices = indices[-num_future_goals:]
            <a id="change">possible_future_obs_lens = np.array([len(self._idx_to_future_obs_idx[i]) for i in future_indices])</a>
            <a id="change">next_obs_idxs = (np.random.random(num_future_goals) * possible_future_obs_lens).astype(np.int)</a>
            <a id="change">future_obs_idxs = np.array([self._idx_to_future_obs_idx[ids][next_obs_idxs[i]] for i, ids in enumerate(future_indices)])</a>

            <a id="change">resampled_goals[-num_future_goals:]</a> = self._next_obs[
                self.achieved_goal_key
            ][future_obs_idxs]
            for goal_key in self.goal_keys:
                new_obs_dict[goal_key][-num_future_goals:] = \
                    self._next_obs[goal_key][future_obs_idxs]
                new_next_obs_dict[goal_key][-num_future_goals:] = \
                    self._next_obs[goal_key][future_obs_idxs]

        new_obs_dict[self.desired_goal_key] = resampled_goals
        new_next_obs_dict[self.desired_goal_key] = resampled_goals
        new_obs_dict = postprocess_obs_dict(new_obs_dict)
        new_next_obs_dict = postprocess_obs_dict(new_next_obs_dict)
        &#47&#47 resampled_goals must be postprocessed as well
        resampled_goals = new_next_obs_dict[self.desired_goal_key]

        new_actions = self._actions[indices]
        
        For example, the environments in this repo have batch-wise
        implementations of computing rewards:

        https://github.com/vitchyr/multiworld
        

        if hasattr(self.env, &quotcompute_rewards&quot):
            new_rewards = self.env.compute_rewards(
                new_actions,
                new_next_obs_dict,
            )
        else:  &#47&#47 Assuming it&quots a (possibly wrapped) gym GoalEnv
            new_rewards = np.ones((batch_size, 1))
            for i in range(batch_size):
                <a id="change">new_rewards[i]</a> = self.env.compute_reward(
                    new_next_obs_dict[self.achieved_goal_key][i],
                    new_next_obs_dict[self.desired_goal_key][i],
                    None</code></pre><img src="306906312.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/vitchyr/rlkit/commit/9bdbb11cf27060e7847a87dcdf691dd6b96ce6df#diff-2178a345f4deabd282ac87d8dcb0e8ccc6d47221247628b8dbc0938f186b7b3aL171' target='_blank'>Link</a></div><div id='project'> Project Name: vitchyr/rlkit</div><div id='commit'> Commit Name: 9bdbb11cf27060e7847a87dcdf691dd6b96ce6df</div><div id='time'> Time: 2020-08-09</div><div id='author'> Author: 38036768+YangRui2015@users.noreply.github.com</div><div id='file'> File Name: rlkit/data_management/obs_dict_replay_buffer.py</div><div id='class'> Class Name: ObsDictRelabelingBuffer</div><div id='method'> Method Name: random_batch</div><BR><BR><div id='link'><a href='https://github.com/sentinel-hub/eo-learn/commit/01342cee01410f5f0278f85c3b95f158614dc3a4#diff-e22402ed02eeb825e688bc8fa71ae63cf9032f2f43c600cf48a6038f26eec7ffL127' target='_blank'>Link</a></div><div id='project'> Project Name: sentinel-hub/eo-learn</div><div id='commit'> Commit Name: 01342cee01410f5f0278f85c3b95f158614dc3a4</div><div id='time'> Time: 2019-10-15</div><div id='author'> Author: jovan.visnjic@sinergise.com</div><div id='file'> File Name: io/eolearn/io/processing_api.py</div><div id='class'> Class Name: SentinelHubProcessingInput</div><div id='method'> Method Name: execute</div><BR><BR><div id='link'><a href='https://github.com/brian-team/brian2/commit/e48fb30ea6f7eb0c3092ca3b55bdc75176be272a#diff-08c9f2c27f3c61e1cceda2b89ad3ba05ebc9f2501c8c1a170121f645c5cc8afaL42' target='_blank'>Link</a></div><div id='project'> Project Name: brian-team/brian2</div><div id='commit'> Commit Name: e48fb30ea6f7eb0c3092ca3b55bdc75176be272a</div><div id='time'> Time: 2013-10-11</div><div id='author'> Author: marcel.stimberg@ens.fr</div><div id='file'> File Name: brian2/stateupdaters/exact.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_linear_system</div><BR>