<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, model, **kwargs):
        <a id="change">self._model_checks = [
            &#47&#47 todo: Check for non-linear output in general.
            {
                "check": lambda layer: kchecks.contains_activation(
                    layer, activation="softmax"),
                "type": "warning",
                "message": ("Typically models are analyzed with respect to "
                            "pre-softmax output."),
            },
            {
                "check":
                lambda layer: not kchecks.only_relu_activation(layer),
                "type": "warning",
                "message": ("Guided Backprop is only well defined for "
                            "neural networks with "
                            "relu activations."),
            },
        ]</a>

        def reverse_layer_instance(Xs, Ys, reversed_Ys, reverse_state):
            activation = keras.layers.Activation("relu")
            reversed_Ys = kutils.apply(activation, reversed_Ys)</code></pre><h3>After Change</h3><pre><code class='java'>

    def __init__(self, model, **kwargs):

        <a id="change">self._add_model_softmax_check()</a>
        self._add_model_check(
            lambda layer: not kchecks.only_relu_activation(layer),
            "Deconvnet is only specified with networks with ReLU activations.",
            check_type="exception",</code></pre>