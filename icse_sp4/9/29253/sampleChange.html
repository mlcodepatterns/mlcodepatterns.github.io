<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if len(inputs) == 1:
          inputs = inputs[0]
        outputs = self.model(inputs)
        <a id="change">if isinstance(outputs, tf.Tensor):
          outputs = [outputs]
       </a> <a id="change">if self._loss_outputs is not None:
          outputs = [outputs[i] for i in self._loss_outputs]
       </a> batch_loss = loss(outputs, labels, weights)
      if variables is None:
        vars = self.model.trainable_variables
      else:</code></pre><h3>After Change</h3><pre><code class='java'>
      &#47&#47 is called for a new set of variables.  If that happens inside a function
      &#47&#47 annotated with tf.function it throws an exception, so call it once here.

      <a id="change">zero_grads = [tf.zeros(v.shape) for v in variables]</a>
      self._tf_optimizer.apply_gradients(zip(zero_grads, variables))
    if var_key not in self._gradient_fn_for_vars:
      self._gradient_fn_for_vars[var_key] = self._create_gradient_fn(variables)
    apply_gradient_for_batch = self._gradient_fn_for_vars[var_key]</code></pre>