<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        fscore_list = [[int(k[1:]), v] for k, v in fscore.viewitems()]
        print(&quotfscore_list&quot)
        print(fscore_list)
        <a id="change">sorted_fscore = fscore_list.sort(key=lambda x: x[0])</a>
        print(&quotfscore_list after sorting&quot)
        print(fscore_list)

        print(&quotlen(fscore_list)&quot)</code></pre><h3>After Change</h3><pre><code class='java'>
        fscore_list = [[int(k[1:]), v] for k, v in fscore.viewitems()]


        <a id="change">feature_infos = []</a>
        sum_of_all_feature_importances = 0.0

        <a id="change">for idx_and_result in fscore_list:
            idx = idx_and_result[0]
            &#47&#47 Use the index that we grabbed above to find the human-readable feature name
            feature_name = trained_feature_names[idx]
            feat_importance = idx_and_result[1]

            &#47&#47 If we sum up all the feature importances and then divide by that sum, we will be able to have each feature importance as it&quots relative feature imoprtance, and the sum of all of them will sum up to 1, just as it is in scikit-learn.
            sum_of_all_feature_importances += feat_importance
            feature_infos.append([feature_name, feat_importance])

       </a> sorted_feature_infos = sorted(feature_infos, key=lambda x: x[1])

        print(&quotHere are the feature_importances from the tree-based model:&quot)
        print(&quotThe printed list will only contain at most the top 50 features.&quot)</code></pre>