<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if self.trained_pipeline.named_steps.get(&quotfeature_selection&quot, False):

            <a id="change">print(&quotheard we had feature selection&quot)</a>

            selected_indices = self.trained_pipeline.named_steps[&quotfeature_selection&quot].support_mask
            feature_names_before_selection = self.trained_pipeline.named_steps[&quotdv&quot].get_feature_names()
            trained_feature_names = [name for idx, name in enumerate(feature_names_before_selection) if selected_indices[idx]]</code></pre><h3>After Change</h3><pre><code class='java'>
        fscore_list = [[int(k[1:]), v] for k, v in fscore.viewitems()]


        <a id="change">feature_infos = []</a>
        sum_of_all_feature_importances = 0.0

        <a id="change">for idx_and_result in fscore_list:
            idx = idx_and_result[0]
            &#47&#47 Use the index that we grabbed above to find the human-readable feature name
            feature_name = trained_feature_names[idx]
            feat_importance = idx_and_result[1]

            &#47&#47 If we sum up all the feature importances and then divide by that sum, we will be able to have each feature importance as it&quots relative feature imoprtance, and the sum of all of them will sum up to 1, just as it is in scikit-learn.
            sum_of_all_feature_importances += feat_importance
            feature_infos.append([feature_name, feat_importance])

       </a> sorted_feature_infos = sorted(feature_infos, key=lambda x: x[1])

        print(&quotHere are the feature_importances from the tree-based model:&quot)
        print(&quotThe printed list will only contain at most the top 50 features.&quot)</code></pre>