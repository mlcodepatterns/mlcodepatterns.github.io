<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
    def make_positional_encodings(self, dim, max_len):
        pe = torch.zeros(max_len, 1, dim)
        <a id="change">for i in range(dim):
            for j in range(max_len):
                k = float(j) / (10000.0 ** (2.0*i / float(dim)))
                pe[j, 0, i] = math.cos(k) if i % 2 == 1 else math.sin(k)
       </a> return pe

    def load_pretrained_vectors(self, emb_file):
        if emb_file is not None:</code></pre><h3>After Change</h3><pre><code class='java'>
            return self.word_lut.embedding_dim

    def make_positional_encodings(self, dim, max_len):
        pe = <a id="change">torch</a>.arange(0, max_len).unsqueeze(1).expand(max_len, dim)
        <a id="change">div_term = 1 / torch.pow(10000, torch.arange(0, dim * 2, 2) / dim)</a>
        pe = pe * div_term.expand_as(pe)
        pe[:, 0::2] = torch.sin(<a id="change">pe[:, 0::2]</a>)
        <a id="change">pe[:, 1::2] = torch.cos(pe[:, 1::2])</a>
        return pe.unsqueeze(1)

    def load_pretrained_vectors(self, emb_file):
        if emb_file is not None:</code></pre>