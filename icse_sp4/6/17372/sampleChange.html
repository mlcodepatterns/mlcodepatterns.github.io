<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if self.model_name in [&quotLGBMClassifier&quot, &quotLGBMRegressor&quot]:
            X_fit = X.toarray()

        <a id="change">try:
            if self.model_name[:12] == &quotDeepLearning&quot:

                print(&quot\nWe will stop training early if we have not seen an improvement in training accuracy in 25 epochs&quot)
                from keras.callbacks import EarlyStopping
                early_stopping = EarlyStopping(monitor=&quotloss&quot, patience=25, verbose=1)
                self.model.fit(X_fit, y, callbacks=[early_stopping])

            elif self.model_name[:4] == &quotLGBM&quot:

                X_fit, X_test, y, y_test = train_test_split(X_fit, y, test_size=0.15)

                if self.type_of_estimator == &quotregressor&quot:
                    eval_metric = &quotrmse&quot
                elif self.type_of_estimator == &quotclassifier&quot:
                    if len(set(y_test)) &gt; 2:
                        eval_metric = &quotmulti_logloss&quot
                    else:
                        eval_metric = &quotbinary_logloss&quot


                cat_feature_indices = self.get_categorical_feature_indices()
                self.model.fit(X_fit, y, eval_set=[(X_test, y_test)], early_stopping_rounds=50, eval_metric=eval_metric, eval_names=[&quotrandom_holdout_set_from_training_data&quot], categorical_feature=cat_feature_indices)

            elif self.model_name[:8] == &quotCatBoost&quot:
                X_fit = X_fit.toarray()

                if self.type_of_estimator == &quotclassifier&quot and len(pd.Series(y).unique()) &gt; 2:
                    &#47&#47 TODO: we might have to modify the format of the y values, converting them all to ints, then back again (sklearn has a useful inverse_transform on some preprocessing classes)
                    self.model.set_params(loss_function=&quotMultiClass&quot)

                cat_feature_indices = self.get_categorical_feature_indices()

                self.model.fit(X_fit, y, cat_features=cat_feature_indices)

            elif self.model_name[:16] == &quotGradientBoosting&quot:
                if scipy.sparse.issparse(X_fit):
                    X_fit = X_fit.todense()

                patience = 20
                best_val_loss = -10000000000
                num_worse_rounds = 0
                best_model = deepcopy(self.model)
                X_fit, X_test, y, y_test = train_test_split(X_fit, y, test_size=0.15)

                &#47&#47 Add a variable number of trees each time, depending how far into the process we are
                if os.environ.get(&quotis_test_suite&quot, False) == &quotTrue&quot:
                    num_iters = list(range(1, 50, 1)) + list(range(50, 100, 2)) + list(range(100, 250, 3))
                else:
                    num_iters = list(range(1, 50, 1)) + list(range(50, 100, 2)) + list(range(100, 250, 3)) + list(range(250, 500, 5)) + list(range(500, 1000, 10)) + list(range(1000, 2000, 20)) + list(range(2000, 10000, 100))

                try:
                    for num_iter in num_iters:
                        warm_start = True
                        if num_iter == 1:
                            warm_start = False

                        self.model.set_params(n_estimators=num_iter, warm_start=warm_start)
                        self.model.fit(X_fit, y)

                        if self.training_prediction_intervals == True:
                            val_loss = self.model.score(X_test, y_test)
                        else:
                            try:
                                val_loss = self._scorer.score(self, X_test, y_test)
                            except Exception as e:
                                val_loss = self.model.score(X_test, y_test)

                        if val_loss - self.min_step_improvement &gt; best_val_loss:
                            best_val_loss = val_loss
                            num_worse_rounds = 0
                            best_model = deepcopy(self.model)
                        else:
                            num_worse_rounds += 1
                        print(&quot[&quot + str(num_iter) + &quot] random_holdout_set_from_training_data\&quots score is: &quot + str(round(val_loss, 3)))
                        if num_worse_rounds &gt;= patience:
                            break
                except KeyboardInterrupt:
                    print(&quotHeard KeyboardInterrupt. Stopping training, and using the best checkpointed GradientBoosting model&quot)
                    pass

                self.model = best_model
                print(&quotThe number of estimators that were the best for this training dataset: &quot + str(self.model.get_params()[&quotn_estimators&quot]))
                print(&quotThe best score on a random 15 percent holdout set of the training data: &quot + str(best_val_loss))

            else:
                self.model.fit(X_fit, y)

        except TypeError as e:
            if scipy.sparse.issparse(X_fit):
                X_fit = X_fit.todense()
            self.model.fit(X_fit, y)

        except KeyboardInterrupt as e:
            print(&quotStopping training at this point because we heard a KeyboardInterrupt&quot)
            print(&quotIf the model is functional at this point, we will output the model in its latest form&quot)
            print(&quotNote that not all models can be interrupted and still used, and that this feature generally is an unofficial beta-release feature that is known to fail on occasion&quot)
            pass

       </a> return self

    def remove_categorical_values(self, features):
        clean_features = set([])</code></pre><h3>After Change</h3><pre><code class='java'>

            cat_feature_indices = self.get_categorical_feature_indices()
            if cat_feature_indices is None:
                <a id="change">self.model.fit(X_fit, y, eval_set=[(X_test, y_test)], early_stopping_rounds=50, eval_metric=eval_metric, eval_names=[&quotrandom_holdout_set_from_training_data&quot])</a>
            else:
                self.model.fit(X_fit, y, eval_set=[(X_test, y_test)], early_stopping_rounds=50, eval_metric=eval_metric, eval_names=[&quotrandom_holdout_set_from_training_data&quot], categorical_feature=cat_feature_indices)

</code></pre>