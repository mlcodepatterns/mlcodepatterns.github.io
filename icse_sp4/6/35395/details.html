<html><h3>3ab517035d61e06e9d59bcc9b39b9710897e82c2,deeplabcut/generate_training_dataset/frame_extraction.py,,extract_frames,#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,62
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        print("Invalid MODE. Choose either &quotmanual&quot or &quotautomatic&quot. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?`` \
              for ipython/jupyter notebook for more details.")

    <a id="change">print(
        "\nFrames were selected.\nYou can now label the frames using the function &quotlabel_frames&quot (if you extracted enough frames for all videos).")</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            from moviepy.editor import VideoFileClip

        <a id="change">has_failed = []</a>
        for vindex, video in enumerate(videos):
            if userfeedback:
                print("Do you want to extract (perhaps additional) frames for video:", video, "?")
                askuser = input("yes/no")
            else:
                askuser = "yes"

            if askuser == &quoty&quot or askuser == &quotyes&quot or askuser == &quotJa&quot or askuser == &quotha&quot\
                    or askuser == &quotoui&quot or askuser == &quotouais&quot:  &#47&#47 multilanguage support :)
                if opencv:
                    cap = cv2.VideoCapture(video)
                    fps = cap.get(
                        5)  &#47&#47 https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html&#47&#47videocapture-get
                    nframes = int(cap.get(7))
                else:
                    &#47&#47 Moviepy:
                    clip = VideoFileClip(video)
                    fps = clip.fps
                    nframes = int(np.ceil(clip.duration * 1. / fps))
                indexlength = int(np.ceil(np.log10(nframes)))

                fname = Path(video)
                output_path = Path(config).parents[0] / &quotlabeled-data&quot / fname.stem

                if output_path.exists():
                    if len(os.listdir(output_path)):
                        askuser = input("The directory already contains some frames. Do you want to add to it?(yes/no): ")
                        if not (askuser == &quoty&quot or askuser == &quotyes&quot or askuser == &quotY&quot or askuser == &quotYes&quot):
                            sys.exit("Delete the frames and try again later!")

                if crop == &quotGUI&quot:
                    cfg = select_cropping_area(config, [video])
                coords = cfg[&quotvideo_sets&quot][video][&quotcrop&quot].split(&quot,&quot)
                if crop and not opencv:
                    clip = clip.crop(y1=int(coords[2]), y2=int(coords[3]), x1=int(coords[0]), x2=int(coords[1]))
                elif not crop:
                    coords = None

                print("Extracting frames based on %s ..." % algo)
                if algo == &quotuniform&quot:
                    if opencv:
                        frames2pick = frameselectiontools.UniformFramescv2(cap, numframes2pick, start, stop)
                    else:
                        frames2pick = frameselectiontools.UniformFrames(clip, numframes2pick, start, stop)
                elif algo == &quotkmeans&quot:
                    if opencv:
                        frames2pick = frameselectiontools.KmeansbasedFrameselectioncv2(cap, numframes2pick, start, stop,
                                                                                       crop, coords, step=cluster_step,
                                                                                       resizewidth=cluster_resizewidth,
                                                                                       color=cluster_color)
                    else:
                        frames2pick = frameselectiontools.KmeansbasedFrameselection(clip, numframes2pick, start, stop,
                                                                                    step=cluster_step,
                                                                                    resizewidth=cluster_resizewidth,
                                                                                    color=cluster_color)
                else:
                    print(
                        "Please implement this method yourself and send us a pull request! Otherwise, choose &quotuniform&quot or &quotkmeans&quot.")
                    frames2pick = []

                if not len(frames2pick):
                    print(&quotFrame selection failed...&quot)
                    return

                output_path = Path(config).parents[0] / &quotlabeled-data&quot / Path(video).stem
                is_valid = []
                if opencv:
                    for index in frames2pick:
                        cap.set(1, index)  &#47&#47 extract a particular frame
                        ret, frame = cap.read()
                        if ret:
                            image = img_as_ubyte(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                            img_name = str(output_path) + &quot/img&quot + str(index).zfill(indexlength) + ".png"
                            if crop:
                                io.imsave(img_name, image[int(coords[2]):int(coords[3]), int(coords[0]):int(coords[1]),
                                                    :])  &#47&#47 y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]
                            else:
                                io.imsave(img_name, image)
                            is_valid.append(True)
                        else:
                            print("Frame", index, " not found!")
                            is_valid.append(False)
                    cap.release()
                else:
                    for index in frames2pick:
                        try:
                            image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps))
                            img_name = str(output_path) + &quot/img&quot + str(index).zfill(indexlength) + ".png"
                            io.imsave(img_name, image)
                            if np.var(image) == 0:  &#47&#47 constant image
                                print(
                                    "Seems like black/constant images are extracted from your video. Perhaps consider using opencv under the hood, by setting: opencv=True")
                            is_valid.append(True)
                        except FileNotFoundError:
                            print("Frame &#47&#47 ", index, " does not exist.")
                            is_valid.append(False)
                    clip.close()
                    del clip

                if not any(is_valid):
                    has_failed.append(True)
                else:
                    has_failed.append(False)

        <a id="change">if all(has_failed):
            print(&quotFrame extraction failed. Video files must be corrupted.&quot)
            return
        elif any(has_failed):
            print(&quotAlthough most frames were extracted, some were invalid.&quot)
        else:
            print("Frames were successfully extracted.")
       </a> print("\nYou can now label the frames using the function &quotlabel_frames&quot "
              "(if you extracted enough frames for all videos).")
    else:
        print("Invalid MODE. Choose either &quotmanual&quot or &quotautomatic&quot. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?`` \</code></pre><img src="172333869.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/AlexEMG/DeepLabCut/commit/3ab517035d61e06e9d59bcc9b39b9710897e82c2#diff-926bb071938dd9ea9d1970dec3686986c426d0c52cea08190c91a6e4ddc7503eL178' target='_blank'>Link</a></div><div id='project'> Project Name: AlexEMG/DeepLabCut</div><div id='commit'> Commit Name: 3ab517035d61e06e9d59bcc9b39b9710897e82c2</div><div id='time'> Time: 2020-04-04</div><div id='author'> Author: mathis@rowland.harvard.edu</div><div id='file'> File Name: deeplabcut/generate_training_dataset/frame_extraction.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: extract_frames</div><BR><BR><div id='link'><a href='https://github.com/AlexEMG/DeepLabCut/commit/a51d9ee3185ae5f83b5903d6b8549257ea4f13ff#diff-926bb071938dd9ea9d1970dec3686986c426d0c52cea08190c91a6e4ddc7503eL178' target='_blank'>Link</a></div><div id='project'> Project Name: AlexEMG/DeepLabCut</div><div id='commit'> Commit Name: a51d9ee3185ae5f83b5903d6b8549257ea4f13ff</div><div id='time'> Time: 2020-04-02</div><div id='author'> Author: amathis@fas.harvard.edu</div><div id='file'> File Name: deeplabcut/generate_training_dataset/frame_extraction.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: extract_frames</div><BR><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/eff8fbd2e3fb295d3c42dbc28d9d7cbcb6ca64ad#diff-526bed3d62774528c3417410086025fbd7487118ade5233f6425f7883177351bL82' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: eff8fbd2e3fb295d3c42dbc28d9d7cbcb6ca64ad</div><div id='time'> Time: 2018-08-02</div><div id='author'> Author: eric-heiden@users.noreply.github.com</div><div id='file'> File Name: tests/envs/test_envs.py</div><div id='class'> Class Name: TestEnvs</div><div id='method'> Method Name: test_env</div><BR>