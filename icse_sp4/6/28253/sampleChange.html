<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if tf.executing_eagerly():
      raise RuntimeError("Attention-based RNN decoder are currently not compatible "
                         "with eager execution")
    <a id="change">initial_cell_state = super(AttentionalRNNDecoder, self)._get_initial_state(
        batch_size, dtype, initial_state=initial_state)</a>
    attention_mechanism = self.attention_mechanism_class(
        self.cell.output_size,
        self.memory,
        memory_sequence_length=self.memory_sequence_length,</code></pre><h3>After Change</h3><pre><code class='java'>
        self.memory,
        memory_sequence_length=self.memory_sequence_length)
    decoder_state = self.cell.get_initial_state(batch_size=batch_size, dtype=dtype)
    <a id="change">if initial_state is not None:
      if self.first_layer_attention:
        cell_state = list(decoder_state)
        cell_state[0] = decoder_state[0].cell_state
        cell_state = self.bridge(initial_state, cell_state)
        cell_state[0] = decoder_state[0].clone(cell_state=cell_state[0])
        decoder_state = tuple(cell_state)
      else:
        cell_state = self.bridge(initial_state, decoder_state.cell_state)
        decoder_state = decoder_state.clone(cell_state=cell_state)
   </a> return decoder_state

  def step(self,
           inputs,</code></pre>