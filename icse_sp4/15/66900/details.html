<html><h3>145170ca9bbd89aa01d8a40841e3c039d3683af8,stellargraph/layer/graph_attention.py,GraphAttention,call,#GraphAttention#Any#,211
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if kwargs.get("add_self_loops", False):
            &#47&#47 get the number of nodes from inputs[1] directly
            N = K.int_shape(inputs[1])[-1]
            <a id="change">if N is not None:
                &#47&#47 create self-loops
                A = tf.linalg.set_diag(A, K.cast(np.ones((N,)), dtype="float"))
            else:
                raise ValueError(
                    "{}: need to know number of nodes to add self-loops; obtained None instead".format(
                        type(self).__name__
                    )
                )

       </a> outputs = []
        for head in range(self.attn_heads):
            kernel = self.kernels[head]  &#47&#47 W in the paper (F x F&quot)
            attention_kernel = self.attn_kernels[</code></pre><h3>After Change</h3><pre><code class='java'>
        print("&gt;&gt;", X.shape, A.shape, out_indices.shape)

        batch_dim, n_nodes, _ = K.int_shape(X)
        <a id="change">if batch_dim != 1:
            raise ValueError(
                "Currently full-batch methods only support a batch dimension of one"
            )

        &#47&#47 Remove singleton batch dimension
       </a> A = K.squeeze(A, 0)
        X = K.squeeze(X, 0)
        out_indices = K.squeeze(out_indices, 0)
</code></pre><img src="310200028.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stellargraph/stellargraph/commit/145170ca9bbd89aa01d8a40841e3c039d3683af8#diff-a7fe88a489b615f3fc3ad491dab9fad6f9195bdf9cea28b70933003c04e8095cL211' target='_blank'>Link</a></div><div id='project'> Project Name: stellargraph/stellargraph</div><div id='commit'> Commit Name: 145170ca9bbd89aa01d8a40841e3c039d3683af8</div><div id='time'> Time: 2019-06-03</div><div id='author'> Author: andrew.docherty@data61.csiro.au</div><div id='file'> File Name: stellargraph/layer/graph_attention.py</div><div id='class'> Class Name: GraphAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/asyml/texar/commit/4a9836127987a39832d1a6f9da7e0cc925423d50#diff-5457aa8137443d3182cdea2d0f90a68ea689c410ae08df60e8856595f6ab0fc8L21' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: 4a9836127987a39832d1a6f9da7e0cc925423d50</div><div id='time'> Time: 2018-07-31</div><div id='author'> Author: zhiting.hu@petuum.com</div><div id='file'> File Name: texar/models/seq2seq/seq2seq_base.py</div><div id='class'> Class Name: Seq2seqBase</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/dask/dask-ml/commit/68c9bab4a5c6821c2a4395763c676ba796406c49#diff-088ce4d73f1855ec1614aa0d440734d53205b5f867c5770d18e783d03f56e792L223' target='_blank'>Link</a></div><div id='project'> Project Name: dask/dask-ml</div><div id='commit'> Commit Name: 68c9bab4a5c6821c2a4395763c676ba796406c49</div><div id='time'> Time: 2020-08-17</div><div id='author'> Author: TomAugspurger@users.noreply.github.com</div><div id='file'> File Name: dask_ml/preprocessing/_encoders.py</div><div id='class'> Class Name: OneHotEncoder</div><div id='method'> Method Name: _transform</div><BR>