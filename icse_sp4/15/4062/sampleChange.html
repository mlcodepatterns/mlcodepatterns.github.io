<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        return kl_divergence

    def initialize_variational_dist(self):
        <a id="change">if not self.variational_params_initialized.item():
            prior_dist = self.prior_distribution
            inv_prior_dist = torch.distributions.MultivariateNormal(
                prior_dist.mean,
                prior_dist.lazy_covariance_matrix.add_jitter()
                .evaluate()
                .double()
                .inverse()
                .type_as(prior_dist.covariance_matrix),
            )
            self.variational_distribution.initialize_variational_distribution(inv_prior_dist)
            self.variational_params_initialized.fill_(1)

   </a> def forward(self, x):
        
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution</code></pre><h3>After Change</h3><pre><code class='java'>
        return kl_divergence

    def initialize_variational_dist(self):
        <a id="change">prior_dist = self.prior_distribution</a>
        <a id="change">inv_prior_dist = torch.distributions.MultivariateNormal(
            prior_dist.mean,
            prior_dist.lazy_covariance_matrix.add_jitter()
            .evaluate()
            .double()
            .inverse()
            .type_as(prior_dist.covariance_matrix),
        )</a>
        <a id="change">self.variational_distribution.initialize_variational_distribution(inv_prior_dist)</a>

    def forward(self, x):
        
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the</code></pre>