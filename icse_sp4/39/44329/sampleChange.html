<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    Test siamese singletask model overfits tiny data.
    np.random.seed(123)
    tf.set_random_seed(123)
    <a id="change">g = tf.Graph()</a>
    <a id="change">sess = tf.Session(graph=g)</a>
    n_tasks = 1
    n_feat = 75
    max_depth = 4
    n_pos = 6
    n_neg = 4
    test_batch_size = 10
    n_train_trials = 80
    support_batch_size = n_pos + n_neg
    
    &#47&#47 Load mini log-solubility dataset.
    featurizer = dc.feat.ConvMolFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_classification.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

    <a id="change">with g.as_default():
      support_model = dc.nn.SequentialSupportGraph(n_feat)
      
      &#47&#47 Add layers
      &#47&#47 output will be (n_atoms, 64)
      support_model.add(dc.nn.GraphConv(64, activation=&quotrelu&quot))
      &#47&#47 Need to add batch-norm separately to test/support due to differing
      &#47&#47 shapes.
      &#47&#47 output will be (n_atoms, 64)
      support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
      &#47&#47 output will be (n_atoms, 64)
      support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
      support_model.add(dc.nn.GraphPool())
      support_model.add_test(dc.nn.GraphGather(test_batch_size))
      support_model.add_support(dc.nn.GraphGather(support_batch_size))

      with self.test_session() as sess:
        model = dc.models.SupportGraphClassifier(
          sess, support_model, test_batch_size=test_batch_size,
          support_batch_size=support_batch_size, learning_rate=1e-3)

        &#47&#47 Fit trained model. Dataset has 6 positives and 4 negatives, so set
        &#47&#47 n_pos/n_neg accordingly.
        model.fit(dataset, n_episodes_per_epoch=n_train_trials, n_pos=n_pos,
                  n_neg=n_neg)
        model.save()

        &#47&#47 Eval model on train. Dataset has 6 positives and 4 negatives, so set
        &#47&#47 n_pos/n_neg accordingly. Note that support is *not* excluded (so we
        &#47&#47 can measure model has memorized support).  Replacement is turned off to
        &#47&#47 ensure that support contains full training set. This checks that the
        &#47&#47 model has mastered memorization of provided support.
        scores, _ = model.evaluate(dataset, classification_metric, n_trials=5,
                                   n_pos=n_pos, n_neg=n_neg,
                                   exclude_support=False)

      &#47&#47 Measure performance on 0-th task.
      assert scores[0] &gt; .9

 </a> def test_attn_lstm_singletask_classification_overfit(self):
    Test attn lstm singletask overfits tiny data.
    np.random.seed(123)
    tf.set_random_seed(123)</code></pre><h3>After Change</h3><pre><code class='java'>

    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

    <a id="change">support_model = dc.nn.SequentialSupportGraph(n_feat)</a>

    &#47&#47 Add layers
    &#47&#47 output will be (n_atoms, 64)
    <a id="change">support_model.add(dc.nn.GraphConv(64, n_feat, activation=&quotrelu&quot))</a>
    &#47&#47 Need to add batch-norm separately to test/support due to differing
    &#47&#47 shapes.
    &#47&#47 output will be (n_atoms, 64)
    <a id="change">support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
    &#47&#47 output will be (n_atoms, 64)
    <a id="change">support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))</a>
    <a id="change">support_model.add(dc.nn.GraphPool())</a>
    <a id="change">support_model.add_test(dc.nn.GraphGather(test_batch_size))</a>
    <a id="change">support_model.add_support(dc.nn.GraphGather(support_batch_size))</a>

    <a id="change">model = dc.models.SupportGraphClassifier(
        support_model,
        test_batch_size=test_batch_size,
        support_batch_size=support_batch_size,
        learning_rate=1e-3)</a>

    &#47&#47 Fit trained model. Dataset has 6 positives and 4 negatives, so set
    &#47&#47 n_pos/n_neg accordingly.
    <a id="change">model.fit(
        dataset, n_episodes_per_epoch=n_train_trials, n_pos=n_pos, n_neg=n_neg)</a>
    <a id="change">model.save()</a>

    &#47&#47 Eval model on train. Dataset has 6 positives and 4 negatives, so set
    &#47&#47 n_pos/n_neg accordingly. Note that support is *not* excluded (so we
    &#47&#47 can measure model has memorized support).  Replacement is turned off to
    &#47&#47 ensure that support contains full training set. This checks that the
    &#47&#47 model has mastered memorization of provided support.
    <a id="change">scores, _ = model.evaluate(
        dataset,
        classification_metric,
        n_trials=5,
        n_pos=n_pos,
        n_neg=n_neg,
        exclude_support=False)</a>

    &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47 DEBUG
    &#47&#47 TODO(rbharath): Check if something went wrong here...
    &#47&#47 Measure performance on 0-th task.</code></pre>