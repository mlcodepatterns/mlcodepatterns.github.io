<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
print(x.data)

&#47&#47 You can also do all the same operations you did with tensors with Variables.
y = autograd.Variable(<a id="change">torch.Tensor([4., 5., 6])</a>, requires_grad=True)
z = x + y
print(z.data)
</code></pre><h3>After Change</h3><pre><code class='java'>

&#47&#47 ``.requires_grad_( ... )`` changes an existing Tensor&quots ``requires_grad``
&#47&#47 flag in-place. The input flag defaults to ``True`` if not given.
<a id="change">x = x.requires_grad_()</a>
y = y.requires_grad_()
&#47&#47 z contains enough information to compute gradients, as we saw above
z = x + y
print(z.grad_fn)
&#47&#47 If any input to an operation has ``requires_grad=True``, so will the output
print(z.requires_grad)

&#47&#47 Now z has the computation history that relates itself to x and y
&#47&#47 Can we just take its values, and **detach** it from its history?
new_z = z.detach()

&#47&#47 ... does new_z have information to backprop to x and y?
&#47&#47 NO!
print(new_z.grad_fn)
&#47&#47 And how could it? ``z.detach()`` returns a tensor that shares the same storage
&#47&#47 as ``z``, but with the computation history forgotten. It doesn&quott know anything
&#47&#47 about how it was computed.
&#47&#47 In essence, we have broken the Tensor away from its past history

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
&#47&#47 You can also stops autograd from tracking history on Tensors
&#47&#47 with requires_grad=True by wrapping the code block in
&#47&#47 ``with torch.no_grad():``
print(x.requires_grad)
<a id="change">print((x ** 2).requires_grad)</a>

with torch.no_grad():
	print((x ** 2).requires_grad)
</code></pre>