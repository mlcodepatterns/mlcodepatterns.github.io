<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        toggled = [False] * len(args)

        for i, arg in enumerate(args):
            <a id="change">if not arg.requires_grad:
                arg.requires_grad = True
                toggled[i] = True

       </a> loss = (left_vecs * self._matmul(right_vecs)).sum()
        loss.requires_grad_(True)
        grads = torch.autograd.grad(loss, args, allow_unused=True)
</code></pre><h3>After Change</h3><pre><code class='java'>
        from collections import deque

        args = tuple(self.representation())
        <a id="change">args_with_grads = tuple(arg for arg in args if arg.requires_grad)</a>

        &#47&#47 Easy case: if we don&quott require any gradients, then just return!
        if not len(args_with_grads):
            return tuple(None for _ in args)

        &#47&#47 Normal case: we&quotll use the autograd to get us a derivative
        with torch.autograd.enable_grad():
            loss = (left_vecs * self._matmul(right_vecs)).sum()
            loss.requires_grad_(True)
            actual_grads = deque(torch.autograd.grad(loss, args_with_grads, allow_unused=True))

        &#47&#47 Now make sure that the object we return has one entry for every item in args
        grads = []
        for arg in args:
            if arg.requires_grad:
                grads.append(<a id="change">actual_grads.popleft()</a>)
            else:
                grads.append(None)
</code></pre>