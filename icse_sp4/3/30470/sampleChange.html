<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        :param dropout (``float``): The amount of dropout to use
        :param attn_fn: A function to apply attention, defaults to SDP
        
        <a id="change">super(MultiHeadedAttention, self).__init__()</a>
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.w_Q = TimeDistributedProjection(d_model)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.d_k = d_model // num_heads
        self.h = num_heads
        self.w_Q = tf.keras.layers.Dense(units=d_model)
        self.w_K = <a id="change">tf.keras.layers.Dense(units=d_model)</a>
        self.w_V = tf.keras.layers.Dense(units=d_model)
        self.w_O = tf.keras.layers.Dense(units=d_model)
        self.attn_fn = self._scaled_dot_product_attention if scale else self._dot_product_attention
        self.attn = None</code></pre>