<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
  input_dim = keys.get_shape().as_list()[-1]
  head_dim = input_dim / num_heads

  <a id="change">with tf.variable_scope("multi_head"):
    for i in range(num_heads):
      with tf.variable_scope("head_" + str(i)):
        &#47&#47 Project queries, keys and values to different and smaller subspaces.
        queries_proj = tf.layers.dense(
          queries,
          head_dim,
          use_bias=False)
        keys_proj = tf.layers.dense(
          keys,
          head_dim,
          use_bias=False)
        values_proj = tf.layers.dense(
          values,
          head_dim,
          use_bias=False)

        head_i = scaled_dot_attention(
          queries_proj,
          keys_proj,
          values_proj,
          mode,
          values_length=values_length,
          mask_future=mask_future,
          dropout=dropout)

        heads.append(head_i)

    &#47&#47 Concatenate all heads output.
    return tf.concat(heads, axis=2)

</a>def feed_forward(x, inner_dim):
  Implements the Transformer&quots "Feed Forward" layer.

  ffn(x) = max(0, x*W_1 + b_1)*W_2 + b_2</code></pre><h3>After Change</h3><pre><code class='java'>
  input_dim = keys.get_shape().as_list()[-1]
  head_dim = input_dim / num_heads

  <a id="change">for i in range(num_heads):
    with tf.variable_scope("head_" + str(i)):
      &#47&#47 Project queries, keys and values to different and smaller subspaces.
      queries_proj = tf.layers.dense(
        queries,
        head_dim,
        use_bias=False)
      keys_proj = tf.layers.dense(
        keys,
        head_dim,
        use_bias=False)
      values_proj = tf.layers.dense(
        values,
        head_dim,
        use_bias=False)

      head_i = scaled_dot_attention(
        queries_proj,
        keys_proj,
        values_proj,
        mode,
        values_length=values_length,
        mask_future=mask_future,
        dropout=dropout)

      heads.append(head_i)

  &#47&#47 Concatenate all heads output.
 </a> return tf.concat(heads, axis=2)

def feed_forward(x, inner_dim):
  Implements the Transformer&quots "Feed Forward" layer.</code></pre>