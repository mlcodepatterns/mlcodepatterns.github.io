<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        assert self.ope_manager
        dataset_as_episodes = self.call_memory(&quotget_all_complete_episodes_from_to&quot,
                                               (<a id="change">self.call_memory(&quotget_last_training_set_episode_id&quot)</a> + 1,
                                                self.call_memory(&quotnum_complete_episodes&quot)))
        if len(dataset_as_episodes) == 0:
            raise ValueError(&quottrain_to_eval_ratio is too high causing the evaluation set to be empty. &quot</code></pre><h3>After Change</h3><pre><code class='java'>
        
        assert self.ope_manager

        <a id="change">if not isinstance(self.pre_network_filter, NoInputFilter) and len(self.pre_network_filter.reward_filters) != 0:
            raise ValueError("Defining a pre-network reward filter when OPEs are calculated will result in a mismatch "
                             "between q values (which are scaled), and actual rewards, which are not. It is advisable "
                             "to use an input_filter, if possible, instead, which will filter the transitions directly "
                             "in the replay buffer, affecting both the q_values and the rewards themselves. ")

       </a> ips, dm, dr, seq_dr, wis = self.ope_manager.evaluate(
                                  evaluation_dataset_as_episodes=self.memory.evaluation_dataset_as_episodes,
                                  evaluation_dataset_as_transitions=self.memory.evaluation_dataset_as_transitions,
                                  batch_size=self.ap.network_wrappers[&quotmain&quot].batch_size,</code></pre>