<html><h3>7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609,src/garage/tf/algos/dqn.py,DQN,train,#DQN#Any#,197
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for _ in runner.step_epochs():
            for cycle in range(self._steps_per_epoch):
                runner.step_path = runner.obtain_trajectories(runner.step_itr)
                <a id="change">last_return = self.train_once(runner.step_itr,
                                              runner.step_path)</a>
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    log_performance(runner.step_itr,
                                    obtain_evaluation_samples(
                                        self.policy, self._eval_env),
                                    discount=self._discount)
                runner.step_itr += 1

        <a id="change">return last_return</a>

    def train_once(self, itr, trajectories):
        Perform one step of policy optimization given one batch of samples.
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        if not self._eval_env:
            self._eval_env = runner.get_env_copy()
        <a id="change">last_returns = [float(&quotnan&quot)]</a>
        runner.enable_logging = False

        qf_losses = []
        for _ in runner.step_epochs():
            for cycle in range(self._steps_per_epoch):
                runner.step_path = runner.obtain_trajectories(runner.step_itr)
                qf_losses.extend(
                    self.train_once(runner.step_itr, runner.step_path))
                if (cycle == 0 and self.replay_buffer.n_transitions_stored &gt;=
                        self._min_buffer_size):
                    runner.enable_logging = True
                    <a id="change">eval_samples = obtain_evaluation_samples(
                        self.policy, self._eval_env)</a>
                    <a id="change">last_returns = log_performance(runner.step_itr,
                                                   eval_samples,
                                                   discount=self._discount)</a>
                runner.step_itr += 1
            tabular.record(&quotDQN/QFLossMean&quot, np.mean(qf_losses))
            tabular.record(&quotDQN/QFLossStd&quot, np.std(qf_losses))

        <a id="change">return np.mean(last_returns)</a>

    def train_once(self, itr, trajectories):
        Perform one step of policy optimization given one batch of samples.
</code></pre><img src="159935392.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609#diff-1af74a4a46df1f819022641daeb70e86ee0fc95c8206d0486f5ebc54eaa6224eL205' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: 7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609</div><div id='time'> Time: 2020-07-17</div><div id='author'> Author: 41180126+krzentner@users.noreply.github.com</div><div id='file'> File Name: src/garage/tf/algos/dqn.py</div><div id='class'> Class Name: DQN</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609#diff-1af74a4a46df1f819022641daeb70e86ee0fc95c8206d0486f5ebc54eaa6224eL205' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: 7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609</div><div id='time'> Time: 2020-07-17</div><div id='author'> Author: 41180126+krzentner@users.noreply.github.com</div><div id='file'> File Name: src/garage/tf/algos/dqn.py</div><div id='class'> Class Name: DQN</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609#diff-a6a25bf4ac5642166f5d49bd1b2e631279d45d65f735827a07dbfcc5304328d9L150' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: 7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609</div><div id='time'> Time: 2020-07-17</div><div id='author'> Author: 41180126+krzentner@users.noreply.github.com</div><div id='file'> File Name: src/garage/torch/algos/ddpg.py</div><div id='class'> Class Name: DDPG</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609#diff-cf25ac911bba2ac0f3eb6b6aeca37dbb926aa0c20336f2e683c693fe0198d50bL280' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: 7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609</div><div id='time'> Time: 2020-07-17</div><div id='author'> Author: 41180126+krzentner@users.noreply.github.com</div><div id='file'> File Name: src/garage/tf/algos/ddpg.py</div><div id='class'> Class Name: DDPG</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/rlworkgroup/garage/commit/7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609#diff-99cd18fc05d5e6d2b18bb69b50319aec038f513c78fa4e156fe40d049b05fa79L301' target='_blank'>Link</a></div><div id='project'> Project Name: rlworkgroup/garage</div><div id='commit'> Commit Name: 7a494dbcc84a5abdf83f3aa9aff2d631f5d5f609</div><div id='time'> Time: 2020-07-17</div><div id='author'> Author: 41180126+krzentner@users.noreply.github.com</div><div id='file'> File Name: src/garage/tf/algos/td3.py</div><div id='class'> Class Name: TD3</div><div id='method'> Method Name: train</div><BR>