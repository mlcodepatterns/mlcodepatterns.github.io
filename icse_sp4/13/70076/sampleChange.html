<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    dataset_url = args.dataset_url
    schema = locate(args.unischema_class)

    <a id="change">if not schema:
        raise ValueError(&quotSchema {} could not be located&quot.format(args.unischema_class))

    &#47&#47 Open Spark Session
   </a> spark_session = SparkSession \
        .builder \
        .appName("Petastorm Metadata Index")
    if args.master:</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Spark Context is available from the SparkSession
    sc = spark.sparkContext

    <a id="change">if args.unischema_class:
        schema = locate(args.unischema_class)
    else:
        resolver = FilesystemResolver(dataset_url, sc._jsc.hadoopConfiguration())
        dataset = pq.ParquetDataset(
            resolver.parsed_dataset_url().path,
            filesystem=resolver.filesystem(),
            validate_schema=False)

        try:
            schema = get_schema(dataset)
        except ValueError:
            raise ValueError(&quotSchema could not be located in existing dataset.&quot
                             &quot Please pass it into the job as --unischema_class&quot)

   </a> with materialize_dataset(spark, dataset_url, schema):
        &#47&#47 Inside the materialize dataset context we just need to write the metadata file as the schema will
        &#47&#47 be written by the context manager.
        &#47&#47 We use the java ParquetOutputCommitter to write the metadata file for the existing dataset</code></pre>