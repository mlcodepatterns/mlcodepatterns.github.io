<html><h3>447885e15243dd18d906e2e35ac34ec6dcf9a600,Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py,PolicyGradient,_build_net,#PolicyGradient#,129
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 build target_net
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&quots_&quot)
        <a id="change">with tf.variable_scope(&quottarget_net&quot):
            self.q_next = self._build_layers(self.s_, self.n_actions, trainable=False)

   </a> def _build_layers(self, inputs, action_size, trainable):
        layers_output = [inputs]
        for i, n_unit in enumerate(self.hidden_layers):
            with tf.variable_scope(&quotlayer%i&quot % i):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.fake_targets = tf.placeholder(tf.float32, [None, 1], name=&quotfake_targets&quot)  &#47&#47 fake targets
        self.advantages = tf.placeholder(tf.float32, [None, 1], name="advantages")  &#47&#47 advantages

        <a id="change">l1 = self._add_layer(&quothidden0&quot, self.x_inputs, self.n_features, 10, tf.nn.relu)</a>     &#47&#47 hidden layer 1
        self.prediction = self._add_layer(&quotoutput&quot, l1, 10, 1, tf.nn.sigmoid)  &#47&#47 predicting for action 0
        with tf.name_scope(&quotloss&quot):
            loglik = self.fake_targets*<a id="change">tf.log(self.prediction)</a> + (1 - self.fake_targets)*tf.log(1-self.prediction)  &#47&#47
            self.loss = -tf.reduce_mean(loglik * self.advantages)
        with tf.name_scope(&quottrain&quot):
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)</code></pre><img src="110478716.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/MorvanZhou/tutorials/commit/447885e15243dd18d906e2e35ac34ec6dcf9a600#diff-0b950c40f343f7642bc3f76d52ac955592df4322a1c323d918cc8a630787ead7L100' target='_blank'>Link</a></div><div id='project'> Project Name: MorvanZhou/tutorials</div><div id='commit'> Commit Name: 447885e15243dd18d906e2e35ac34ec6dcf9a600</div><div id='time'> Time: 2016-12-30</div><div id='author'> Author: morvanzhou@hotmail.com</div><div id='file'> File Name: Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py</div><div id='class'> Class Name: PolicyGradient</div><div id='method'> Method Name: _build_net</div><BR><BR><div id='link'><a href='https://github.com/explosion/prodigy-recipes/commit/59a5632f2c57bb4747849863c5ba0c33077dceb3#diff-7fa975b80a37743987a2ad83455308c06778ee3b7af3fa7b10ffa6cb29b21e64L120' target='_blank'>Link</a></div><div id='project'> Project Name: explosion/prodigy-recipes</div><div id='commit'> Commit Name: 59a5632f2c57bb4747849863c5ba0c33077dceb3</div><div id='time'> Time: 2019-06-25</div><div id='author'> Author: balaabhijit5@gmail.com</div><div id='file'> File Name: image/tf_odapi/image_frozen_model.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: get_predictions</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/ParlAI/commit/06bb64cd6f06413528f5c634da7517350541adeb#diff-03ba76090f760d9fb2e591960d2781d76a4d164006a0c3f04d84a10dd77783e2L24' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/ParlAI</div><div id='commit'> Commit Name: 06bb64cd6f06413528f5c634da7517350541adeb</div><div id='time'> Time: 2018-02-05</div><div id='author'> Author: kshuster@fb.com</div><div id='file'> File Name: examples/extract_image_feature.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: main</div><BR>