<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>


def ppl(sentence_list):
    <a id="change">ppl_list = []</a>
    &#47&#47 load data dict
    word_to_int = load_word_dict(conf.word_dict_path)
    &#47&#47 init params
    batch_size = 1
    tf.reset_default_graph()
    input_data = tf.placeholder(tf.int32, [batch_size, None])
    output_targets = tf.placeholder(tf.int32, [batch_size, None])
    &#47&#47 init model
    end_points = rnn_model(model=&quotlstm&quot,
                           input_data=input_data,
                           output_data=output_targets,
                           vocab_size=len(word_to_int),
                           rnn_size=128,
                           num_layers=2,
                           batch_size=batch_size,
                           learning_rate=conf.learning_rate)
    saver = tf.train.Saver(tf.global_variables())
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    with tf.Session() as sess:
        &#47&#47 init op
        sess.run(init_op)
        checkpoint = tf.train.latest_checkpoint(conf.model_dir)
        saver.restore(sess, checkpoint)
        print("loading model from the checkpoint {0}".format(checkpoint))

        &#47&#47 infer each sentence
        for sentence in sentence_list:
            ppl = 0
            &#47&#47 data idx
            x = [word_to_int[c] if c in word_to_int else word_to_int[UNK_TOKEN] for c in sentence]
            x = [word_to_int[START_TOKEN]] + x + [word_to_int[END_TOKEN]]
            print(&quotx:&quot, x)
            &#47&#47 reshape
            y = np.array(x[1:]).reshape((-1, batch_size))
            x = np.array(x[:-1]).reshape((-1, batch_size))
            print(x.shape)
            print(y.shape)
            &#47&#47 get each word perplexity
            word_count = x.shape[0]
            for i in range(word_count):
                perplexity = sess.run(end_points[&quotperplexity&quot],
                                      feed_dict={input_data: x[i:i + 1, :],
                                                 output_targets: y[i:i + 1, :]})
                print(&quot{0} -&gt; {1}, perplexity: {2}&quot.format(x[i:i + 1, :], y[i:i + 1, :], perplexity))
                if i == 0 or i == word_count:
                    continue
                ppl += perplexity
            ppl /= (word_count - 2)
            print(&quotperplexity:&quot + str(ppl))
            <a id="change">ppl_list.append(ppl)</a>
    return ppl_list


def infer_generate():</code></pre><h3>After Change</h3><pre><code class='java'>


def ppl(sentence_list):
    <a id="change">result = dict()</a>
    &#47&#47 load data dict
    word_to_idx = load_word_dict(config.word_dict_path)
    idx_to_word = {v: k for k, v in word_to_idx.items()}
    &#47&#47 init params
    batch_size = 1
    tf.reset_default_graph()
    input_data = tf.placeholder(tf.int32, [batch_size, None])
    output_targets = tf.placeholder(tf.int32, [batch_size, None])
    &#47&#47 init model
    end_points = rnn_model(model=&quotlstm&quot,
                           input_data=input_data,
                           output_data=output_targets,
                           vocab_size=len(word_to_idx),
                           rnn_size=128,
                           num_layers=2,
                           batch_size=batch_size,
                           learning_rate=config.learning_rate)
    saver = tf.train.Saver(tf.global_variables())
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    with tf.Session() as sess:
        &#47&#47 init op
        sess.run(init_op)
        checkpoint = tf.train.latest_checkpoint(config.model_dir)
        saver.restore(sess, checkpoint)
        print("loading model from the checkpoint {0}".format(checkpoint))

        &#47&#47 infer each sentence
        for sentence in sentence_list:
            ppl = 0
            &#47&#47 data idx
            x = [word_to_idx[c] if c in word_to_idx else word_to_idx[UNK_TOKEN] for c in sentence]
            x = [word_to_idx[START_TOKEN]] + x + [word_to_idx[END_TOKEN]]
            &#47&#47 print(&quotx:&quot, x)
            &#47&#47 reshape
            y = np.array(x[1:]).reshape((-1, batch_size))
            x = np.array(x[:-1]).reshape((-1, batch_size))
            &#47&#47 get each word perplexity
            word_count = x.shape[0]
            for i in range(word_count):
                perplexity = sess.run(end_points[&quotperplexity&quot],
                                      feed_dict={input_data: x[i:i + 1, :],
                                                 output_targets: y[i:i + 1, :]})
                print(&quot{0} -&gt; {1}, perplexity: {2}&quot.format(idx_to_word[x[i:i + 1, :].tolist()[0][0]],
                                                           idx_to_word[y[i:i + 1, :].tolist()[0][0]],
                                                           perplexity))
                if i == 0 or i == word_count:
                    continue
                ppl += perplexity
            ppl /= (word_count - 2)
            <a id="change">result[sentence] = ppl</a>
    return result


def infer_generate():</code></pre>