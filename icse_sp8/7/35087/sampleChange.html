<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if idx % 10 == 0:
            total_rewards = exp_source.pop_total_rewards()
            <a id="change">if total_rewards:
                mean_reward = np.mean(total_rewards)
                print("%d: Mean reward: %.2f, done: %d, epsilon: %.4f" % (
                    idx, mean_reward, len(total_rewards), action_selector.epsilon
                ))
                if mean_reward &gt; run.getfloat("defaults", "stop_mean_reward", fallback=2*abs(mean_reward)):
                    print("We&quotve reached mean reward bound, exit")
                    break
            else:
                print("%d: no reward info, epsilon: %.4f" % (idx, action_selector.epsilon))
   </a> env.close()
    pass
</code></pre><h3>After Change</h3><pre><code class='java'>
            q_vals.append(train_q)
        return torch.from_numpy(np.array(states, dtype=np.float32)), torch.stack(q_vals)

    <a id="change">reward_sma = utils.SMAQueue(run.getint("stop", "mean_games", fallback=100))</a>

    for idx in range(10000):
        exp_replay.populate(run.getint("exp_buffer", "populate"))

        for batch in exp_replay.batches(run.getint("learning", "batch_size")):
            optimizer.zero_grad()

            &#47&#47 populate buffer
            states, q_vals = batch_to_train(batch)
            &#47&#47 ready to train
            states, q_vals = Variable(states), Variable(q_vals)
            if cuda_enabled:
                states = states.cuda()
                q_vals = q_vals.cuda()
            l = loss_fn(model(states), q_vals)
            l.backward()
            optimizer.step()

        action_selector.epsilon *= run.getfloat("defaults", "epsilon_decay")

        if idx % 10 == 0:
            total_rewards = exp_source.pop_total_rewards()
            <a id="change">reward_sma</a> += total_rewards
            mean_reward = reward_sma.mean()
            print("%d: Mean reward: %.2f, done: %d, epsilon: %.4f" % (
                idx, mean_reward, len(total_rewards), action_selector.epsilon</code></pre>