<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Make future reward 0 if the current state is done
        float_data_list = [
            &quotstates&quot, &quotactions&quot, &quotrewards&quot, &quotdones&quot, &quotnext_states&quot]
        <a id="change">for k in float_data_list:
            batch[k] = Variable(torch.from_numpy(batch[k]).float())
        &#47&#47 print(&quotbatch&quot)
        &#47&#47 print(batch[&quotstates&quot])
        &#47&#47 print(batch[&quotactions&quot])
        &#47&#47 print(batch[&quotrewards&quot])
        &#47&#47 print(batch[&quotdones&quot])
        &#47&#47 print(1 - batch[&quotdones&quot])
       </a> q_vals = self.net.wrap_eval(batch[&quotstates&quot])
        &#47&#47 print(f&quotq_vals {q_vals}&quot)
        q_targets_all = batch[&quotrewards&quot].data + self.gamma * \
            torch.mul((1 - batch[&quotdones&quot].data),</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Depending on the algorithm this is either the current
        &#47&#47 net or target net
        q_next_st_vals = self.eval_net.wrap_eval(batch[&quotnext_states&quot])
        <a id="change">idx = torch.from_numpy(np.array(list(range(self.batch_size))))</a>
        <a id="change">q_next_st_vals_max = q_next_st_vals[idx, q_next_actions]</a>
        q_next_st_vals_max.unsqueeze_(1)
        &#47&#47 Compute final q_target using reward and estimated
        &#47&#47 best Q value from the next state if there is one
        &#47&#47 Make future reward 0 if the current state is done</code></pre>