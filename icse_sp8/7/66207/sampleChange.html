<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        state = self.init_hidden(batch_size)

        &#47&#47 outputs = []
        outputs = <a id="change">Variable(fc_feats.data.new(batch_size, seq.size(1) - 1, self.vocab_size+1).zero_())</a>

        fc_feats, att_feats, p_att_feats = self._prepare_feature(fc_feats, att_feats, att_masks)

        for i in range(seq.size(1) - 1):
            if self.training and i &gt;= 1 and self.ss_prob &gt; 0.0: &#47&#47 otherwiste no need to sample
                sample_prob = fc_feats.data.new(batch_size).uniform_(0, 1)
                sample_mask = sample_prob &lt; self.ss_prob
                if sample_mask.sum() == 0:
                    it = seq[:, i].clone()
                else:
                    sample_ind = sample_mask.nonzero().view(-1)
                    it = seq[:, i].data.clone()
                    &#47&#47prob_prev = torch.exp(outputs[-1].data.index_select(0, sample_ind)) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    &#47&#47it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1))
                    &#47&#47 prob_prev = torch.exp(outputs[-1].data) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    prob_prev = torch.exp(<a id="change">outputs[:, i-1].data</a>) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1).index_select(0, sample_ind))
                    it = Variable(it, requires_grad=False)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
        state = self.init_hidden(batch_size)

        &#47&#47 outputs = []
        outputs = <a id="change">fc_feats.new_zeros(batch_size, seq.size(1) - 1, self.vocab_size+1)</a>

        fc_feats, att_feats, p_att_feats = self._prepare_feature(fc_feats, att_feats, att_masks)

        for i in range(seq.size(1) - 1):
            if self.training and i &gt;= 1 and self.ss_prob &gt; 0.0: &#47&#47 otherwiste no need to sample
                sample_prob = fc_feats.new(batch_size).uniform_(0, 1)
                sample_mask = sample_prob &lt; self.ss_prob
                if sample_mask.sum() == 0:
                    it = seq[:, i].clone()
                else:
                    sample_ind = sample_mask.nonzero().view(-1)
                    it = seq[:, i].data.clone()
                    &#47&#47prob_prev = torch.exp(outputs[-1].data.index_select(0, sample_ind)) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    &#47&#47it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1))
                    &#47&#47 prob_prev = torch.exp(outputs[-1].data) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    prob_prev = torch.exp(<a id="change">outputs[:, i-1].detach()</a>) &#47&#47 fetch prev distribution: shape Nx(M+1)
                    it.index_copy_(0, sample_ind, torch.multinomial(prob_prev, 1).view(-1).index_select(0, sample_ind))
            else:
                it = seq[:, i].clone()          </code></pre>