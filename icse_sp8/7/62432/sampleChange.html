<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        model = state.model
        self.zero_grads(state)
        for i, (batch_id, (inputs, targets, context)) in enumerate(samples):
            <a id="change">if cuda.DISTRIBUTED_WORLD_SIZE &gt; 1:
                &#47&#47 Whenever *samples* contains more than one mini-batch, we
                &#47&#47 want to accumulate gradients locally and only call
                &#47&#47 all-reduce in the last backwards pass.
                if i &lt; sample_size - 1:
                    &#47&#47 sync gradients in the last sample backward
                    model.accumulate_gradients(True)
                else:
                    model.accumulate_gradients(False)

            &#47&#47 pass context to model to use in forward call if needed
           </a> model.contextualize(context)
            with timing.time("model.forward"):
                logits = model(*inputs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        model = state.model
        self.zero_grads(state)
        for idx, (batch_id, (inputs, targets, context)) in enumerate(samples):
            <a id="change">with maybe_no_sync(model, idx, sample_size):
                &#47&#47 pass context to model to use in forward call if needed
                model.contextualize(context)
                with timing.time("model.forward"):
                    logits = model(*inputs)

                with timing.time("compute loss"):
                    loss = precision.maybe_float(
                        model.get_loss(logits, targets, context)
                    )
                    if BatchContext.IGNORE_LOSS in context:
                        loss *= 0
                    elif sample_size &gt; 1:
                        &#47&#47 gradients averaged per batch and accumulated across samples.
                        &#47&#47 divide sample_size to let gradients averaged per example
                        loss = loss / sample_size

                self.backprop(state, loss)

           </a> if report_metric:
                with timing.time("get pred"):
                    preds, scores = model.get_pred(
                        logits, targets, context, state.stage, *inputs</code></pre>