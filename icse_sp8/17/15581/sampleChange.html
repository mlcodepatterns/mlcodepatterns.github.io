<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        logits_train,
        logits_val,
        logits_test) = makeBiasParamsAndApplyToFms(self.input["train"], self.input["val"], self.input["test"])
        <a id="change">self._params</a> = <a id="change">self._params + [self._b]</a>
        
        &#47&#47 ============ Softmax ==============
        self.p_y_given_x_train = tf.nn.softmax(logits_train/t, axis=1)
        self.y_pred_train = tf.argmax(self.p_y_given_x_train, axis=1)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 NOTE: So, two biases are associated with this layer. self.b which is added in the ouput of the previous layer&quots output of conv,
        &#47&#47 and this self._bClassLayer that is added only to this final output before the softmax.
        
        <a id="change">bias_layer = BiasLayer(self.input["train"].shape[1])</a>
        <a id="change">logits_train = bias_layer.apply(self.input["train"])</a>
        <a id="change">logits_val = bias_layer.apply(self.input["val"])</a>
        <a id="change">logits_test = bias_layer.apply(self.input["test"])</a>
        <a id="change">self._params</a> = <a id="change">self._params + bias_layer.trainable_params()</a>
        
        
        &#47&#47 ============ Softmax ==============
        self.p_y_given_x_train = tf.nn.softmax(logits_train/t, axis=1)</code></pre>