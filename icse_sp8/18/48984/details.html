<html><h3>775122950d145382146e9120308432a9faf9a9b8,fairseq/optim/adam.py,Adam,step,#Adam#Any#,131
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    raise RuntimeError(&quotAdam does not support sparse gradients, please consider SparseAdam instead&quot)
                amsgrad = group[&quotamsgrad&quot]

                p_data_fp32 = <a id="change">p.data.float()</a>

                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    &#47&#47 Exponential moving average of gradient values
                    <a id="change">state[&quotexp_avg&quot]</a> = torch.zeros_like(p_data_fp32)
                    &#47&#47 Exponential moving average of squared gradient values
                    <a id="change">state[&quotexp_avg_sq&quot]</a> = torch.zeros_like(p_data_fp32)
                    if amsgrad:
                        &#47&#47 Maintains max of all exp. moving avg. of sq. grad. values
                        state[&quotmax_exp_avg_sq&quot] = torch.zeros_like(p_data_fp32)
                else:
                    <a id="change">state[&quotexp_avg&quot]</a> = state[&quotexp_avg&quot].type_as(p_data_fp32)
                    state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot].type_as(p_data_fp32)
                    if amsgrad:
                        <a id="change">state[&quotmax_exp_avg_sq&quot]</a> = state[&quotmax_exp_avg_sq&quot].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&quotexp_avg&quot], state[&quotexp_avg_sq&quot]
                if amsgrad:</code></pre><h3>After Change</h3><pre><code class='java'>
                amsgrad = group[&quotamsgrad&quot]

                p_data_fp32 = p.data
                <a id="change">if p.data.dtype in {torch.float16, torch.bfloat16}:
                    p_data_fp32 = p_data_fp32.float()

               </a> state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    &#47&#47 Exponential moving average of gradient values
                    <a id="change">state[&quotexp_avg&quot]</a> = torch.zeros_like(p_data_fp32)
                    &#47&#47 Exponential moving average of squared gradient values
                    <a id="change">state[&quotexp_avg_sq&quot]</a> = torch.zeros_like(p_data_fp32)
                    if amsgrad:
                        &#47&#47 Maintains max of all exp. moving avg. of sq. grad. values
                        state[&quotmax_exp_avg_sq&quot] = torch.zeros_like(p_data_fp32)
                else:
                    <a id="change">state[&quotexp_avg&quot]</a> = state[&quotexp_avg&quot].to(p_data_fp32)
                    state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot].to(p_data_fp32)
                    if amsgrad:
                        <a id="change">state[&quotmax_exp_avg_sq&quot]</a> = state[&quotmax_exp_avg_sq&quot].to(p_data_fp32)

                exp_avg, exp_avg_sq = state[&quotexp_avg&quot], state[&quotexp_avg_sq&quot]
                if amsgrad:</code></pre><img src="227153211.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 17</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/775122950d145382146e9120308432a9faf9a9b8#diff-d0f1814d2f162ba7c67209130e8a071048855102aa528fbdae9b64fe5a0ab649L146' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 775122950d145382146e9120308432a9faf9a9b8</div><div id='time'> Time: 2020-05-18</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/optim/adam.py</div><div id='class'> Class Name: Adam</div><div id='method'> Method Name: step</div><BR><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/775122950d145382146e9120308432a9faf9a9b8#diff-69b10cc6379afd44ce1460cf824253135af1da7d2b98dcb39808c2d580d6a98bL114' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 775122950d145382146e9120308432a9faf9a9b8</div><div id='time'> Time: 2020-05-18</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/optim/adamax.py</div><div id='class'> Class Name: Adamax</div><div id='method'> Method Name: step</div><BR><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/775122950d145382146e9120308432a9faf9a9b8#diff-c8ff6e9ad677733577ae29b16bab6bcd5f2a83bdb9fb9e5e7f79ee3f42939b1cL158' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 775122950d145382146e9120308432a9faf9a9b8</div><div id='time'> Time: 2020-05-18</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/optim/adafactor.py</div><div id='class'> Class Name: Adafactor</div><div id='method'> Method Name: step</div><BR>