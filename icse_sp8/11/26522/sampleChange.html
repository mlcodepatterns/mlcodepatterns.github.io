<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Add uncertainty scores to our unlabeled data, and keep a copy of our unlabeled data.
    unlabeled_uncertainty = np.concatenate((unlabeled, np.expand_dims(uncertainty_scores, axis=1)), axis=1)
    <a id="change">unlabeled_uncertainty_copy = np.copy(unlabeled_uncertainty)</a>

    &#47&#47 Define our record container and the maximum number of records to sample.
    instance_index_ranking = []
    ceiling = np.minimum(unlabeled.shape[0], n_instances)

    &#47&#47 TODO (dataframing) is there a better way to do this? Inherently sequential.
    for _ in range(ceiling):

        &#47&#47 Select the instance from our unlabeled copy that scores highest.
        raw_instance = select_instance(X_training=labeled, X_uncertainty=unlabeled_uncertainty_copy)
        instance = np.expand_dims(raw_instance, axis=1)

        &#47&#47 Find our record&quots index in both the original unlabeled and our uncertainty copy.
        instance_index_original = np.where(np.all(unlabeled == raw_instance, axis=1))[0][0]
        <a id="change">instance_index_copy = np.where(np.all(unlabeled_uncertainty_copy[:, :-1] == instance.T, axis=1))[0][0]</a>

        &#47&#47 Add our instance we&quotve considered for labeling to our labeled set. Although we don&quott
        &#47&#47 know it&quots label, we want further iterations to consider the newly-added instance so
        &#47&#47 that we don&quott query the same instance redundantly.
        labeled = np.concatenate((labeled, instance.T), axis=0)

        &#47&#47 Remove our instance from the unlabeled set and append it to our list of records to label.
        <a id="change">unlabeled_uncertainty_copy = np.delete(unlabeled_uncertainty_copy, instance_index_copy, axis=0)</a>
        instance_index_ranking.append(instance_index_original)

    &#47&#47 Return numpy array, not a list.
    return np.array(instance_index_ranking)</code></pre><h3>After Change</h3><pre><code class='java'>
    unlabeled_uncertainty = np.concatenate((unlabeled, expanded_uncertainty_scores), axis=1)

    &#47&#47 Define our null row, which will be filtered during the select_instance call.
    <a id="change">null_row = np.ones(shape=(unlabeled_uncertainty.shape[1],)) * np.nan</a>

    &#47&#47 Define our record container and the maximum number of records to sample.
    instance_index_ranking = []
    ceiling = np.minimum(unlabeled.shape[0], n_instances)

    for _ in range(ceiling):

        &#47&#47 Receive the instance and corresponding index from our unlabeled copy that scores highest.
        instance_index, instance = select_instance(
            X_training=labeled, X_uncertainty=unlabeled_uncertainty
        )

        &#47&#47 Prepare our most informative instance for concatenation.
        expanded_instance = np.expand_dims(instance, axis=0)

        &#47&#47 Add our instance we&quotve considered for labeling to our labeled set. Although we don&quott
        &#47&#47 know it&quots label, we want further iterations to consider the newly-added instance so
        &#47&#47 that we don&quott query the same instance redundantly.
        labeled = np.concatenate((labeled, expanded_instance), axis=0)

        &#47&#47 We "remove" our instance from the unlabeled set by setting that row to an array
        &#47&#47 of np.nan and filtering within select_instance.
        <a id="change">unlabeled_uncertainty[instance_index] = null_row</a>

        &#47&#47 Finally, append our instance&quots index to the bottom of our ranking.
        instance_index_ranking.append(instance_index)
</code></pre>