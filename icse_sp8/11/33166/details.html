<html><h3>ba2af6efeb086391d286ee5d86635f723e806c15,fonduer/parser/parser.py,SimpleTokenizer,parse,#SimpleTokenizer#Any#Any#,30
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                                 for x in words]))[:-1]
            text = &quot &quot.join(words)
            stable_id = construct_stable_id(document, &quotphrase&quot, i, i)
            <a id="change">yield {
                &quottext&quot: text,
                &quotwords&quot: words,
                &quotchar_offsets&quot: char_offsets,
                &quotstable_id&quot: stable_id
            }</a>
            i += 1


class OmniParser(UDFRunner):</code></pre><h3>After Change</h3><pre><code class='java'>
            if not len(text.strip()):
                continue
            words = text.split()
            char_offsets = [0] + [int(_) <a id="change">for</a> _ in np.cumsum([len(x) + 1
                                  for x in words])[:-1]]
            text = &quot &quot.join(words)
            stable_id = construct_stable_id(document, &quotphrase&quot, i, i)
            <a id="change">yield {
                &quottext&quot: text,
                &quotwords&quot: words,
                &quotpos_tags&quot: [&quot&quot] * len(words),
                &quotner_tags&quot: [&quot&quot] * len(words),
                &quotlemmas&quot: [&quot&quot] * len(words),
                &quotdep_parents&quot: [0] * len(words),
                &quotdep_labels&quot: [&quot&quot] * len(words),
                &quotchar_offsets&quot: char_offsets,
                &quotabs_char_offsets&quot: char_offsets,
                &quotstable_id&quot: stable_id
            }</a>
            i += 1


class OmniParser(UDFRunner):</code></pre><img src="162830772.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/HazyResearch/fonduer/commit/ba2af6efeb086391d286ee5d86635f723e806c15#diff-912a53e62dfa0930fcab69e482b1f874f18fd093c1e137b42274969c0ef3d70eL35' target='_blank'>Link</a></div><div id='project'> Project Name: HazyResearch/fonduer</div><div id='commit'> Commit Name: ba2af6efeb086391d286ee5d86635f723e806c15</div><div id='time'> Time: 2018-05-17</div><div id='author'> Author: senwu@cs.stanford.edu</div><div id='file'> File Name: fonduer/parser/parser.py</div><div id='class'> Class Name: SimpleTokenizer</div><div id='method'> Method Name: parse</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/datasets/commit/52ba3c53fdeb5806b3626b873eefabf8c065d9d4#diff-471344476ea5f15e8c7ae3d43d285101ab244a4bae20509fb3fbcafbe77a05ffL164' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/datasets</div><div id='commit'> Commit Name: 52ba3c53fdeb5806b3626b873eefabf8c065d9d4</div><div id='time'> Time: 2019-05-21</div><div id='author'> Author: adarob@google.com</div><div id='file'> File Name: tensorflow_datasets/text/squad.py</div><div id='class'> Class Name: Squad</div><div id='method'> Method Name: _generate_examples</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/datasets/commit/3be1154314111df8cd327a321fe3d68e7b661c68#diff-79df8f0e2c4f225092d48390231b98f6efd51d9362f894a0ef125fe0dbdedcf9L108' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/datasets</div><div id='commit'> Commit Name: 3be1154314111df8cd327a321fe3d68e7b661c68</div><div id='time'> Time: 2020-05-29</div><div id='author'> Author: charlespatel07@gmail.com</div><div id='file'> File Name: tensorflow_datasets/text/pg19.py</div><div id='class'> Class Name: Pg19</div><div id='method'> Method Name: _generate_examples</div><BR>