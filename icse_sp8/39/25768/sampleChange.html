<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        num_sequences = layer_input.shape[1]
        num_samples = self._network.num_noise_samples
        num_classes = numpy.int64(self._network.vocabulary.num_classes())
        <a id="change">random = self._network.random</a>

        minibatch_size = num_time_steps * num_sequences
        <a id="change">if self._network.noise_probs is None:
            &#47&#47 The upper bound is exclusive, so this always creates samples that
            &#47&#47 are &lt; num_classes.
            num_batch_samples = minibatch_size * num_samples
            sample = random.uniform((num_batch_samples,)) * num_classes
            sample = sample.astype(&quotint64&quot)
        else:
            &#47&#47 We repeat the distribution for each mini-batch element, and sample
            &#47&#47 k noise words per mini-batch element. k &lt; number of outpus, so
            &#47&#47 it&quots possible without replacement.
            class_probs = self._network.noise_probs[None, :]
            class_probs = tensor.tile(class_probs, [minibatch_size, 1])
            &#47&#47 Since we sample different noise words for different data words, we
            &#47&#47 could set the probability of the correct data words to zero, as
            &#47&#47 suggested in the BlackOut paper. That seems to result in a little
            &#47&#47 bit worse model with NCE and BlackOut.
&#47&#47            target_class_ids = self._network.target_class_ids.flatten()
&#47&#47            target_sample_ids = tensor.arange(minibatch_size)
&#47&#47            class_probs = tensor.set_subtensor(
&#47&#47                class_probs[(target_sample_ids, target_class_ids)], 0)
&#47&#47            denominators = class_probs.sum(1)
&#47&#47            denominators = denominators[:, None]
&#47&#47            class_probs /= denominators
            sample = multinomial(random, class_probs, num_samples)
            &#47&#47 For some reason (maybe a rounding error) it may happen that the
            &#47&#47 sample contains a very high or negative value.
            sample = tensor.maximum(sample, 0)
            sample = tensor.minimum(sample, num_classes - 1)

       </a> sample = sample.reshape([num_time_steps, num_sequences, num_samples])
        return sample, self._get_target_preact(layer_input, sample)

    def _get_seqshared_sample_tensors(self, layer_input):</code></pre><h3>After Change</h3><pre><code class='java'>
        num_sequences = layer_input.shape[1]
        num_samples = self._network.num_noise_samples
        num_classes = numpy.int64(self._network.vocabulary.num_classes())
        <a id="change">noise_sampler = self._network.noise_sampler</a>

        minibatch_size = num_time_steps * num_sequences
        <a id="change">sample = noise_sampler.sample(minibatch_size, num_samples)</a>
        sample = sample.reshape([num_time_steps, num_sequences, num_samples])
        return sample, self._get_target_preact(layer_input, sample)

    def _get_seqshared_sample_tensors(self, layer_input):</code></pre>