<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Validation data loader
        if dataset_valid is not None:
            <a id="change">if isinstance(dataset_valid, OfflineDataset):
                if distributed_world_size == 1:
                    dataloader_valid = DataLoader(dataset_valid, batch_sampler=TraceBatchSampler(dataset_valid, batch_size=batch_size, shuffle_batches=True), num_workers=dataloader_offline_num_workers, collate_fn=lambda x: Batch(x))
                else:
                    dataloader_valid = DataLoader(dataset_valid, batch_sampler=DistributedTraceBatchSampler(dataset_valid, batch_size=batch_size, num_buckets=distributed_num_buckets, shuffle_batches=True, shuffle_buckets=True), num_workers=dataloader_offline_num_workers, collate_fn=lambda x: Batch(x))
            else:
                dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, num_workers=0, collate_fn=lambda x: Batch(x))

       </a> while not stop:
            epoch += 1
            for i_batch, batch in enumerate(dataloader):
                &#47&#47 Important, a self._distributed_sync_parameters() needs to happen at the very beginning of a training</code></pre><h3>After Change</h3><pre><code class='java'>
            else:
                dataloader_valid = DataLoader(dataset_valid, batch_sampler=DistributedTraceBatchSampler(dataset_valid, batch_size=batch_size, num_buckets=distributed_num_buckets, shuffle_batches=True, shuffle_buckets=True), num_workers=dataloader_offline_num_workers, collate_fn=lambda x: Batch(x))
            if not self._layers_pre_generated:
                <a id="change">for i_batch, batch in enumerate(dataloader_valid):
                    self._polymorph(batch)

       </a> while not stop:
            epoch += 1
            for i_batch, batch in enumerate(dataloader):
                &#47&#47 Important, a self._distributed_sync_parameters() needs to happen at the very beginning of a training</code></pre>