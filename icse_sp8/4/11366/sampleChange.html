<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        flin = gswap
                        flin = []
                        for grad, jg in zip(gswap, Jgrads):
                            <a id="change">if jg is None:
                                print("JG NONE", grad)
                                flin += [grad]
                            else:
                                flin += [grad + jg * self._beta]
                            
                       </a> step3 = zip(flin, var_list)
                        op6 = self.optimizer.apply_gradients(step3, global_step=global_step, name=name)
                        with tf.get_default_graph().control_dependencies([op6]):
                            return tf.no_op()</code></pre><h3>After Change</h3><pre><code class='java'>
                            consensus_reg = 0.5 * sum(
                                    tf.reduce_sum(tf.square(g)) for g in all_grads[:len(d_vars)] if g is not None
                            )
                            Jgrads = <a id="change">tf.gradients(consensus_reg, d_vars)</a> + [tf.zeros_like(g) for g in g_vars]
                            op7 = [tf.assign_sub(v, (jg * self._beta)) if jg is not None else tf.assign_sub(v,grad) <a id="change">for</a> v,grad, jg in zip(var_list, all_grads, Jgrads)]
                            with tf.get_default_graph().control_dependencies(op7):
                                return tf.no_op()
</code></pre>