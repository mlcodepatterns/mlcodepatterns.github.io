<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def __init__(self,
                 n_class=None, pretrained_model=None, mean=None,
                 initialW=None, initial_bias=None):
        <a id="change">if n_class is None:
            if pretrained_model in self._models:
                n_class = self._models[pretrained_model][&quotn_class&quot]
            else:
                n_class = 1000

       </a> if mean is None:
            if pretrained_model in self._models:
                mean = self._models[pretrained_model][&quotmean&quot]
            else:
                mean = _imagenet_mean
        self.mean = mean

        if initialW is None:
            &#47&#47 Employ default initializers used in the original paper.
            initialW = normal.Normal(0.01)
        if pretrained_model:
            &#47&#47 As a sampling process is time-consuming,
            &#47&#47 we employ a zero initializer for faster computation.
            initialW = constant.Zero()
        kwargs = {&quotinitialW&quot: initialW, &quotinitial_bias&quot: initial_bias}

        super(VGG16, self).__init__()
        with self.init_scope():
            self.conv1_1 = Conv2DActiv(None, 64, 3, 1, 1, **kwargs)
            self.conv1_2 = Conv2DActiv(None, 64, 3, 1, 1, **kwargs)
            self.pool1 = _max_pooling_2d
            self.conv2_1 = Conv2DActiv(None, 128, 3, 1, 1, **kwargs)
            self.conv2_2 = Conv2DActiv(None, 128, 3, 1, 1, **kwargs)
            self.pool2 = _max_pooling_2d
            self.conv3_1 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.conv3_2 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.conv3_3 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.pool3 = _max_pooling_2d
            self.conv4_1 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv4_2 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv4_3 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.pool4 = _max_pooling_2d
            self.conv5_1 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv5_2 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv5_3 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.pool5 = _max_pooling_2d
            self.fc6 = Linear(None, 4096, **kwargs)
            self.fc6_relu = relu
            self.fc6_dropout = dropout
            self.fc7 = Linear(None, 4096, **kwargs)
            self.fc7_relu = relu
            self.fc7_dropout = dropout
            self.fc8 = Linear(None, n_class, **kwargs)
            self.prob = softmax

        <a id="change">if pretrained_model in self._models:
            path = download_model(self._models[pretrained_model][&quoturl&quot])
            chainer.serializers.load_npz(path, self)
        elif pretrained_model:
            chainer.serializers.load_npz(pretrained_model, self)


</a>def _max_pooling_2d(x):
    return max_pooling_2d(x, ksize=2)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self,
                 n_class=None, pretrained_model=None, mean=None,
                 initialW=None, initial_bias=None):
        <a id="change">n_class, path = prepare_link_initialization(
            n_class, pretrained_model, self._models, False,
            default_out_channels=1000)</a>

        if mean is None:
            if pretrained_model in self._models:
                mean = self._models[pretrained_model][&quotmean&quot]
            else:
                mean = _imagenet_mean
        self.mean = mean

        if initialW is None:
            &#47&#47 Employ default initializers used in the original paper.
            initialW = normal.Normal(0.01)
        if pretrained_model:
            &#47&#47 As a sampling process is time-consuming,
            &#47&#47 we employ a zero initializer for faster computation.
            initialW = constant.Zero()
        kwargs = {&quotinitialW&quot: initialW, &quotinitial_bias&quot: initial_bias}

        super(VGG16, self).__init__()
        with self.init_scope():
            self.conv1_1 = Conv2DActiv(None, 64, 3, 1, 1, **kwargs)
            self.conv1_2 = Conv2DActiv(None, 64, 3, 1, 1, **kwargs)
            self.pool1 = _max_pooling_2d
            self.conv2_1 = Conv2DActiv(None, 128, 3, 1, 1, **kwargs)
            self.conv2_2 = Conv2DActiv(None, 128, 3, 1, 1, **kwargs)
            self.pool2 = _max_pooling_2d
            self.conv3_1 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.conv3_2 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.conv3_3 = Conv2DActiv(None, 256, 3, 1, 1, **kwargs)
            self.pool3 = _max_pooling_2d
            self.conv4_1 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv4_2 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv4_3 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.pool4 = _max_pooling_2d
            self.conv5_1 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv5_2 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.conv5_3 = Conv2DActiv(None, 512, 3, 1, 1, **kwargs)
            self.pool5 = _max_pooling_2d
            self.fc6 = Linear(None, 4096, **kwargs)
            self.fc6_relu = relu
            self.fc6_dropout = dropout
            self.fc7 = Linear(None, 4096, **kwargs)
            self.fc7_relu = relu
            self.fc7_dropout = dropout
            self.fc8 = Linear(None, n_class, **kwargs)
            self.prob = softmax

        <a id="change">if path:
            chainer.serializers.load_npz(path, self)


</a>def _max_pooling_2d(x):
    return max_pooling_2d(x, ksize=2)
</code></pre>