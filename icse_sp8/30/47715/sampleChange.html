<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._l2_reg_dual = float(l2_reg_dual)
        self._l2_reg_loss = float(l2_reg_loss)

        <a id="change">super(REPS, self).__init__(env_spec=env_spec,
                                   policy=policy,
                                   baseline=baseline,
                                   max_path_length=max_path_length,
                                   discount=discount,
                                   gae_lambda=gae_lambda,
                                   center_adv=center_adv,
                                   positive_adv=positive_adv,
                                   fixed_horizon=fixed_horizon)</a>

    def init_opt(self):
        Initialize the optimization procedure.
        pol_loss_inputs, pol_opt_inputs, dual_opt_inputs = self._build_inputs()</code></pre><h3>After Change</h3><pre><code class='java'>
        optimizer_args = optimizer_args or dict(max_opt_itr=50)
        dual_optimizer_args = dual_optimizer_args or dict(maxiter=50)

        <a id="change">self.policy = policy</a>
        <a id="change">self.max_path_length = max_path_length</a>

        <a id="change">self._env_spec = env_spec</a>
        <a id="change">self._baseline = baseline</a>
        <a id="change">self._discount = discount</a>
        <a id="change">self._gae_lambda = gae_lambda</a>
        <a id="change">self._center_adv = center_adv</a>
        <a id="change">self._positive_adv = positive_adv</a>
        <a id="change">self._fixed_horizon = fixed_horizon</a>
        <a id="change">self._flatten_input = True</a>

        self._name = name
        self._name_scope = tf.name_scope(self._name)
        self._old_policy = policy.clone(&quotold_policy&quot)

        self._feat_diff = None
        self._param_eta = None
        self._param_v = None
        self._f_dual = None
        self._f_dual_grad = None
        self._f_policy_kl = None

        self._optimizer = optimizer(**optimizer_args)
        self._dual_optimizer = dual_optimizer
        self._dual_optimizer_args = dual_optimizer_args
        self._epsilon = float(epsilon)
        self._l2_reg_dual = float(l2_reg_dual)
        self._l2_reg_loss = float(l2_reg_loss)

        <a id="change">self._episode_reward_mean = collections.deque(maxlen=100)</a>
        if policy.vectorized:
            <a id="change">self.sampler_cls = OnPolicyVectorizedSampler</a>
        else:
            <a id="change">self.sampler_cls = BatchSampler</a>

        <a id="change">self.init_opt()</a>

    def init_opt(self):
        Initialize the optimization procedure.
        pol_loss_inputs, pol_opt_inputs, dual_opt_inputs = self._build_inputs()</code></pre>