<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                op5 = tf.group(*[ tf.assign(w,v) for w,v in zip(restored_vars, tmp_vars)])
                with tf.get_default_graph().control_dependencies([op5]):
                    &#47&#47 Flin = gamma * IF - rho * JF + beta * JtF
                    <a id="change">op7 = tf.group(*[tf.assign_add(gsw, (jg * self._beta)) if jg is not None else tf.no_op() for gsw, jg in zip(gswap, Jgrads)])</a>
                    with tf.get_default_graph().control_dependencies([op7]):
                        flin_grads_and_vars = zip(gswap, var_list)
                        &#47&#47 step 1
                        op8 = self.optimizer.apply_gradients(list(flin_grads_and_vars).copy(), global_step=global_step, name=name)</code></pre><h3>After Change</h3><pre><code class='java'>
                with tf.get_default_graph().control_dependencies([op5]):
                    flin = []
                    for grad, jg in zip(gswap, Jgrads):
                        <a id="change">if jg is None:
                            print("JG NONE", grad)
                            flin += [grad]
                        else:
                            flin += [grad + jg * self._beta]

                   </a> step3 = list(zip(flin, var_list))
                    op6 = self.optimizer.apply_gradients(step3.copy(), global_step=global_step, name=name)
                    with tf.get_default_graph().control_dependencies([op6]):
                        return tf.no_op()</code></pre>