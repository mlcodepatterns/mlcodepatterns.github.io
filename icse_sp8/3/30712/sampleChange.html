<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )

        x = x.astype(ART_NUMPY_DTYPE)
        x_adv = <a id="change">np.zeros_like(x, dtype=ART_NUMPY_DTYPE)</a>

        &#47&#47 Compute perturbation with implicit batching
        for i_batch in range(int(np.ceil(x.shape[0] / self.batch_size))):
</code></pre><h3>After Change</h3><pre><code class='java'>

        x_batch = x_batch + np.random.normal(scale=self.sigma, size=x_batch)

        <a id="change">y_batch = np.repeat(y, repeats=self.batch_size, axis=0)</a>

        perturbation = (
            np.random.uniform(
                low=self.estimator.clip_values[0], high=self.estimator.clip_values[1], size=x.shape
            ).astype(ART_NUMPY_DTYPE)
            - (self.estimator.clip_values[1] - self.estimator.clip_values[0]) / 2
        )

        for _ in range(self.nb_steps):

            gradients_ce = np.mean(
                self.estimator.loss_gradient(x=x_batch + perturbation, y=y_batch, sampling=False)
                * (1 - 2 * int(self.targeted)),
                axis=0,
                keepdims=True,
            )

            gradients = gradients_ce - self._get_regularisation_loss_gradients(perturbation)

            perturbation += self.learning_rate * gradients

        x_p = x_batch + perturbation

        x_adv = <a id="change">np.clip(x_p, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(
            ART_NUMPY_DTYPE
        )</a>

        return x_adv

    def _get_regularisation_loss_gradients(self, perturbation):</code></pre>