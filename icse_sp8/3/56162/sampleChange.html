<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
          time_hook.compute_speed(len(results) * eval_batch_size)))
  elif mode == &quotexport_savedmodel&quot:
    tf.logging.info(&quotStarting exporting saved model ...&quot)
    <a id="change">image_classifier.export_saved_model(
        export_dir_base=model_dir + &quot/export_savedmodel/&quot,
        serving_input_receiver_fn=build_tensor_serving_input_receiver_fn(
            [hparams.image_size, hparams.image_size, 3],
            batch_size=hparams.eval_batch_size),
        as_text=True)</a>
  else:  &#47&#47 default to train mode.
    current_step = _load_global_step_from_checkpoint_dir(model_dir)
    total_step = int(hparams.num_epochs * train_steps_per_epoch)
    if current_step &lt; total_step:</code></pre><h3>After Change</h3><pre><code class='java'>
        serving_input_receiver_fn=build_image_serving_input_receiver_fn(
            serving_shape),
        as_text=True)
    <a id="change">if FLAGS.add_warmup_requests:
      write_warmup_requests(
          export_path,
          FLAGS.model_name,
          hparams.image_size,
          batch_sizes=FLAGS.inference_batch_sizes)


</a>if __name__ == &quot__main__&quot:
  tf.logging.set_verbosity(tf.logging.INFO)
  app.run(main)
</code></pre>