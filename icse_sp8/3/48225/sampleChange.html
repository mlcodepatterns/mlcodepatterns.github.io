<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Flatten the param_groups, save the partition which logs the rank &lt;&gt; shard correspondence
        partition: List[Tuple[int, int]] = []
        param_groups: <a id="change">List[Dict[Any, Any]] = []</a>

        start = 0
        for i, s in enumerate(self._all_states):
            param_groups.extend(s["param_groups"])</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Unify the shard states and the state that pytorch would expect, given the model.
        &#47&#47 Indexation needs several redirections, since each shard only knows a limited scope of the model
        &#47&#47 - get the pytorch compliant parameter indexing
        state_dict = <a id="change">super().state_dict()</a>

        &#47&#47 - go through the per-shard states, which are all indexed locally
        for rank, s in enumerate(self._all_states):
            &#47&#47 -- match the local indexing and the global partition, update the corresponding saved state globally</code></pre>