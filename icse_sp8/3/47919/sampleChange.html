<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            y_true=q_t_selected_target, y_pred=q_t_selected)
    ]
    if policy.config["twin_q"]:
        <a id="change">critic_loss.append(0.5 * tf.keras.losses.MSE(
            y_true=q_t_selected_target, y_pred=twin_q_t_selected))</a>

    &#47&#47 Alpha- and actor losses.
    &#47&#47 Note: In the papers, alpha is used directly, here we take the log.
    &#47&#47 Discrete case: Multiply the action probs as weights with the original</code></pre><h3>After Change</h3><pre><code class='java'>
        td_error = base_td_error

    &#47&#47 Calculate one or two critic losses (2 in the twin_q case).
    <a id="change">prio_weights = tf.cast(train_batch[PRIO_WEIGHTS], tf.float32)</a>
    critic_loss = [<a id="change">tf.reduce_mean(prio_weights * huber_loss(base_td_error))</a>]
    if policy.config["twin_q"]:
        critic_loss.append(
            tf.reduce_mean(prio_weights * huber_loss(twin_td_error)))</code></pre>