<html><h3>2a11d3f0bad2430b81628fa6cb0a6301d099d77d,performer/fast_attention/tensorflow/fast_attention_test.py,TransformerLayersTest,test_softmax_noncausal_attention_block_output,#TransformerLayersTest#,53
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    num_heads = 1
    dim = 4
    num_random_features = 350
    <a id="change">query = tf.ones([batch_size, length, num_heads, dim])</a>
    key = tf.ones([batch_size, length, num_heads, dim])
    value = tf.ones([batch_size, length, num_heads, dim])
    kernel_transformation = fast_attention.softmax_kernel_transformation
    projection_matrix = fast_attention.create_projection_matrix(</code></pre><h3>After Change</h3><pre><code class='java'>
    attention_block_output = fast_attention.favor_attention(
        query, key, value, kernel_transformation, False, projection_matrix)

    query = tf.multiply(query, 1.0 / math.sqrt(<a id="change">float(dim)</a>))
    attention_scores = tf.einsum("BXHD,BYHD-&gt;BXYH", query, key)
    attention_scores = tf.nn.softmax(attention_scores, axis=2)
    <a id="change">exact_attention_block_output = tf.einsum("BXYH,BYHD-&gt;BXHD",
                                             attention_scores, value)</a>
    max_error = 2.0
    error = tf.math.abs(
        (exact_attention_block_output - attention_block_output) /
        exact_attention_block_output)</code></pre><img src="80338153.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/google-research/google-research/commit/2a11d3f0bad2430b81628fa6cb0a6301d099d77d#diff-78046a5477c219e0bcdfd0eeafa7661087d424f6d2883881054fe28bedb84dedL54' target='_blank'>Link</a></div><div id='project'> Project Name: google-research/google-research</div><div id='commit'> Commit Name: 2a11d3f0bad2430b81628fa6cb0a6301d099d77d</div><div id='time'> Time: 2020-12-11</div><div id='author'> Author: xingyousong@google.com</div><div id='file'> File Name: performer/fast_attention/tensorflow/fast_attention_test.py</div><div id='class'> Class Name: TransformerLayersTest</div><div id='method'> Method Name: test_softmax_noncausal_attention_block_output</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/9cf8f6cdf6a2008843cb37da6e34b8d10353b0bf#diff-40d0522b82c5b70c345cafb4ea88481bd3a3a6fb58079921a00ba83d2cb9f0c5L73' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 9cf8f6cdf6a2008843cb37da6e34b8d10353b0bf</div><div id='time'> Time: 2019-12-12</div><div id='author'> Author: kittipat@fb.com</div><div id='file'> File Name: ml/rl/preprocessing/sparse_to_dense.py</div><div id='class'> Class Name: PythonSparseToDenseProcessor</div><div id='method'> Method Name: process</div><BR><BR><div id='link'><a href='https://github.com/modAL-python/modAL/commit/9bf5f79f13ebd9f86111e75b872d692b92eec5b9#diff-6f949166cc52d17d3324b07852a231b9830bcdb419fe9a7de4d116faad8ac71fL73' target='_blank'>Link</a></div><div id='project'> Project Name: modAL-python/modAL</div><div id='commit'> Commit Name: 9bf5f79f13ebd9f86111e75b872d692b92eec5b9</div><div id='time'> Time: 2018-09-18</div><div id='author'> Author: theodore.danka@gmail.com</div><div id='file'> File Name: tests/core_tests.py</div><div id='class'> Class Name: TestUtils</div><div id='method'> Method Name: test_linear_combination</div><BR>