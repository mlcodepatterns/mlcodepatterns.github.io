<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        num_minibatch = variational_dist_f.event_shape.numel()

        &#47&#47 Now make the variational distribution Normal - for conditional indepdence
        variational_dist_f = <a id="change">pyro.distributions.Normal(variational_dist_f.mean, variational_dist_f.variance)</a>

        with pyro.poutine.scale(scale=float(self.num_data / num_minibatch)):
            <a id="change">return self.likelihood.pyro_sample_outputs(y, variational_dist_f)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Draw samples from p(u) for KL divergence computation
        inducing_values_samples = self.sample_inducing_values(prior_dist)
        <a id="change">sample_shape = inducing_values_samples.shape[:-len(prior_dist.shape())] + \
            torch.Size([1] * len(prior_dist.batch_shape))</a>

        &#47&#47 Get the variational distribution for the function
        function_dist = self(input)

        &#47&#47 Go from function -&gt; output
        num_minibatch = function_dist.batch_shape[-1]
        with pyro.poutine.scale(scale=float(self.num_data / num_minibatch)):
            <a id="change">return self.likelihood.pyro_sample_output(
                output, function_dist, *params, **kwargs, sample_shape=sample_shape
            )</a>

    def sample_inducing_values(self, inducing_values_dist):
        
        Sample values from the inducing point distribution `p(u)` or `q(u)`.</code></pre>