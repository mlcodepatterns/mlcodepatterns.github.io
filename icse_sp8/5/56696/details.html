<html><h3>2a548989f90026395d3d47ccf15ac331728c64bf,ml/rl/training/sac_trainer.py,SACTrainer,train,#SACTrainer#Any#,134
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Do this in 2 steps so we can log histogram of actor loss
        actor_loss_mean = actor_loss.mean()
        actor_loss_mean.backward()
        <a id="change">self._maybe_run_optimizer(
            self.actor_network_optimizer, self.minibatches_per_step
        )</a>

        &#47&#47
        &#47&#47 Lastly, if applicable, optimize value network; minimizing MSE between
        &#47&#47 V(s) & E_a~pi(s) [ Q(s,a) - log(pi(a|s)) ]</code></pre><h3>After Change</h3><pre><code class='java'>
                )
            )

        <a id="change">with torch.enable_grad():
            &#47&#47
            &#47&#47 First, optimize Q networks; minimizing MSE between
            &#47&#47 Q(s, a) & r + discount * V&quot(next_s)
            &#47&#47

            current_state_action = rlt.StateAction(state=state, action=action)
            q1_value = self.q1_network(current_state_action).q_value
            if self.q2_network:
                q2_value = self.q2_network(current_state_action).q_value
            actor_output = self.actor_network(rlt.StateInput(state=state))

            &#47&#47 Optimize Alpha
            if self.alpha_optimizer is not None:
                alpha_loss = -(
                    self.log_alpha
                    * (actor_output.log_prob + self.target_entropy).detach()
                ).mean()
                self.alpha_optimizer.zero_grad()
                alpha_loss.backward()
                self.alpha_optimizer.step()
                self.entropy_temperature = self.log_alpha.exp()

            with torch.no_grad():
                if self.value_network is not None:
                    next_state_value = self.value_network_target(
                        learning_input.next_state.float_features
                    )
                else:
                    next_state_actor_output = self.actor_network(
                        rlt.StateInput(state=learning_input.next_state)
                    )
                    next_state_actor_action = rlt.StateAction(
                        state=learning_input.next_state,
                        action=rlt.FeatureVector(
                            float_features=next_state_actor_output.action
                        ),
                    )
                    next_state_value = self.q1_network_target(
                        next_state_actor_action
                    ).q_value

                    if self.q2_network is not None:
                        target_q2_value = self.q2_network_target(
                            next_state_actor_action
                        ).q_value
                        next_state_value = torch.min(next_state_value, target_q2_value)

                    log_prob_a = self.actor_network.get_log_prob(
                        learning_input.next_state, next_state_actor_output.action
                    )
                    log_prob_a = log_prob_a.clamp(-20.0, 20.0)
                    next_state_value -= self.entropy_temperature * log_prob_a

                target_q_value = (
                    reward + discount * next_state_value * not_done_mask.float()
                )

            q1_loss = F.mse_loss(q1_value, target_q_value)
            q1_loss.backward()
            self._maybe_run_optimizer(
                self.q1_network_optimizer, self.minibatches_per_step
            )
            if self.q2_network:
                q2_loss = F.mse_loss(q2_value, target_q_value)
                q2_loss.backward()
                self._maybe_run_optimizer(
                    self.q2_network_optimizer, self.minibatches_per_step
                )

            &#47&#47
            &#47&#47 Second, optimize the actor; minimizing KL-divergence between action propensity
            &#47&#47 & softmax of value. Due to reparameterization trick, it ends up being
            &#47&#47 log_prob(actor_action) - Q(s, actor_action)
            &#47&#47

            state_actor_action = rlt.StateAction(
                state=state,
                action=rlt.FeatureVector(float_features=actor_output.action),
            )
            q1_actor_value = self.q1_network(state_actor_action).q_value
            min_q_actor_value = q1_actor_value
            if self.q2_network:
                q2_actor_value = self.q2_network(state_actor_action).q_value
                min_q_actor_value = torch.min(q1_actor_value, q2_actor_value)

            actor_loss = (
                self.entropy_temperature * actor_output.log_prob - min_q_actor_value
            )
            &#47&#47 Do this in 2 steps so we can log histogram of actor loss
            actor_loss_mean = actor_loss.mean()
            actor_loss_mean.backward()
            self._maybe_run_optimizer(
                self.actor_network_optimizer, self.minibatches_per_step
            )

            &#47&#47
            &#47&#47 Lastly, if applicable, optimize value network; minimizing MSE between
            &#47&#47 V(s) & E_a~pi(s) [ Q(s,a) - log(pi(a|s)) ]
            &#47&#47

            if self.value_network is not None:
                state_value = self.value_network(state.float_features)

                if self.logged_action_uniform_prior:
                    log_prob_a = torch.zeros_like(min_q_actor_value)
                    target_value = min_q_actor_value
                else:
                    with torch.no_grad():
                        log_prob_a = actor_output.log_prob
                        log_prob_a = log_prob_a.clamp(-20.0, 20.0)
                        target_value = (
                            min_q_actor_value - self.entropy_temperature * log_prob_a
                        )

                value_loss = F.mse_loss(state_value, target_value.detach())
                value_loss.backward()
                self._maybe_run_optimizer(
                    self.value_network_optimizer, self.minibatches_per_step
                )

        &#47&#47 Use the soft update rule to update the target networks
       </a> if self.value_network is not None:
            self._maybe_soft_update(
                self.value_network,
                self.value_network_target,</code></pre><img src="260886620.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/2a548989f90026395d3d47ccf15ac331728c64bf#diff-1458fd4d72c999f6eec93ce7d3414a04bc3864600342529de8965ee95e44dbc6L163' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 2a548989f90026395d3d47ccf15ac331728c64bf</div><div id='time'> Time: 2019-06-22</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/sac_trainer.py</div><div id='class'> Class Name: SACTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/2a548989f90026395d3d47ccf15ac331728c64bf#diff-a421c1688e7de52470d40f812d85accc1903ba475daa8bd7034891dbc49631f4L100' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 2a548989f90026395d3d47ccf15ac331728c64bf</div><div id='time'> Time: 2019-06-22</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/parametric_dqn_trainer.py</div><div id='class'> Class Name: ParametricDQNTrainer</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/Horizon/commit/2a548989f90026395d3d47ccf15ac331728c64bf#diff-1e05823efcf5119bf771fc16f182df9cf4b8aac20b722908d7557012b69896d4L246' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/Horizon</div><div id='commit'> Commit Name: 2a548989f90026395d3d47ccf15ac331728c64bf</div><div id='time'> Time: 2019-06-22</div><div id='author'> Author: jjg@fb.com</div><div id='file'> File Name: ml/rl/training/dqn_trainer.py</div><div id='class'> Class Name: DQNTrainer</div><div id='method'> Method Name: calculate_cpes</div><BR>