<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        "is the global batch size and not the per-shard batch.")
tf.flags.DEFINE_float("dropout", 0.1, "dropout")
tf.flags.DEFINE_float("label_smoothing", 0.1, "label smoothing")
<a id="change">tf.flags.DEFINE_integer(
    "train_steps", 100000, "Total number of training steps.")</a>

&#47&#47 DISTRIBUTED LAYOUT
&#47&#47 When running on TPU, make sure that the size of the mesh
&#47&#47 (the product of all dimension sizes) equals the number of TPU cores.</code></pre><h3>After Change</h3><pre><code class='java'>
    ,
    "data_dir for TensorFlow Datasets"
)
<a id="change">tf.flags.DEFINE_boolean(
    "text2self",
    False,
    "Whether to train a language model (True) or encoder-decoder text-to-text "
    "model (False)."
)</a>

&#47&#47 MODEL HYPERPARAMETERS
tf.flags.DEFINE_integer("max_length", 256,
                        "maximum sequence length (checkpoints depend on this)")</code></pre>