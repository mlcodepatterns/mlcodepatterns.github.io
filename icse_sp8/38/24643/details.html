<html><h3>360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02,theanolm/training/adamoptimizer.py,AdamOptimizer,__init__,#AdamOptimizer#Any#Any#,17
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            numpy.dtype(theano.config.floatX).type(0.0)

        for name, param in network.params.items():
            <a id="change">self.param_init_values[name + &quot_gradient&quot] = \
                numpy.zeros_like(param.get_value())</a>
            <a id="change">self.param_init_values[name + &quot_mean_gradient&quot] = \
                numpy.zeros_like(param.get_value())</a>
            <a id="change">self.param_init_values[name + &quot_mean_sqr_gradient&quot] = \
                numpy.zeros_like(param.get_value())</a>

        &#47&#47 geometric rate for averaging gradients
        if not &quotgradient_decay_rate&quot in optimization_options:
            raise ValueError("Gradient decay rate is not given in training "</code></pre><h3>After Change</h3><pre><code class='java'>
        :param network: the neural network object
        

        <a id="change">self._params</a> = Parameters()

        float_type = numpy.dtype(theano.config.floatX).type
        self._params.add(&quotoptimizer/timestep&quot, float_type(0.0))

        for path, param in <a id="change">network</a>.get_variables().items():
            <a id="change">self._params.add(path + &quot_gradient&quot,
                             numpy.zeros_like(param.get_value()))</a>
            <a id="change">self._params.add(path + &quot_mean_gradient&quot,
                             numpy.zeros_like(param.get_value()))</a>
            <a id="change">self._params.add(path + &quot_mean_sqr_gradient&quot,
                             numpy.zeros_like(param.get_value()))</a>

        &#47&#47 geometric rate for averaging gradients
        if not &quotgradient_decay_rate&quot in optimization_options:
            raise ValueError("Gradient decay rate is not given in training "</code></pre><img src="128716076.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 33</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/senarvi/theanolm/commit/360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02#diff-a5a13ef4e5f731b2cdc36bfb4046cc8256bcf97a9dee00a7a49a2b3121c76812L17' target='_blank'>Link</a></div><div id='project'> Project Name: senarvi/theanolm</div><div id='commit'> Commit Name: 360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02</div><div id='time'> Time: 2016-12-06</div><div id='author'> Author: seppo.git@marjaniemi.com</div><div id='file'> File Name: theanolm/training/adamoptimizer.py</div><div id='class'> Class Name: AdamOptimizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/senarvi/theanolm/commit/360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02#diff-a5a13ef4e5f731b2cdc36bfb4046cc8256bcf97a9dee00a7a49a2b3121c76812L17' target='_blank'>Link</a></div><div id='project'> Project Name: senarvi/theanolm</div><div id='commit'> Commit Name: 360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02</div><div id='time'> Time: 2016-12-06</div><div id='author'> Author: seppo.git@marjaniemi.com</div><div id='file'> File Name: theanolm/training/adamoptimizer.py</div><div id='class'> Class Name: AdamOptimizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/senarvi/theanolm/commit/360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02#diff-8a40cedcb0063a3bc233cbb4cf8dc68e28a2bba72130be83f6396e9c0e6caad7L33' target='_blank'>Link</a></div><div id='project'> Project Name: senarvi/theanolm</div><div id='commit'> Commit Name: 360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02</div><div id='time'> Time: 2016-12-06</div><div id='author'> Author: seppo.git@marjaniemi.com</div><div id='file'> File Name: theanolm/training/rmspropnesterovoptimizer.py</div><div id='class'> Class Name: RMSPropNesterovOptimizer</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/senarvi/theanolm/commit/360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02#diff-52831835416c9dcec37fee2442d68154300b2daaef3e22cebfae9de882692480L25' target='_blank'>Link</a></div><div id='project'> Project Name: senarvi/theanolm</div><div id='commit'> Commit Name: 360ed9d9b6a8f25e8ec1302b523ca3dcc97c6f02</div><div id='time'> Time: 2016-12-06</div><div id='author'> Author: seppo.git@marjaniemi.com</div><div id='file'> File Name: theanolm/training/adadeltaoptimizer.py</div><div id='class'> Class Name: AdadeltaOptimizer</div><div id='method'> Method Name: __init__</div><BR>