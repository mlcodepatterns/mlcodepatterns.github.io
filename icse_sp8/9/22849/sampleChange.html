<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def connect_data_and_network(self,
                                 outputs_collector=None,
                                 gradients_collector=None):
        <a id="change">data_dict = self.get_sampler()[0].pop_batch_op()</a>
        <a id="change">image = tf.cast(data_dict[&quotimage&quot], tf.float32)</a>
        <a id="change">net_out = self.net(image, self.is_training)</a>

        if self.is_training:
            with tf.name_scope(&quotOptimiser&quot):</code></pre><h3>After Change</h3><pre><code class='java'>


        if self.is_training:
            <a id="change">data_dict</a>, <a id="change">net_out</a> = tf.cond(self.is_validation,
                                         lambda: data_net(False),
                                         lambda: data_net(True))
            with tf.name_scope(&quotOptimiser&quot):
                optimiser_class = OptimiserFactory.create(
                    name=self.action_param.optimiser)
                self.optimiser = optimiser_class.get_instance(
                    learning_rate=self.action_param.lr)
            loss_func = LossFunction(
                n_class=self.segmentation_param.num_classes,
                loss_type=self.action_param.loss_type)
            data_loss = loss_func(
                prediction=net_out,
                ground_truth=data_dict.get(&quotlabel&quot, None),
                weight_map=data_dict.get(&quotweight&quot, None))
            reg_losses = tf.get_collection(
                tf.GraphKeys.REGULARIZATION_LOSSES)
            if self.net_param.decay &gt; 0.0 and reg_losses:
                reg_loss = tf.reduce_mean(
                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])
                loss = data_loss + reg_loss
            else:
                loss = data_loss
            grads = self.optimiser.compute_gradients(loss)
            &#47&#47 collecting gradients variables
            gradients_collector.add_to_collection([grads])
            &#47&#47 collecting output variables
            outputs_collector.add_to_collection(
                var=data_loss, name=&quotdice_loss&quot,
                average_over_devices=False, collection=CONSOLE)
            outputs_collector.add_to_collection(
                var=data_loss, name=&quotdice_loss&quot,
                average_over_devices=True, summary_type=&quotscalar&quot,
                collection=TF_SUMMARIES)
        else:
            &#47&#47 converting logits into final output for
            &#47&#47 classification probabilities or argmax classification labels
            <a id="change">data_dict</a>, <a id="change">net_out</a> = data_net(for_training=False)
            output_prob = self.segmentation_param.output_prob
            num_classes = self.segmentation_param.num_classes
            if output_prob and num_classes &gt; 1:</code></pre>