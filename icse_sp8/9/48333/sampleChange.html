<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        policy_loss = -torch.mean(q_values_tp0_min)

        &#47&#47 critic loss (kl-divergence between categorical distributions)
        actions_tp1 = <a id="change">self.target_actor(states_tp1).detach()</a>
        actions_tp1 = <a id="change">self._add_noise_to_actions(actions_tp1)</a>
        logits_t = [
            x(states_t, actions_t).squeeze_(dim=2) for x in self.critics
        ]
        logits_tp1 = [
            x(states_tp1, actions_tp1).squeeze_(dim=2)
            for x in self.target_critics
        ]
        probs_tp1 = [torch.softmax(x, dim=-1) for x in logits_tp1]
        q_values_tp1 = [
            torch.sum(x * self.z, dim=-1, keepdim=True) for x in probs_tp1
        ]
        probs_ids_tp1_min = torch.cat(q_values_tp1, dim=-1).argmin(dim=-1)
        &#47&#47 B x num_heads

        logits_tp1 = torch.cat([x.unsqueeze(-1) for x in logits_tp1], dim=-1)
        &#47&#47 B x num_heads x num_atoms x num_critics
        &#47&#47 @TODO: smarter way to do this (other than reshaping)?
        probs_ids_tp1_min = probs_ids_tp1_min.view(-1)
        logits_tp1 = <a id="change">logits_tp1.view(-1, self.num_atoms, self._num_critics)</a>

        <a id="change">logits_tp1 = \
            logits_tp1[range(len(logits_tp1)), :, probs_ids_tp1_min].\
            view(-1, self._num_heads, self.num_atoms).detach()</a>

        atoms_target_t = rewards_t + (1 - done_t) * gammas * self.z
        value_loss = [
            utils.categorical_loss(</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 critic loss (kl-divergence between categorical distributions)
        &#47&#47 [bs; action_size]
        actions_tp1 = <a id="change">self.target_actor(states_tp1)</a>
        actions_tp1 = <a id="change">self._add_noise_to_actions(actions_tp1).detach()</a>

        &#47&#47 {num_critics} * [bs; num_heads; num_atoms]
        &#47&#47 -&gt; many-heads view transform
        &#47&#47 {num_critics} * [{bs * num_heads}; num_atoms]
        logits_t = [
            x(states_t, actions_t).squeeze_(dim=2).view(-1, self.num_atoms)
            for x in self.critics
        ]

        &#47&#47 {num_critics} * [bs; num_heads; num_atoms]
        logits_tp1 = [
            x(states_tp1, actions_tp1).squeeze_(dim=2)
            for x in self.target_critics
        ]
        &#47&#47 {num_critics} * [{bs * num_heads}; num_atoms]
        probs_tp1 = [torch.softmax(x, dim=-1) for x in logits_tp1]
        &#47&#47 {num_critics} * [bs; num_heads; 1]
        q_values_tp1 = [
            torch.sum(x * self.z, dim=-1, keepdim=True) for x in probs_tp1
        ]
        &#47&#47  [{bs * num_heads}; num_critics] -&gt;  argmin over all critics
        &#47&#47  [{bs * num_heads}]
        probs_ids_tp1_min = torch.cat(q_values_tp1, dim=-1).argmin(dim=-1)

        &#47&#47 [bs; num_heads; num_atoms; num_critics]
        logits_tp1 = torch.cat([x.unsqueeze(-1) for x in logits_tp1], dim=-1)
        &#47&#47 @TODO: smarter way to do this (other than reshaping)?
        probs_ids_tp1_min = probs_ids_tp1_min.view(-1)
        &#47&#47 [bs; num_heads; num_atoms; num_critics] -&gt; many-heads view transform
        &#47&#47 [{bs * num_heads}; num_atoms; num_critics] -&gt; min over all critics
        &#47&#47 [{bs * num_heads}; num_atoms; 1] -&gt; target view transform
        &#47&#47 [{bs; num_heads}; num_atoms]
        logits_tp1 = (
            <a id="change">logits_tp1</a>
            .view(-1, self.num_atoms, self._num_critics)[
                range(len(probs_ids_tp1_min)), :, probs_ids_tp1_min]
            .view(-1, self.num_atoms)
        ).detach()</code></pre>