<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if (anchor == positive) or (y_anchor != y_positive):
                    continue

                <a id="change">for negative, y_negative in enumerate(y):

                    &#47&#47 if same label, skip
                    if y_negative == y_positive:
                        continue

                    loss_ = distance[anchor, positive] - \
                            distance[anchor, negative]

                    if self.clamp == &quotpositive&quot:
                        loss_ = loss_ + self.margin * self.metric_max_
                        loss_ = ag_np.maximum(loss_, 0.)

                    elif self.clamp == &quotsigmoid&quot:
                        loss_ = loss_ - self.margin * self.metric_max_
                        loss_ = 1. / (1. + ag_np.exp(-10. * loss_))

                    &#47&#47 do not use += because autograd does not support it
                    loss = loss + loss_

                    n_comparisons = n_comparisons + 1

       </a> return loss, n_comparisons

    def loss_z(self, fX, y, n, *args):
        Differentiable loss</code></pre><h3>After Change</h3><pre><code class='java'>
                if (anchor == positive) or (y_anchor != y_positive):
                    continue

                <a id="change">for i, negative in enumerate(
                    self.triplet_sampling(y, anchor, positive,
                                          distance=distance)):

                    &#47&#47 at most n_negative negative samples
                    if i + 1 &gt; self.n_negative:
                        break

                    loss_ = self.triplet_loss(distance,
                                              anchor,
                                              positive,
                                              negative=negative,
                                              clamp=True)

                    &#47&#47 do not use += because autograd does not support it
                    loss = loss + loss_

                    n_comparisons = n_comparisons + 1

       </a> return loss, n_comparisons

    def loss_z(self, fX, y, n, *args):
        Differentiable loss</code></pre>