<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47FIXME normalizing doesn&quott seem to have the desired effect, currently; it makes the step size very small (for large scales)
    &#47&#47if "a" was defaulted, use the average scale of the input space.
    &#47&#47This is the suggested value from the paper, missing a 1/gradient term since we don&quott know it yet.
    <a id="change">if self.paramDict[&quota&quot] is None:
      self.paramDict[&quota&quot] = mathUtils.hyperdiagonal(np.ones(numValues)) &#47&#47 the features are always normalized
      self.raiseAMessage(&quotDefaulting "a" gradient parameter to&quot,self.paramDict[&quota&quot])
    else:
      self.paramDict[&quota&quot] = float(self.paramDict[&quota&quot])

   </a> self.constraintHandlingPara[&quotinnerBisectionThreshold&quot] = float(self.paramDict.get(&quotinnerBisectionThreshold&quot, 1e-2))
    self.constraintHandlingPara[&quotinnerLoopLimit&quot] = float(self.paramDict.get(&quotinnerLoopLimit&quot, 1000))

    self.gradDict[&quotpertNeeded&quot] = self.gradDict[&quotnumIterForAve&quot] * (self.paramDict[&quotpertSingleGrad&quot]+1)</code></pre><h3>After Change</h3><pre><code class='java'>
                         .format(self.paramDict[&quotpertDist&quot]))

    self.constraintHandlingPara[&quotinnerBisectionThreshold&quot] = float(self.paramDict.get(&quotinnerBisectionThreshold&quot, 1e-2))
    <a id="change">if not 0 &lt; self.constraintHandlingPara[&quotinnerBisectionThreshold&quot] &lt; 1:
      self.raiseAnError(IOError,&quotinnerBisectionThreshold must be between 0 and 1; got&quot,self.constraintHandlingPara[&quotinnerBisectionThreshold&quot])
    self.constraintHandlingPara[&quotinnerLoopLimit&quot] = float(self.paramDict.get(&quotinnerLoopLimit&quot, 1000))

    self.gradDict[&quotpertNeeded&quot] = self.gradDict[&quotnumIterForAve&quot] * (self.paramDict[&quotpertSingleGrad&quot]+1)

    &#47&#47 determine the number of indpendent variables (scalar and vectors included)
    stochDist = self.paramDict.get(&quotstochasticDistribution&quot, &quotHypersphere&quot)
    if stochDist == &quotBernoulli&quot:
      self.stochasticDistribution = Distributions.returnInstance(&quotBernoulli&quot,self)
      self.stochasticDistribution.p = 0.5
      self.stochasticDistribution.initializeDistribution()
      &#47&#47 Initialize bernoulli distribution for random perturbation. Add artificial noise to avoid that specular loss functions get false positive convergence
      &#47&#47 FIXME there has to be a better way to get two random numbers
      self.stochasticEngine = lambda: [(0.5+randomUtils.random()*(1.+randomUtils.random()/1000.*randomUtils.randomIntegers(-1, 1, self))) if self.stochasticDistribution.rvs() == 1 else
                                   -1.*(0.5+randomUtils.random()*(1.+randomUtils.random()/1000.*randomUtils.randomIntegers(-1, 1, self))) for _ in range(numValues)]
    elif stochDist == &quotHypersphere&quot:
      &#47&#47 TODO assure you can&quott get a "0" along any dimension! Need to be &gt; 1e-15. Right now it&quots just highly unlikely.
      self.stochasticEngine = lambda: randomUtils.randPointsOnHypersphere(numValues) if numValues &gt; 1 else [randomUtils.randPointsOnHypersphere(numValues)]
    else:
      self.raiseAnError(IOError, self.paramDict[&quotstochasticEngine&quot]+&quotis currently not supported for SPSA&quot)

  def localLocalInitialize(self, solutionExport):
    
      Method to initialize local settings.
      @ In, solutionExport, DataObject, a PointSet to hold the solution
      @ Out, None
    
    self._endJobRunnable = (self._endJobRunnable*self.gradDict[&quotpertNeeded&quot])+len(self.optTraj)
    &#47&#47 set up cycler for trajectories
    self.trajCycle = cycle(self.optTraj)
    &#47&#47 build up queue of initial runs
    for traj in self.optTraj:
      &#47&#47 for the first run, set the step size to the initial step size
      self.counter[&quotlastStepSize&quot][traj] = self.paramDict[&quotinitialStepSize&quot]
      &#47&#47 construct initial point for trajectory
      values = {}
      for var in self.getOptVars():
        &#47&#47 user-provided points
        values[var] = self.optVarsInit[&quotinitial&quot][var][traj]
        &#47&#47 assure points are within bounds; correct them if not
        values[var] = self._checkBoundariesAndModify(self.optVarsInit[&quotupperBound&quot][var],
                                                     self.optVarsInit[&quotlowerBound&quot][var],
                                                     self.optVarsInit[&quotranges&quot][var],
                                                     values[var], 0.99, 0.01)
      &#47&#47 normalize initial point for this trajectory
      data = self.normalizeData(values)
      &#47&#47 store (unnormalized?) point in history
      self.updateVariableHistory(values,traj)
      &#47&#47 set up a new batch of runs on the new optimal point (batch size 1 unless more requested by user)
      self.queueUpOptPointRuns(traj,data)
      &#47&#47 set up grad point near initial point
      pertPoints = self._createPerturbationPoints(traj,data)
      &#47&#47 set up storage structure for results
      self._setupNewStorage(traj)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 Run Methods &#47&#47
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  def localStillReady(self, ready, convergence = False):
    
      Determines if optimizer is ready to provide another input.  If not, and if jobHandler is finished, this will end sampling.
      @ In, ready, bool, variable indicating whether the caller is prepared for another input.
      @ In, convergence, bool, optional, variable indicating whether the convergence criteria has been met.
      @ Out, ready, bool, variable indicating whether the caller is prepared for another input.
    
    &#47&#47 accept unreadiness from another source if applicable
    if not ready:
      return ready
    if any(len(self.submissionQueue[t]) for t in self.optTraj):
      return True
    return False

  def localGenerateInput(self,model,oldInput):
    
      Method to generate input for model to run
      @ In, model, model instance, it is the instance of a RAVEN model
      @ In, oldInput, list, a list of the original needed inputs for the model (e.g. list of files, etc. etc)
      @ Out, None
    
    GradientBasedOptimizer.localGenerateInput(self,model,oldInput)
    &#47&#47 find something to submit
    for _ in self.optTraj:
      &#47&#47 get next trajectory in line, which assures each gets fair treatment in submissions
      traj = next(self.trajCycle)
      &#47&#47 if this trajectory has a run to submit, populate the submission dictionaries
      if len(self.submissio</a>nQueue[traj]):
        prefix, point = self.getQueuedPoint(traj)
        for var in self.getOptVars():
          self.values[var] = point[var]</code></pre>