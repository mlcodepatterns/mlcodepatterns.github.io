<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        training_summary[&quoteval_metrics&quot] = _float_metric_value(eval_metric)

      summary_path = os.path.join(model_dir, SUMMARY_TXT)
      <a id="change">with tf.io.gfile.GFile(summary_path, &quotwb&quot) as f:
        logging.info(&quotTraining Summary: \n%s&quot, str(training_summary))
        f.write(json.dumps(training_summary, indent=4))

     </a> return model
</code></pre><h3>After Change</h3><pre><code class='java'>
      if steps_per_loop &gt;= _MIN_SUMMARY_STEPS:
        &#47&#47 Only writes summary when the stats are collected sufficiently over
        &#47&#47 enough steps.
        <a id="change">train_summary_writer = tf.summary.create_file_writer(
            os.path.join(model_dir, &quotsummaries/train&quot))</a>
      else:
        train_summary_writer = None

      def _replicated_step(inputs):
        Replicated training step.

        inputs, labels = inputs
        with tf.GradientTape() as tape:
          model_outputs = model(inputs)
          loss = loss_fn(labels, model_outputs)

        tvars = model.trainable_variables
        grads = tape.gradient(loss, tvars)
        optimizer.apply_gradients(zip(grads, tvars))
        &#47&#47 For reporting, the metric takes the mean of losses.
        train_loss_metric.update_state(loss)
        if train_metric:
          train_metric.update_state(labels, model_outputs)

      @tf.function
      def train_steps(iterator, steps):
        Performs distributed training steps in a loop.

        Args:
          iterator: the distributed iterator of training datasets.
          steps: an tf.int32 integer tensor to specify number of steps to run
            inside host training loop.
        Raises:
          ValueError: Any of the arguments or tensor shapes are invalid.
        
        if not isinstance(steps, tf.Tensor):
          raise ValueError(&quotsteps should be an Tensor. Python object may cause &quot
                           &quotretracing.&quot)

        for _ in tf.range(steps):
          strategy.experimental_run_v2(_replicated_step, args=(next(iterator),))

      @tf.function
      def train_single_step(iterator):
        Performs a distributed training step.

        Args:
          iterator: the distributed iterator of training datasets.
        Raises:
          ValueError: Any of the arguments or tensor shapes are invalid.
        
        strategy.experimental_run_v2(_replicated_step, args=(next(iterator),))

      @tf.function
      def test_step(iterator):
        Calculates evaluation metrics on distributed devices.

        def _test_step_fn(inputs):
          Replicated accuracy calculation.

          inputs, labels = inputs
          model_outputs = model(inputs, training=False)
          eval_metric.update_state(labels, model_outputs)

        strategy.experimental_run_v2(_test_step_fn, args=(next(iterator),))

      def _run_evaluation(current_training_step, test_iterator):
        Runs validation steps and aggregate metrics.
        for _ in range(eval_steps):
          test_step(test_iterator)
        eval_metric_value = _float_metric_value(eval_metric)
        logging.info(&quotStep: [%d] Validation metric = %f&quot, current_training_step,
                     eval_metric_value)
        with eval_summary_writer.as_default():
          tf.summary.scalar(
              eval_metric.name, eval_metric_value, step=current_training_step)
          eval_summary_writer.flush()

      def _run_callbacks_on_batch_begin(batch):
        Runs custom callbacks at the start of every step.
        if not custom_callbacks:
          return
        for callback in custom_callbacks:
          callback.on_batch_begin(batch)

      def _run_callbacks_on_batch_end(batch):
        Runs custom callbacks at the end of every step.
        if not custom_callbacks:
          return
        for callback in custom_callbacks:
          callback.on_batch_end(batch)

      &#47&#47 Training loop starts here.
      checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
      latest_checkpoint_file = tf.train.latest_checkpoint(model_dir)
      if latest_checkpoint_file:
        logging.info(
            &quotCheckpoint file %s found and restoring from &quot
            &quotcheckpoint&quot, latest_checkpoint_file)
        checkpoint.restore(latest_checkpoint_file)
        logging.info(&quotLoading from checkpoint file completed&quot)

      current_step = optimizer.iterations.numpy()
      checkpoint_name = &quotctl_step_{step}.ckpt&quot

      while current_step &lt; total_training_steps:
        &#47&#47 Training loss/metric are taking average over steps inside micro
        &#47&#47 training loop. We reset the their values before each round.
        train_loss_metric.reset_states()
        if train_metric:
          train_metric.reset_states()

        _run_callbacks_on_batch_begin(current_step)
        &#47&#47 Runs several steps in the host while loop.
        steps = _steps_to_run(current_step, steps_per_epoch, steps_per_loop)

        if steps == 1:
          &#47&#47 TODO(zongweiz): merge with train_steps once tf.while_loop
          &#47&#47 GPU performance bugs are fixed.
          train_single_step(train_iterator)
        else:
          &#47&#47 Converts steps to a Tensor to avoid tf.function retracing.
          train_steps(train_iterator,
                      tf.convert_to_tensor(steps, dtype=tf.int32))
        _run_callbacks_on_batch_end(current_step)
        current_step += steps

        train_loss = _float_metric_value(train_loss_metric)
        &#47&#47 Updates training logging.
        training_status = &quotTrain Step: %d/%d  / loss = %s&quot % (
            current_step, total_training_steps, train_loss)
        if train_metric:
          train_metric_value = _float_metric_value(train_metric)
          training_status += &quot training metric = %f&quot % train_metric_value
        else:
          train_metric_value = None
        logging.info(training_status)

        if train_summary_writer:
          <a id="change">with train_summary_writer.as_default():
            tf.summary.scalar(
                train_loss_metric.name, train_loss, step=current_step)
            if train_metric_value:
              tf.summary.scalar(
                  train_metric.name, train_metric_value, step=current_step)
            train_summary_writer.flush()

        &#47&#47 Saves model checkpoints and run validation steps at every epoch end.
       </a> if current_step % steps_per_epoch == 0:
          &#47&#47 To avoid repeated model saving, we do not save after the last
          &#47&#47 step of training.
          if current_step &lt; total_training_steps:</code></pre>