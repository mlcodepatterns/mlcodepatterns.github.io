<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 print to screen
            if i % self.print_freq == 0:
                <a id="change">print(
                    &quotEpoch: [{0}][{1}/{2}]\t&quot
                    &quotTime: {batch_time.val:.3f} ({batch_time.avg:.3f})\t&quot
                    &quotTrain Loss: {loss.val:.4f} ({loss.avg:.4f})\t&quot
                    &quotTrain Acc: {acc.val:.3f} ({acc.avg:.3f})&quot.format(
                        epoch, i, len(self.train_loader),
                        batch_time=batch_time, loss=losses, acc=accs)
                )</a>

        &#47&#47 log to tensorboard
        if self.use_tensorboard:
            log_value(&quottrain_loss&quot, losses.avg, epoch)</code></pre><h3>After Change</h3><pre><code class='java'>
        accs = AverageMeter()

        tic = time.time()
        <a id="change">with tqdm(total=self.num_train) as pbar:
            for i, (x, y) in enumerate(self.train_loader):
                x, y = Variable(x), Variable(y)

                self.batch_size = x.shape[0]
                self.reset()

                &#47&#47 initialize location vector
                l_t = torch.Tensor(self.batch_size, 2).uniform_(-1, 1)
                l_t = Variable(l_t)

                &#47&#47 extract the glimpses
                log_pi = 0.
                for t in range(self.num_glimpses - 1):

                    &#47&#47 forward pass through model
                    self.h_t, mu, l_t, p = self.model(x, l_t, self.h_t)

                    &#47&#47 accumulate log of policy
                    log_pi += p

                &#47&#47 last iteration
                self.h_t, mu, l_t, p, b_t, log_probas = self.model(
                    x, l_t, self.h_t, last=True
                )
                log_pi += p

                &#47&#47 calculate reward
                predicted = torch.max(log_probas, 1)[1]
                R = (predicted == y).float()

                &#47&#47 compute losses for differentiable modules
                loss_action = F.nll_loss(log_probas, y)
                loss_baseline = F.mse_loss(b_t, R)

                &#47&#47 compute reinforce loss
                adjusted_reward = R - b_t
                log_pi = log_pi / self.num_glimpses
                loss_reinforce = torch.mean(-log_pi*adjusted_reward)

                &#47&#47 sum up into a hybrid loss
                loss = loss_action + loss_baseline + loss_reinforce

                &#47&#47 compute accuracy
                acc = 100 * (R.sum() / len(y))

                &#47&#47 store
                losses.update(loss.data[0], x.size()[0])
                accs.update(acc.data[0], x.size()[0])

                &#47&#47 compute gradients and update SGD
                &#47&#47 a = list(self.model.rnn.parameters())[0].clone()
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                &#47&#47 b = list(self.model.rnn.parameters())[0].clone()
                &#47&#47 assert(torch.equal(a.data, b.data))

                &#47&#47 measure elapsed time
                toc = time.time()
                batch_time.update(toc-tic)

                pbar.set_description(
                    (
                        "{:.1f}s - loss: {:.3f} - acc: {:.3f}".format(
                            (toc-tic), loss.data[0], acc.data[0]
                            )
                    )
                )
                pbar.update(self.batch_size)

            &#47&#47 log to tensorboard
            if self.use_tensorboard:
                log_value(&quottrain_loss&quot, losses.avg, epoch)
                log_value(&quottrain_acc&quot, accs.avg, epoch)

   </a> def validate(self, epoch):
        
        Evaluate the model on the validation set.
        </code></pre>