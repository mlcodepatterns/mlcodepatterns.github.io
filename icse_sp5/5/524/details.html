<html><h3>7856d22b09561e33522bdc0bd00218ae75b84bd7,softlearning/algorithms/sac.py,SAC,_init_critic_update,#SAC#,232
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        q_target = self._get_q_target()

        <a id="change">self._qf1_t = self._qf1.output_for(
            self._observations_ph, self._actions_ph, reuse=True)</a>  &#47&#47 N
        self._qf2_t = self._qf2.output_for(
            self._observations_ph, self._actions_ph, reuse=True)  &#47&#47 N

        <a id="change">self._td_loss1_t = tf.losses.mean_squared_error(
            labels=q_target, predictions=self._qf1_t, weights=0.5)</a>
        self._td_loss2_t = tf.losses.mean_squared_error(
            labels=q_target, predictions=self._qf2_t, weights=0.5)

        qf1_train_op = tf.contrib.layers.optimize_loss(
            self._td_loss1_t,
            self.global_step,
            learning_rate=self._qf_lr,
            optimizer=tf.train.AdamOptimizer,
            variables=self._qf1.get_params_internal(),
            increment_global_step=False,
            name="td_loss_1_optimizer",
            summaries=([
                "loss", "gradients", "gradient_norm", "global_gradient_norm"
            ] if self._tf_summaries else []))

        qf2_train_op = tf.contrib.layers.optimize_loss(
            self._td_loss2_t,
            self.global_step,
            learning_rate=self._qf_lr,
            optimizer=tf.train.AdamOptimizer,
            variables=self._qf2.get_params_internal(),
            increment_global_step=False,
            name="td_loss_2_optimizer",
            summaries=([
                "loss", "gradients", "gradient_norm", "global_gradient_norm"
            ] if self._tf_summaries else []))

        self._training_ops.update({
            &quotqf&quot: <a id="change">(qf1_train_op, qf2_train_op)</a>
        })

    def _init_actor_update(self):
        Create minimization operations for policy and state value functions.</code></pre><h3>After Change</h3><pre><code class='java'>
                self._observations_ph, self._actions_ph, reuse=True)  &#47&#47 N
            for q_function in self._q_functions)

        <a id="change">q_losses = </a>self._q_losses = tuple(
            tf.losses.mean_squared_error(
                labels=q_target, predictions=q_value, weights=0.5)
            for q_value in q_values)</code></pre><img src="2887724.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/7856d22b09561e33522bdc0bd00218ae75b84bd7#diff-c427dfbb58b5337d68da626b8e4cbd3324fb1836da0ca03e40cb9be61520c9ddL229' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 7856d22b09561e33522bdc0bd00218ae75b84bd7</div><div id='time'> Time: 2018-09-09</div><div id='author'> Author: kristian.hartikainen@gmail.com</div><div id='file'> File Name: softlearning/algorithms/sac.py</div><div id='class'> Class Name: SAC</div><div id='method'> Method Name: _init_critic_update</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/0b47129713e8cd63c49a0c53202f2b3deac941cc#diff-d720c7ccf51e8660c302874569569e5bcc7eb762222d3e1fddd5ff7754a712e8L175' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 0b47129713e8cd63c49a0c53202f2b3deac941cc</div><div id='time'> Time: 2018-07-09</div><div id='author'> Author: azhou42@berkeley.edu</div><div id='file'> File Name: softlearning/policies/latent_space_policy.py</div><div id='class'> Class Name: LatentSpacePolicy</div><div id='method'> Method Name: get_action</div><BR><BR><div id='link'><a href='https://github.com/YerevaNN/mimic3-benchmarks/commit/39e6232af3c56843c48d89466ecf8478eec5b9af#diff-6208cbe4ee661396f56c79f610ee7722f80263a1125459f2e17275d470b51c55L45' target='_blank'>Link</a></div><div id='project'> Project Name: YerevaNN/mimic3-benchmarks</div><div id='commit'> Commit Name: 39e6232af3c56843c48d89466ecf8478eec5b9af</div><div id='time'> Time: 2017-07-30</div><div id='author'> Author: harhro@gmail.com</div><div id='file'> File Name: mimic3models/phenotyping/utils.py</div><div id='class'> Class Name: BatchGen</div><div id='method'> Method Name: _generator</div><BR><BR><div id='link'><a href='https://github.com/rail-berkeley/softlearning/commit/459441e166ebcd176698041e260d4f467652e7ac#diff-d720c7ccf51e8660c302874569569e5bcc7eb762222d3e1fddd5ff7754a712e8L172' target='_blank'>Link</a></div><div id='project'> Project Name: rail-berkeley/softlearning</div><div id='commit'> Commit Name: 459441e166ebcd176698041e260d4f467652e7ac</div><div id='time'> Time: 2018-07-21</div><div id='author'> Author: kristian.hartikainen@gmail.com</div><div id='file'> File Name: softlearning/policies/latent_space_policy.py</div><div id='class'> Class Name: LatentSpacePolicy</div><div id='method'> Method Name: get_action</div><BR>