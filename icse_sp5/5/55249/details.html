<html><h3>213d55dbdaa82a8d5edeede2a821fe2163881eb7,transcribe.py,,,#,41
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

if __name__ == &quot__main__&quot:
    arg_parser = argparse.ArgumentParser(description=&quotDeepSpeech transcription&quot)
    <a id="change">arg_parser = add_inference_args(arg_parser)</a>
    arg_parser.add_argument(&quot--audio-path&quot,
                            default=&quotaudio.wav&quot,
                            help=&quotAudio file to predict on&quot)
    arg_parser.add_argument(&quot--offsets&quot,
                            dest=&quotoffsets&quot,
                            action=&quotstore_true&quot,
                            help=&quotReturns time offset information&quot)
    arg_parser = add_decoder_args(arg_parser)
    args = arg_parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")
    <a id="change">model = load_model(device, args.model_path, args.half)</a>

    decoder = load_decoder(decoder_type=args.decoder,
                           labels=model.labels,
                           lm_path=args.lm_path,
                           alpha=args.alpha,
                           beta=args.beta,
                           cutoff_top_n=args.cutoff_top_n,
                           cutoff_prob=args.cutoff_prob,
                           beam_width=args.beam_width,
                           lm_workers=args.lm_workers)

    <a id="change">spect_parser</a> = SpectrogramParser(audio_conf=model.audio_conf,
                                     normalize=True)

    decoded_output, decoded_offsets = transcribe(audio_path=args.audio_path,</code></pre><h3>After Change</h3><pre><code class='java'>
    transcribe(cfg=cfg)


<a id="change">if __name__ == &quot__main__&quot:
    hydra_main()</a>
</code></pre><img src="254861169.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/SeanNaren/deepspeech.pytorch/commit/213d55dbdaa82a8d5edeede2a821fe2163881eb7#diff-c0686afd0fcea55c25940cd0d547340100e0031516469da9e6f56aad950da872L12' target='_blank'>Link</a></div><div id='project'> Project Name: SeanNaren/deepspeech.pytorch</div><div id='commit'> Commit Name: 213d55dbdaa82a8d5edeede2a821fe2163881eb7</div><div id='time'> Time: 2020-07-14</div><div id='author'> Author: sean.narenthiran@digitalreasoning.com</div><div id='file'> File Name: transcribe.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/DenisTome/Lifting-from-the-Deep-release/commit/70f229dde624412adb0bc466b4eee4929fcc1d91#diff-27419e27638ba26c3b055e9daca00840a52779aa0e76b06c6a2a68d128abffb0L31' target='_blank'>Link</a></div><div id='project'> Project Name: DenisTome/Lifting-from-the-Deep-release</div><div id='commit'> Commit Name: 70f229dde624412adb0bc466b4eee4929fcc1d91</div><div id='time'> Time: 2017-07-13</div><div id='author'> Author: dario.turchi@ocado.com</div><div id='file'> File Name: demo.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/virajmavani/semi-auto-image-annotation-tool/commit/fdc5ba0874f3c978d185f86c67f815527f9afa6f#diff-b10564ab7d2c520cdd0243874879fb0a782862c3c902ab535faabe57d5a505e1L30' target='_blank'>Link</a></div><div id='project'> Project Name: virajmavani/semi-auto-image-annotation-tool</div><div id='commit'> Commit Name: fdc5ba0874f3c978d185f86c67f815527f9afa6f</div><div id='time'> Time: 2018-05-25</div><div id='author'> Author: vishalscience75@gmail.com</div><div id='file'> File Name: main.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>