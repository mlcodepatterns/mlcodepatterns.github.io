<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            Output tensor
        
        H = x[0]
        <a id="change">extra_in = x[1:]</a>

        for layer in self._layers:
            if isinstance(layer, GraphConvolution):
                &#47&#47 It is a GCN layer add the extra inputs</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Currently we require the batch dimension to be one for full-batch methods
        batch_dim, n_nodes, _ = K.int_shape(x_in)
        <a id="change">if batch_dim != 1:
            raise ValueError(
                "Currently full-batch methods only support a batch dimension of one"
            )

        &#47&#47 Convert input indices & values to a sparse matrix
       </a> if self.use_sparse:
            <a id="change">A_indices, A_values = As</a>
            Ainput = TFSparseConversion(shape=(n_nodes, n_nodes))([A_indices, A_values])

        &#47&#47 Otherwise, create dense matrix from input tensor
        else:
            <a id="change">Ainput = Lambda(lambda A: K.squeeze(A, 0))(As[0])</a>

        &#47&#47 Remove singleton batch dimension
        h_layer = x_in
        for layer in self._layers:</code></pre>