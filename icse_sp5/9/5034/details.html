<html><h3>10b7c76231fe2fd1ee0ab5260ec67a424c37a1a9,torchsample/modules/super_module.py,SuperModule,fit,#SuperModule#Any#Any#Any#Any#Any#Any#Any#Any#Any#,112
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        callbacks.on_train_begin()

        nb_batches = int(math.ceil(inputs[0].size(0) / batch_size))
        <a id="change">for epoch_idx in range(nb_epoch):
            epoch_logs = {
                &quotnb_batches&quot: nb_batches,
                &quotnb_epoch&quot: nb_epoch,
                &quothas_validation_data&quot: has_validation_data
            }
            callbacks.on_epoch_begin(epoch_idx, epoch_logs)

            for batch_idx in range(nb_batches):
                batch_logs = {&quotbatch_idx&quot: batch_idx}  
                callbacks.on_batch_begin(batch_idx, batch_logs) 

                input_batch = [Variable(x[batch_idx*batch_size:(batch_idx+1)*batch_size]) for x in inputs]
                if has_target:
                    target_batch = [Variable(y[batch_idx*batch_size:(batch_idx+1)*batch_size]) for y in targets]

                if cuda_device &gt; -1:
                    input_batch = [ins.cuda(cuda_device) for ins in input_batch]
                    if has_target:
                        target_batch = [targs.cuda(cuda_device) for targs in target_batch]

                batch_logs[&quotbatch_samples&quot] = len(input_batch[0])

                &#47&#47&#47&#47 ZERO GRAD AND FORWARD PASS
                self._optimizer.zero_grad()
                outputs = self(*input_batch)

                if not isinstance(outputs, (list,tuple)):
                    outputs = [outputs]
                if has_target:
                    loss = self._loss_fns[0](outputs[0], target_batch[0])
                    for loss_idx in range(1,nb_targets):
                        if self._has_multiple_loss_fns:
                            loss += self._loss_fns[loss_idx](outputs[loss_idx], target_batch[loss_idx])
                        else:
                            loss += self._loss_fns[0](outputs[loss_idx], target_batch[loss_idx])
                else:
                    loss = self._loss_fns[0](outputs[0])
                    for loss_idx in range(1,nb_targets):
                        if self._has_multiple_loss_fns:
                            loss += self._loss_fns[loss_idx](outputs[loss_idx])
                        else:
                            loss += self._loss_fns[0](outputs[loss_idx])
                
                if self._has_regularizers:
                    regularizer_loss = regularizers(self)
                    loss += regularizer_loss
                    batch_logs[&quotregularizer_loss&quot] = regularizer_loss

                if self._has_constraints and constraints.has_lagrangian:
                    constraint_loss = constraints(self)
                    loss += constraint_loss
                    batch_logs[&quotconstraint_loss&quot] = constraint_loss

                batch_logs[&quotloss&quot] = loss.data[0]

                if self._has_metrics:
                    metric_logs = metrics(outputs[0], target_batch[0])
                    batch_logs.update(metric_logs)

                &#47&#47 BACKWARD PASS AND OPTIMIZER STEP
                loss.backward()
                self._optimizer.step()

                callbacks.on_batch_end(batch_idx, batch_logs)
                if self._has_constraints:
                    constraints.on_batch_end(batch_idx)

            if has_validation_data:
                val_loss = self.evaluate(*validation_data, 
                                         batch_size=batch_size,
                                         cuda_device=cuda_device)
                if self._has_metrics:
                    val_loss, val_metric_logs = val_loss
                    epoch_logs.update(val_metric_logs)
                epoch_logs[&quotval_loss&quot] = val_loss
                self.history.batch_metrics[&quotval_loss&quot] = val_loss
            
            &#47&#47 END OF EPOCH
            epoch_logs.update(self.history.batch_metrics)
            if self._has_metrics:
                epoch_logs.update(metric_logs)

            callbacks.on_epoch_end(epoch_idx, epoch_logs)

            if self._has_constraints:
                constraints.on_epoch_end(epoch_idx)
            if self._has_metrics:
                metrics.reset()
            if self._stop_training:
                break

       </a> callbacks.on_train_end()

    def fit_loader(self, 
                   loader, </code></pre><h3>After Change</h3><pre><code class='java'>
            metrics = MetricsModule(self._metrics)

        &#47&#47&#47&#47 create callbacks
        <a id="change">with TQDM() as pbar:
            progressbar = []
            if verbose &gt; 0:
                progressbar = [pbar]
            callbacks = CallbackModule(self._callbacks + progressbar)
            callbacks.set_model(self)

            callbacks.on_train_begin()

            nb_batches = int(math.ceil(inputs[0].size(0) / batch_size))
            for epoch_idx in range(nb_epoch):
                epoch_logs = {
                    &quotnb_batches&quot: nb_batches,
                    &quotnb_epoch&quot: nb_epoch,
                    &quothas_validation_data&quot: has_validation_data
                }
                callbacks.on_epoch_begin(epoch_idx, epoch_logs)

                for batch_idx in range(nb_batches):
                    batch_logs = {&quotbatch_idx&quot: batch_idx}  
                    callbacks.on_batch_begin(batch_idx, batch_logs) 

                    input_batch = [Variable(x[batch_idx*batch_size:(batch_idx+1)*batch_size]) for x in inputs]
                    if has_target:
                        target_batch = [Variable(y[batch_idx*batch_size:(batch_idx+1)*batch_size]) for y in targets]

                    if cuda_device &gt; -1:
                        input_batch = [ins.cuda(cuda_device) for ins in input_batch]
                        if has_target:
                            target_batch = [targs.cuda(cuda_device) for targs in target_batch]

                    batch_logs[&quotbatch_samples&quot] = len(input_batch[0])

                    &#47&#47&#47&#47 ZERO GRAD AND FORWARD PASS
                    self._optimizer.zero_grad()
                    outputs = self(*input_batch)

                    if not isinstance(outputs, (list,tuple)):
                        outputs = [outputs]
                    if has_target:
                        loss = self._loss_fns[0](outputs[0], target_batch[0])
                        for loss_idx in range(1,nb_targets):
                            if self._has_multiple_loss_fns:
                                loss += self._loss_fns[loss_idx](outputs[loss_idx], target_batch[loss_idx])
                            else:
                                loss += self._loss_fns[0](outputs[loss_idx], target_batch[loss_idx])
                    else:
                        loss = self._loss_fns[0](outputs[0])
                        for loss_idx in range(1,nb_targets):
                            if self._has_multiple_loss_fns:
                                loss += self._loss_fns[loss_idx](outputs[loss_idx])
                            else:
                                loss += self._loss_fns[0](outputs[loss_idx])
                    
                    if self._has_regularizers:
                        regularizer_loss = regularizers(self)
                        loss += regularizer_loss
                        batch_logs[&quotregularizer_loss&quot] = regularizer_loss

                    if self._has_constraints and constraints.has_lagrangian:
                        constraint_loss = constraints(self)
                        loss += constraint_loss
                        batch_logs[&quotconstraint_loss&quot] = constraint_loss

                    batch_logs[&quotloss&quot] = loss.data[0]

                    if self._has_metrics:
                        metric_logs = metrics(outputs[0], target_batch[0])
                        batch_logs.update(metric_logs)

                    &#47&#47 BACKWARD PASS AND OPTIMIZER STEP
                    loss.backward()
                    self._optimizer.step()

                    callbacks.on_batch_end(batch_idx, batch_logs)
                    if self._has_constraints:
                        constraints.on_batch_end(batch_idx)

                if has_validation_data:
                    val_loss = self.evaluate(*validation_data, 
                                             batch_size=batch_size,
                                             cuda_device=cuda_device)
                    if self._has_metrics:
                        val_loss, val_metric_logs = val_loss
                        epoch_logs.update(val_metric_logs)
                    epoch_logs[&quotval_loss&quot] = val_loss
                    self.history.batch_metrics[&quotval_loss&quot] = val_loss
                
                &#47&#47 END OF EPOCH
                epoch_logs.update(self.history.batch_metrics)
                if self._has_metrics:
                    epoch_logs.update(metric_logs)

                callbacks.on_epoch_end(epoch_idx, epoch_logs)

                if self._has_constraints:
                    constraints.on_epoch_end(epoch_idx)
                if self._has_metrics:
                    metrics.reset()
                if self._stop_training:
                    break

            callbacks.on_train_end()

   </a> def fit_loader(self, 
                   loader, 
                   val_loader=None, 
                   nb_epoch=100,</code></pre><img src="32779797.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ncullen93/torchsample/commit/10b7c76231fe2fd1ee0ab5260ec67a424c37a1a9#diff-b3b934d999eb60c8b28f562b73c81279c415c0299e404a57d3a24e21bac45a31L112' target='_blank'>Link</a></div><div id='project'> Project Name: ncullen93/torchsample</div><div id='commit'> Commit Name: 10b7c76231fe2fd1ee0ab5260ec67a424c37a1a9</div><div id='time'> Time: 2017-04-28</div><div id='author'> Author: ncullen@Nicks-MacBook-Pro.local</div><div id='file'> File Name: torchsample/modules/super_module.py</div><div id='class'> Class Name: SuperModule</div><div id='method'> Method Name: fit</div><BR><BR><div id='link'><a href='https://github.com/pantsbuild/pants/commit/0093628056bbf454faf35709ee3c61acc133d0e9#diff-21d47889f9ebc1d29fd053648f327e1f30bda26ccfb10c837fee7536dda474e6L99' target='_blank'>Link</a></div><div id='project'> Project Name: pantsbuild/pants</div><div id='commit'> Commit Name: 0093628056bbf454faf35709ee3c61acc133d0e9</div><div id='time'> Time: 2016-11-05</div><div id='author'> Author: wisechengyi@gmail.com</div><div id='file'> File Name: contrib/scrooge/src/python/pants/contrib/scrooge/tasks/thrift_linter.py</div><div id='class'> Class Name: ThriftLinter</div><div id='method'> Method Name: execute</div><BR><BR><div id='link'><a href='https://github.com/asyml/texar/commit/cf0cb3360a77041187c655891da4ddffbe4b13dd#diff-b89b23bb5bd7e0de33d86ed3a5842c59b3e818a108686a5e53b521160238ca74L443' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: cf0cb3360a77041187c655891da4ddffbe4b13dd</div><div id='time'> Time: 2018-11-27</div><div id='author'> Author: haoranshi97@gmail.com</div><div id='file'> File Name: texar/core/optimization.py</div><div id='class'> Class Name: AdamWeightDecayOptimizer</div><div id='method'> Method Name: apply_gradients</div><BR>