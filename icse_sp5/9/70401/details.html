<html><h3>164dd4dd3df73af90931d0dcc6ccab94956adc32,src/models.py,HeadlessPairAttnEncoder,__init__,#HeadlessPairAttnEncoder#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#Any#,664
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                                                                encoding_dim))
        &#47&#47if text_field_embedder.get_output_dim() != phrase_layer.get_input_dim():
        if (cove_layer is None and text_field_embedder.get_output_dim() != phrase_layer.get_input_dim()) \
                or (cove_layer is not None and <a id="change">text_field_embedder.get_output_dim()</a> + 600 != phrase_layer.get_input_dim()):

            raise ConfigurationError("The output dimension of the "
                                     "text_field_embedder (embedding_dim + "</code></pre><h3>After Change</h3><pre><code class='java'>
                 initializer=InitializerApplicator(), regularizer=None):
        super(HeadlessPairAttnEncoder, self).__init__(vocab)&#47&#47, regularizer)

        <a id="change">if text_field_embedder is None: &#47&#47 just using ELMo embeddings
            self._text_field_embedder = lambda x: x
            d_emb = 0
            self._highway_layer = lambda x: x
        else:
            self._text_field_embedder = text_field_embedder
            d_emb = text_field_embedder.get_output_dim()
            self._highway_layer = TimeDistributed(Highway(d_emb, num_highway_layers))

       </a> self._phrase_layer = phrase_layer
        self._matrix_attention = MatrixAttention(attention_similarity_function)
        self._modeling_layer = modeling_layer
        self._cove = cove_layer</code></pre><img src="325435237.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jsalt18-sentence-repl/jiant/commit/164dd4dd3df73af90931d0dcc6ccab94956adc32#diff-f4ca477841bd3ad5c0b292cbfff7c3ae403a65fb6c97deb15ad996f85dc87bd8L670' target='_blank'>Link</a></div><div id='project'> Project Name: jsalt18-sentence-repl/jiant</div><div id='commit'> Commit Name: 164dd4dd3df73af90931d0dcc6ccab94956adc32</div><div id='time'> Time: 2018-04-08</div><div id='author'> Author: wang.alex.c@gmail.com</div><div id='file'> File Name: src/models.py</div><div id='class'> Class Name: HeadlessPairAttnEncoder</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp/commit/700abc65fd2172a2c6809dd9b72cf50fc2407772#diff-3bf7957a6edb5f382874f84103b2b1a1400dfe04e60e17423482bcaee2182e55L64' target='_blank'>Link</a></div><div id='project'> Project Name: allenai/allennlp</div><div id='commit'> Commit Name: 700abc65fd2172a2c6809dd9b72cf50fc2407772</div><div id='time'> Time: 2020-02-03</div><div id='author'> Author: mattg@allenai.org</div><div id='file'> File Name: allennlp/models/decomposable_attention.py</div><div id='class'> Class Name: DecomposableAttention</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/jsalt18-sentence-repl/jiant/commit/164dd4dd3df73af90931d0dcc6ccab94956adc32#diff-f4ca477841bd3ad5c0b292cbfff7c3ae403a65fb6c97deb15ad996f85dc87bd8L387' target='_blank'>Link</a></div><div id='project'> Project Name: jsalt18-sentence-repl/jiant</div><div id='commit'> Commit Name: 164dd4dd3df73af90931d0dcc6ccab94956adc32</div><div id='time'> Time: 2018-04-08</div><div id='author'> Author: wang.alex.c@gmail.com</div><div id='file'> File Name: src/models.py</div><div id='class'> Class Name: HeadlessSentEncoder</div><div id='method'> Method Name: __init__</div><BR>