<html><h3>8be52c9dc6198fbbd5a0b3e8e80571a505a21b1d,art/attacks/carlini.py,CarliniL2Method,generate,#CarliniL2Method#Any#,184
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    if attack_success:
                        logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                        if l2dist &lt; best_l2dist:
                            <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot % (l2dist, best_l2dist))</a>
                            best_l2dist = l2dist
                            best_adv_image = adv_image
                    
                    &#47&#47 compute gradient:
                    logger.debug(&quotCompute loss gradient&quot)
                    perturbation_tanh = -self._loss_gradient(z, target, image, adv_image, adv_image_tanh, 
                                                             c, clip_min, clip_max)
                    
                    &#47&#47 perform line search to optimize perturbation                     
                    &#47&#47 first, halve the learning rate until perturbation actually decreases the loss:                      
                    prev_loss = loss
                    best_loss = loss
                    best_lr = 0
                    
                    halving = 0
                    while loss &gt;= prev_loss and halving &lt; self.max_halving:
                        logger.debug(&quotApply gradient with learning rate %f (halving=%i)&quot % (lr, halving))
                        new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                        new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                        _, l2dist, loss = self._loss(image, new_adv_image, target, c) 
                        logger.debug(&quotNew Total Loss: %f&quot, loss)
                        logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                        logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)      
                        if loss &lt; best_loss:
                            best_loss = loss
                            best_lr = lr
                        lr /= 2
                        halving += 1                        
                    lr *= 2
                    
                    &#47&#47 if no halving was actually required, double the learning rate as long as this
                    &#47&#47 decreases the loss:
                    if halving == 1 and loss &lt;= prev_loss:
                        doubling = 0
                        while loss &lt;= prev_loss and doubling &lt; self.max_doubling:  
                            prev_loss = loss
                            lr *= 2     
                            logger.debug(&quotApply gradient with learning rate %f (doubling=%i)&quot % (lr, doubling))
                            doubling += 1
                            new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                            new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                            _, l2dist, loss = self._loss(image, new_adv_image, target, c)                            
                            logger.debug(&quotNew Total Loss: %f&quot, loss)
                            logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                            logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)     
                            if loss &lt; best_loss:
                                best_loss = loss
                                best_lr = lr            
                        lr /= 2
                    
                    if best_lr &gt;0:
                        logger.debug(&quotFinally apply gradient with learning rate %f&quot, best_lr)
                        &#47&#47 apply the optimal learning rate that was found and update the loss:
                        adv_image_tanh = adv_image_tanh + best_lr * perturbation_tanh
                        adv_image = self._tanh_to_original(adv_image_tanh, clip_min, clip_max)
                        
                    z, l2dist, loss = self._loss(image, adv_image, target, c)                    
                    attack_success = (loss - l2dist &lt;= 0)
                    overall_attack_success = overall_attack_success or attack_success
                
                &#47&#47 Update depending on attack success:
                if attack_success:
                    logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                    if l2dist &lt; best_l2dist:
                        <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot % (l2dist, best_l2dist))</a>
                        best_l2dist = l2dist
                        best_adv_image = adv_image
                
                if overall_attack_success:</code></pre><h3>After Change</h3><pre><code class='java'>
                    if attack_success:
                        logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                        if l2dist &lt; best_l2dist:
                            <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot, l2dist, best_l2dist)</a>
                            best_l2dist = l2dist
                            best_adv_image = adv_image
                    
                    &#47&#47 compute gradient:
                    logger.debug(&quotCompute loss gradient&quot)
                    perturbation_tanh = -self._loss_gradient(z, target, image, adv_image, adv_image_tanh, 
                                                             c, clip_min, clip_max)
                    
                    &#47&#47 perform line search to optimize perturbation                     
                    &#47&#47 first, halve the learning rate until perturbation actually decreases the loss:                      
                    prev_loss = loss
                    best_loss = loss
                    best_lr = 0
                    
                    halving = 0
                    while loss &gt;= prev_loss and halving &lt; self.max_halving:
                        logger.debug(&quotApply gradient with learning rate %f (halving=%i)&quot, lr, halving)
                        new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                        new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                        _, l2dist, loss = self._loss(image, new_adv_image, target, c) 
                        logger.debug(&quotNew Total Loss: %f&quot, loss)
                        logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                        logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)      
                        if loss &lt; best_loss:
                            best_loss = loss
                            best_lr = lr
                        lr /= 2
                        halving += 1                        
                    lr *= 2
                    
                    &#47&#47 if no halving was actually required, double the learning rate as long as this
                    &#47&#47 decreases the loss:
                    if halving == 1 and loss &lt;= prev_loss:
                        doubling = 0
                        while loss &lt;= prev_loss and doubling &lt; self.max_doubling:  
                            prev_loss = loss
                            lr *= 2     
                            logger.debug(&quotApply gradient with learning rate %f (doubling=%i)&quot, lr, doubling)
                            doubling += 1
                            new_adv_image_tanh = adv_image_tanh + lr * perturbation_tanh
                            new_adv_image = self._tanh_to_original(new_adv_image_tanh, clip_min, clip_max)
                            _, l2dist, loss = self._loss(image, new_adv_image, target, c)                            
                            logger.debug(&quotNew Total Loss: %f&quot, loss)
                            logger.debug(&quotNew L2Dist: %f&quot, l2dist)
                            logger.debug(&quotNew Margin Loss: %f&quot, loss-l2dist)     
                            if loss &lt; best_loss:
                                best_loss = loss
                                best_lr = lr            
                        lr /= 2
                    
                    if best_lr &gt;0:
                        logger.debug(&quotFinally apply gradient with learning rate %f&quot, best_lr)
                        &#47&#47 apply the optimal learning rate that was found and update the loss:
                        adv_image_tanh = adv_image_tanh + best_lr * perturbation_tanh
                        adv_image = self._tanh_to_original(adv_image_tanh, clip_min, clip_max)
                        
                    z, l2dist, loss = self._loss(image, adv_image, target, c)                    
                    attack_success = (loss - l2dist &lt;= 0)
                    overall_attack_success = overall_attack_success or attack_success
                
                &#47&#47 Update depending on attack success:
                if attack_success:
                    logger.debug(&quotMargin Loss &lt;= 0 --&gt; Attack Success!&quot)
                    if l2dist &lt; best_l2dist:
                        <a id="change">logger.debug(&quotNew best L2Dist: %f (previous=%f)&quot, l2dist, best_l2dist)</a>
                        best_l2dist = l2dist
                        best_adv_image = adv_image
                
                if overall_attack_success:</code></pre><img src="34840723.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/8be52c9dc6198fbbd5a0b3e8e80571a505a21b1d#diff-c1ad1dccee8a742954884bfabae3e4b877eace13c3da1e2c11eec208489a5e5fL213' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 8be52c9dc6198fbbd5a0b3e8e80571a505a21b1d</div><div id='time'> Time: 2018-10-31</div><div id='author'> Author: mathsinn@ie.ibm.com</div><div id='file'> File Name: art/attacks/carlini.py</div><div id='class'> Class Name: CarliniL2Method</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/IBM/adversarial-robustness-toolbox/commit/8be52c9dc6198fbbd5a0b3e8e80571a505a21b1d#diff-c1ad1dccee8a742954884bfabae3e4b877eace13c3da1e2c11eec208489a5e5fL213' target='_blank'>Link</a></div><div id='project'> Project Name: IBM/adversarial-robustness-toolbox</div><div id='commit'> Commit Name: 8be52c9dc6198fbbd5a0b3e8e80571a505a21b1d</div><div id='time'> Time: 2018-10-31</div><div id='author'> Author: mathsinn@ie.ibm.com</div><div id='file'> File Name: art/attacks/carlini.py</div><div id='class'> Class Name: CarliniL2Method</div><div id='method'> Method Name: generate</div><BR><BR><div id='link'><a href='https://github.com/Microsoft/nni/commit/7c4b8c0d3d7d14c362892e44a1d256732d13a258#diff-e356772dd8c2332d4f9cac86ad88d995274647f7d6c22a52fd4572f8c5f8015eL302' target='_blank'>Link</a></div><div id='project'> Project Name: Microsoft/nni</div><div id='commit'> Commit Name: 7c4b8c0d3d7d14c362892e44a1d256732d13a258</div><div id='time'> Time: 2019-10-29</div><div id='author'> Author: 40699903+liuzhe-lz@users.noreply.github.com</div><div id='file'> File Name: src/sdk/pynni/nni/bohb_advisor/config_generator.py</div><div id='class'> Class Name: CG_BOHB</div><div id='method'> Method Name: new_result</div><BR><BR><div id='link'><a href='https://github.com/RaRe-Technologies/gensim/commit/9112ee7e48ef67af672d171874bdc2c7315d0dc2#diff-0035b20cc255b7b262c2e2b68e1a434a74dc719dfbd5b1cef9f7b8ef7b216eb4L535' target='_blank'>Link</a></div><div id='project'> Project Name: RaRe-Technologies/gensim</div><div id='commit'> Commit Name: 9112ee7e48ef67af672d171874bdc2c7315d0dc2</div><div id='time'> Time: 2017-01-10</div><div id='author'> Author: akutuzov72@gmail.com</div><div id='file'> File Name: gensim/models/keyedvectors.py</div><div id='class'> Class Name: KeyedVectors</div><div id='method'> Method Name: evaluate_word_pairs</div><BR>