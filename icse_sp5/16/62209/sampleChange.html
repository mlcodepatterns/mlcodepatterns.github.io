<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.logger.debug(&quott:%s r:%s&quot, self.t, reward)

        <a id="change">if self.async_update and self.update_future:
            self.update_future.result()
            self.update_future = None

       </a> if self.clip_reward:
            reward = np.clip(reward, -1, 1)

        greedy_action = self.act(state)
        action = self.explorer.select_action(self.t, lambda: greedy_action)
        self.t += 1

        &#47&#47 Update the target network
        if self.t % self.target_update_frequency == 0:
            self.sync_target_network()

        if self.last_state is not None:
            assert self.last_action is not None
            &#47&#47 Add a transition to the replay buffer
            self.replay_buffer.append(
                state=self.last_state,
                action=self.last_action,
                reward=reward,
                next_state=state,
                next_action=action,
                is_state_terminal=False)

        self.last_state = state
        self.last_action = action

        <a id="change">self.update_if_necessary()</a>

        return self.last_action

    def update_if_necessary(self):</code></pre><h3>After Change</h3><pre><code class='java'>
                actor_loss += self.compute_actor_loss(batch)
            self.actor_optimizer.update(lambda: actor_loss / max_epi_len)

    def act_and_train(<a id="change">self</a>, state, reward):

        self.logger.debug(&quott:%s r:%s&quot, self.t, reward)

        if self.clip_reward:
            reward = np.clip(reward, -1, 1)

        greedy_action = self.act(state)
        action = self.explorer.select_action(self.t, lambda: greedy_action)
        <a id="change">self.t</a> += 1

        &#47&#47 Update the target network
        if self.t % self.target_update_frequency == 0:
            self.sync_target_network()

        if self.last_state is not None:
            assert self.last_action is not None
            &#47&#47 Add a transition to the replay buffer
            self.replay_buffer.append(
                state=self.last_state,
                action=self.last_action,
                reward=reward,
                next_state=state,
                next_action=action,
                is_state_terminal=False)

        self.last_state = state
        self.last_action = action

        <a id="change">self.replay_updator.update_if_necessary(self.t)</a>

        return self.last_action

    def act(self, state):</code></pre>