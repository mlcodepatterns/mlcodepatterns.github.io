<html><h3>1e9c3ee592be5e11dcce932a73009488d6f85474,ch17/02_imag.py,,,#,78
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                tb_tracker.track("total_reward", m_reward, step_idx)
                tb_tracker.track("total_steps", m_steps, step_idx)

            obs_v = <a id="change">Variable(torch.from_numpy(mb_obs))</a>
            obs_next_v = <a id="change">Variable(torch.from_numpy(mb_obs_next))</a>
            actions_t = torch.LongTensor(mb_actions.tolist())
            rewards_v = <a id="change">Variable(torch.from_numpy(mb_rewards))</a>
            <a id="change">if args.cuda:
                obs_v = obs_v.cuda()
                actions_t = actions_t.cuda()
                obs_next_v = obs_next_v.cuda()
                rewards_v = rewards_v.cuda()

           </a> optimizer.zero_grad()
            out_obs_next_v, out_reward_v = net_em(obs_v.float()/255, actions_t)
            loss_obs_v = F.mse_loss(out_obs_next_v, obs_next_v)
            loss_rew_v = F.mse_loss(out_reward_v, rewards_v)</code></pre><h3>After Change</h3><pre><code class='java'>

            optimizer.zero_grad()
            out_obs_next_v, out_reward_v = net_em(obs_v.float()/255, actions_t)
            loss_obs_v = F.mse_loss(<a id="change">out_obs_next_v.squeeze(-1)</a>, obs_next_v)
            loss_rew_v = F.mse_loss(out_reward_v.squeeze(-1), rewards_v)
            loss_total_v = OBS_WEIGHT * loss_obs_v + REWARD_WEIGHT * loss_rew_v
            loss_total_v.backward()</code></pre><img src="29855399.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/1e9c3ee592be5e11dcce932a73009488d6f85474#diff-4167e56f1146ee4586776221565ff5ec0cdeed70670f15816d5f232f757fbd6aL81' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 1e9c3ee592be5e11dcce932a73009488d6f85474</div><div id='time'> Time: 2018-04-29</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch17/02_imag.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/171e9e18a10f2daea090bc6f4815db41072d66b6#diff-3127b6eeb9f67b6225c6a2477817085516f6694a6a98b4f484633768f899b193L115' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 171e9e18a10f2daea090bc6f4815db41072d66b6</div><div id='time'> Time: 2018-04-27</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch10/03_pong_a2c_rollouts.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/commit/1e9c3ee592be5e11dcce932a73009488d6f85474#diff-f0bf571463d6a24e7ec7c08e3b48b5dea2f2559833e5c8491ce3a32a1e0aab1bL154' target='_blank'>Link</a></div><div id='project'> Project Name: PacktPublishing/Deep-Reinforcement-Learning-Hands-On</div><div id='commit'> Commit Name: 1e9c3ee592be5e11dcce932a73009488d6f85474</div><div id='time'> Time: 2018-04-29</div><div id='author'> Author: max.lapan@gmail.com</div><div id='file'> File Name: ch17/lib/common.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: train_a2c</div><BR>