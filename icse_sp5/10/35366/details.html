<html><h3>145170ca9bbd89aa01d8a40841e3c039d3683af8,stellargraph/layer/graph_attention.py,GraphAttention,call,#GraphAttention#Any#,211
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Convert A to dense tensor - needed for the mask to work
        &#47&#47 TODO: replace this dense implementation of GraphAttention layer with a sparse implementation
        if K.is_sparse(A):
            <a id="change">A = tf.sparse_tensor_to_dense(A, validate_indices=False)</a>

        &#47&#47 For the GAT model to match that in the paper, we need to ensure that the graph has self-loops,
        &#47&#47 since the neighbourhood of node i in eq. (4) includes node i itself.
        &#47&#47 Adding self-loops to A via setting the diagonal elements of A to 1.0:
        <a id="change">if kwargs.get("add_self_loops", False):
            &#47&#47 get the number of nodes from inputs[1] directly
            N = K.int_shape(inputs[1])[-1]
            if N is not None:
                &#47&#47 create self-loops
                A = tf.linalg.set_diag(A, K.cast(np.ones((N,)), dtype="float"))
            else:
                raise ValueError(
                    "{}: need to know number of nodes to add self-loops; obtained None instead".format(
                        type(self).__name__
                    )
                )

       </a> outputs = []
        for head in range(self.attn_heads):
            kernel = self.kernels[head]  &#47&#47 W in the paper (F x F&quot)
            attention_kernel = self.attn_kernels[</code></pre><h3>After Change</h3><pre><code class='java'>
            dense += mask

            &#47&#47 Apply softmax to get attention coefficients
            dense = <a id="change">K.softmax(dense, axis=1)</a>  &#47&#47 (N x N), Eq. 3 of the paper

            &#47&#47 Apply dropout to features and attention coefficients
            dropout_feat = Dropout(self.in_dropout_rate)(features)  &#47&#47 (N x F&quot)</code></pre><img src="172048724.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stellargraph/stellargraph/commit/145170ca9bbd89aa01d8a40841e3c039d3683af8#diff-a7fe88a489b615f3fc3ad491dab9fad6f9195bdf9cea28b70933003c04e8095cL211' target='_blank'>Link</a></div><div id='project'> Project Name: stellargraph/stellargraph</div><div id='commit'> Commit Name: 145170ca9bbd89aa01d8a40841e3c039d3683af8</div><div id='time'> Time: 2019-06-03</div><div id='author'> Author: andrew.docherty@data61.csiro.au</div><div id='file'> File Name: stellargraph/layer/graph_attention.py</div><div id='class'> Class Name: GraphAttention</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/4c02d6d4da5ba31d7c9e6e11e6415e8bd2fa2962#diff-fbbf2e5c0c8f7f982f1f21477db91c6661e370163db852d18a9d4b446ce2be0bL100' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 4c02d6d4da5ba31d7c9e6e11e6415e8bd2fa2962</div><div id='time'> Time: 2016-11-21</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: tensorlayer/activation.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pixel_wise_softmax</div><BR><BR><div id='link'><a href='https://github.com/zsdonghao/text-to-image/commit/74796ff02e9425ca336f595978fe6e7c422c0378#diff-fbbf2e5c0c8f7f982f1f21477db91c6661e370163db852d18a9d4b446ce2be0bL100' target='_blank'>Link</a></div><div id='project'> Project Name: zsdonghao/text-to-image</div><div id='commit'> Commit Name: 74796ff02e9425ca336f595978fe6e7c422c0378</div><div id='time'> Time: 2017-04-11</div><div id='author'> Author: dhsig552@163.com</div><div id='file'> File Name: tensorlayer/activation.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: pixel_wise_softmax</div><BR>