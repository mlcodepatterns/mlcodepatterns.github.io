<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 cross-validation strategy
            kfold.shuffle = self.shuffle

        <a id="change">meta_features = np.zeros((X.shape[0], len(self.regressors)))</a>

        &#47&#47
        &#47&#47 The outer loop iterates over the base-regressors. Each regressor
        &#47&#47 is trained cv times and makes predictions, after which we train
        &#47&#47 the meta-regressor on their combined results.
        &#47&#47
        <a id="change">for i, regr in enumerate(self.regressors):
            &#47&#47
            &#47&#47 In the inner loop, each model is trained cv times on the
            &#47&#47 training-part of this fold of data; and the holdout-part of data
            &#47&#47 is used for predictions. This is repeated cv times, so in
            &#47&#47 the end we have predictions for each data point.
            &#47&#47
            &#47&#47 Advantage of this complex approach is that data points we&quotre
            &#47&#47 predicting have not been trained on by the algorithm, so it&quots
            &#47&#47 less susceptible to overfitting.
            &#47&#47
            for train_idx, holdout_idx in kfold.split(X, y, groups):
                instance = clone(regr)
                if sample_weight is None:
                    instance.fit(X[train_idx], y[train_idx])
                else:
                    instance.fit(X[train_idx], y[train_idx],
                                 sample_weight=sample_weight[train_idx])
                y_pred = instance.predict(X[holdout_idx])
                meta_features[holdout_idx, i] = y_pred

        &#47&#47 save meta-features for training data
       </a> if self.store_train_meta_features:
            self.train_meta_features_ = meta_features

        &#47&#47 Train meta-model on the out-of-fold predictions</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 predicting have not been trained on by the algorithm, so it&quots
        &#47&#47 less susceptible to overfitting.
        if sample_weight is None:
            <a id="change">fit_params = None</a>
        else:
            <a id="change">fit_params = dict(sample_weight=sample_weight)</a>
        meta_features = np.column_stack([cross_val_predict(
                regr, X, y, groups=groups, cv=kfold,
                n_jobs=self.n_jobs, fit_params=fit_params,
                pre_dispatch=self.pre_dispatch)</code></pre>