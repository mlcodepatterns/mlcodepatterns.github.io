<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            predict_terminal_values=predict_terminal_values
        )

        <a id="change">baseline_policy = dict(network=critic_network, distributions=dict(float=&quotgaussian&quot))</a>
        baseline_optimizer = critic_optimizer
        baseline_objective = dict(type=&quotvalue&quot, value=&quotaction&quot)

        super().__init__(</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Config, saver, summarizer, recorder
        config=None, saver=None, summarizer=None, recorder=None,
        &#47&#47 Deprecated
        estimate_terminal=None, critic_network=None, **kwargs<a id="change">
    ):
        raise TensorforceError(message=&quotTemporarily broken.&quot)
        if estimate_terminal is not None:
            raise TensorforceError.deprecated(
                n</a>ame=&quotDPG&quot, argument=&quotestimate_terminal&quot, replacement=&quotpredict_terminal_values&quot
            )
        if critic_network is not None:
            raise TensorforceError.deprecated(
                name=&quotDPG&quot, argument=&quotcritic_network&quot, replacement=&quotcritic&quot
            )

        self.spec = OrderedDict(
            agent=&quotdpg&quot,
            states=states, actions=actions, memory=memory, batch_size=batch_size,
            max_episode_timesteps=max_episode_timesteps,
            network=network, use_beta_distribution=use_beta_distribution,
            update_frequency=update_frequency, start_updating=start_updating,
            learning_rate=learning_rate,
            horizon=horizon, discount=discount, predict_terminal_values=predict_terminal_values,
            critic=critic, critic_optimizer=critic_optimizer,
            preprocessing=preprocessing,
            exploration=exploration, variable_noise=variable_noise,
            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,
            parallel<a id="change">_interactions=parallel_interactions,
            config=config, saver=saver, summarizer=summarizer, recorder=recorder
        )

        policy =</a> dict(
            type=&quotparametrized_distributions&quot, network=network, temperature=0.0,
            use_beta_distribution=use_beta_distribution
        )</code></pre>