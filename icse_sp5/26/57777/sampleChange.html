<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
             norm=&quotbatch&quot, nl=&quotlrelu&quot,
             use_sigmoid=False, init_type=&quotxavier&quot, num_Ds=1, gpu_ids=[]):
    netD = None
    <a id="change">use_gpu = len(gpu_ids) &gt; 0</a><a id="change">
    n</a>orm_layer = get_norm_layer(layer_type=norm)
    nl = &quotlrelu&quot  &#47&#47 use leaky relu for D
    nl_layer = get_non_linearity(layer_type=nl)
    if use_gpu:
        assert(torch.cuda.is_available())
    if which_model_netD == &quotbasic_128&quot:
        netD = D_NLayers(input_nc, ndf, n_layers=2, norm_layer=norm_layer,
                         nl_layer=nl_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)
    elif which_model_netD == &quotbasic_256&quot:
        netD = D_NLayers(input_nc, ndf, n_layers=3, norm_layer=norm_layer,
                         nl_layer=nl_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)
    elif which_model_netD == &quotbasic_128_multi&quot:
        netD = D_NLayersMulti(input_nc=input_nc, ndf=ndf, n_layers=2, norm_layer=norm_layer,
                              use_sigmoid=use_sigmoid, gpu_ids=gpu_ids, num_D=num_Ds)
    elif which_model_netD == &quotbasic_256_multi&quot:
        netD = D_NLayersMulti(input_nc=input_nc, ndf=ndf, n_layers=3, norm_layer=norm_layer,
                              use_sigmoid=use_sigmoid, gpu_ids=gpu_ids, num_D=num_Ds)
    else:
        raise NotImplementedError(
            &quotDiscriminator model name [%s] is not recognized&quot % which_model_netD)
    <a id="change">if use_gpu:
        netD.cuda(gpu_ids[0])
   </a> <a id="change">init_weights(netD, init_type=init_type)</a>
    <a id="change">return netD</a>


def define_E(input_nc, output_nc, ndf, which_model_netE,
             norm=&quotbatch&quot, nl=&quotlrelu&quot,</code></pre><h3>After Change</h3><pre><code class='java'>
def define_D(input_nc, ndf, which_model_netD,
             norm=&quotbatch&quot, nl=&quotlrelu&quot,
             use_sigmoid=False, init_type=&quotxavier&quot, num_Ds=1, gpu_ids=[]):
    <a id="change">netD</a> = None
    norm_layer = get_norm_layer(layer_type=norm)
    nl = &quotlrelu&quot  &#47&#47 use leaky relu for D
    nl_layer = get_non_linearity(layer_type=nl)

    if which_model_netD == &quotbasic_128&quot:
        <a id="change">netD</a> = D_NLayers(input_nc, ndf, n_layers=2, norm_layer=norm_layer,
                         nl_layer=nl_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)
    elif which_model_netD == &quotbasic_256&quot:
        <a id="change">netD</a> = D_NLayers(input_nc, ndf, n_layers=3, norm_layer=norm_layer,
                         nl_layer=nl_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)
    elif which_model_netD == &quotbasic_128_multi&quot:
        netD = D_NLayersMulti(input_nc=input_nc, ndf=ndf, n_layers=2, norm_layer=norm_layer,
                              use_sigmoid=use_sigmoid, gpu_ids=gpu_ids, num_D=num_Ds)
    elif which_model_netD == &quotbasic_256_multi&quot:
        netD = D_NLayersMulti(input_nc=input_nc, ndf=ndf, n_layers=3, norm_layer=norm_layer,
                              use_sigmoid=use_sigmoid, gpu_ids=gpu_ids, num_D=num_Ds)
    else:
        raise NotImplementedError(
            &quotDiscriminator model name [%s] is not recognized&quot % which_model_netD)
    <a id="change">return init_net(netD, init_type, gpu_ids)</a>


def define_E(input_nc, output_nc, ndf, which_model_netE,
             norm=&quotbatch&quot, nl=&quotlrelu&quot,</code></pre>