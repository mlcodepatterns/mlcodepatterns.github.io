<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Main optimization loop.

    for i in range(steps):
      <a id="change">self</a>._session.run(self._clear_gradients)
      for j in range(self.meta_batch_size):
        self.learner.select_task()
        inputs = self.learner.get_batch()
        feed_dict = <a id="change">{}</a>
        feed_dict[self._global_step] = i
        for k in range(len(inputs)):
          feed_dict[self._input_placeholders[k]] = inputs[k]
          feed_dict[<a id="change">self</a>._meta_placeholders[k]] = inputs[k]
        <a id="change">self._session.run(self._add_gradients, feed_dict=feed_dict)</a>
      self._session.run(self._meta_train_op)

      &#47&#47 Do checkpointing.
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Main optimization loop.

    learner = self.learner
    <a id="change">variables = learner.variables</a>
    for i in range(steps):
      for j in range(self.meta_batch_size):
        learner.select_task()
        meta_loss, meta_gradients = self._compute_meta_loss(
            learner.get_batch(), learner.get_batch(), variables)
        if j == 0:
          summed_gradients = meta_gradients
        else:
          summed_gradients = [
              s + g for s, g in zip(summed_gradients, meta_gradients)
          ]
      <a id="change">self._tf_optimizer.apply_gradients(zip(summed_gradients, variables))</a>

      &#47&#47 Do checkpointing.

      if i == steps - 1 or time.time() &gt;= checkpoint_time + checkpoint_interval:</code></pre>