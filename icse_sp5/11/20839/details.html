<html><h3>8fae3aef46180186d420db3ec88fc747261f0d5c,models/ShowTellModel.py,ShowTellModel,_sample,#ShowTellModel#Any#Any#Any#Any#,120
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        batch_size = fc_feats.size(0)
        state = self.init_hidden(batch_size)
        <a id="change">seq = []</a>
        seqLogprobs = <a id="change">[]</a>
        for t in range(self.seq_length + 2):
            if t == 0:
                xt = self.img_embed(fc_feats)
            else:
                if t == 1: &#47&#47 input &lt;bos&gt;
                    it = fc_feats.data.new(batch_size).long().zero_()
                elif sample_max:
                    sampleLogprobs, it = torch.max(logprobs.data, 1)
                    it = it.view(-1).long()
                else:
                    if temperature == 1.0:
                        prob_prev = torch.exp(logprobs.data).cpu() &#47&#47 fetch prev distribution: shape Nx(M+1)
                    else:
                        &#47&#47 scale logprobs by temperature
                        prob_prev = torch.exp(torch.div(logprobs.data, temperature)).cpu()
                    it = torch.multinomial(prob_prev, 1).cuda()
                    sampleLogprobs = logprobs.gather(1, it) &#47&#47 gather the logprobs at sampled positions
                    it = it.view(-1).long() &#47&#47 and flatten indices for downstream processing

                xt = self.embed(it)

            if t &gt;= 2:
                &#47&#47 stop when all finished
                if t == 2:
                    unfinished = it &gt; 0
                else:
                    unfinished = unfinished * (it &gt; 0)
                if unfinished.sum() == 0:
                    break
                it = it * unfinished.type_as(it)
                seq.append(it) &#47&#47seq[t] the input of t+2 time step
                seqLogprobs.append(sampleLogprobs.view(-1))

            output, state = self.core(xt.unsqueeze(0), state)
            logprobs = F.log_softmax(self.logit(self.dropout(output.squeeze(0))), dim=1)

        <a id="change">return torch.cat([_.unsqueeze(1) for _ in seq], 1), torch.cat([_.unsqueeze(1) for _ in seqLogprobs], 1)</a></code></pre><h3>After Change</h3><pre><code class='java'>

        batch_size = fc_feats.size(0)
        state = self.init_hidden(batch_size)
        seq = <a id="change">fc_feats.new_zeros(batch_size, self.seq_length, dtype=torch.long)</a>
        <a id="change">seqLogprobs = fc_feats.new_zeros(batch_size, self.seq_length)</a>
        for t in range(self.seq_length + 2):
            if t == 0:
                xt = self.img_embed(fc_feats)
            else:
                if t == 1: &#47&#47 input &lt;bos&gt;
                    it = fc_feats.data.new(batch_size).long().zero_()
                xt = self.embed(it)

            output, state = self.core(xt.unsqueeze(0), state)
            logprobs = F.log_softmax(self.logit(self.dropout(output.squeeze(0))), dim=1)

            &#47&#47 sample the next word
            if t == self.seq_length + 1: &#47&#47 skip if we achieve maximum length
                break
            if sample_max:
                sampleLogprobs, it = torch.max(logprobs.data, 1)
                it = it.view(-1).long()
            else:
                if temperature == 1.0:
                    prob_prev = torch.exp(logprobs.data).cpu() &#47&#47 fetch prev distribution: shape Nx(M+1)
                else:
                    &#47&#47 scale logprobs by temperature
                    prob_prev = torch.exp(torch.div(logprobs.data, temperature)).cpu()
                it = torch.multinomial(prob_prev, 1).cuda()
                sampleLogprobs = logprobs.gather(1, it) &#47&#47 gather the logprobs at sampled positions
                it = it.view(-1).long() &#47&#47 and flatten indices for downstream processing

            if t &gt;= 1:
                &#47&#47 stop when all finished
                if t == 1:
                    unfinished = it &gt; 0
                else:
                    unfinished = unfinished * (it &gt; 0)
                it = it * unfinished.type_as(it)
                <a id="change">seq[:,t-1] = it</a> &#47&#47seq[t] the input of t+2 time step
                seqLogprobs[:,t-1] = sampleLogprobs.view(-1)
                if unfinished.sum() == 0:
                    break

        <a id="change">return seq, seqLogprobs</a></code></pre><img src="112061485.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ruotianluo/ImageCaptioning.pytorch/commit/8fae3aef46180186d420db3ec88fc747261f0d5c#diff-eae75c2309aabbe47eeda73d20e6b7289e1224f02d5308a6a7c91ca7c1b39f62L120' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/ImageCaptioning.pytorch</div><div id='commit'> Commit Name: 8fae3aef46180186d420db3ec88fc747261f0d5c</div><div id='time'> Time: 2018-05-30</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/ShowTellModel.py</div><div id='class'> Class Name: ShowTellModel</div><div id='method'> Method Name: _sample</div><BR><BR><div id='link'><a href='https://github.com/pytorch/fairseq/commit/27568a7ebed1a35f08ac0390f35b3de9b8dad0dd#diff-408774411f25f5b4ba7f99eb97f35f1d0d37c2a722a9e7af6ff1dfc3c6bf70a6L491' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/fairseq</div><div id='commit'> Commit Name: 27568a7ebed1a35f08ac0390f35b3de9b8dad0dd</div><div id='time'> Time: 2019-11-13</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/models/levenshtein_transformer.py</div><div id='class'> Class Name: LevenshteinTransformerModel</div><div id='method'> Method Name: initialize_output_tokens</div><BR>