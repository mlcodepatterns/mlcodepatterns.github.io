<html><h3>8fae3aef46180186d420db3ec88fc747261f0d5c,models/ShowTellModel.py,ShowTellModel,_sample,#ShowTellModel#Any#Any#Any#Any#,120
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        batch_size = fc_feats.size(0)
        state = self.init_hidden(batch_size)
        <a id="change">seq = []</a>
        <a id="change">seqLogprobs = []</a>
        for t in range(self.seq_length + 2):
            if t == 0:
                xt = self.img_embed(fc_feats)
            else:
                if t == 1: &#47&#47 input &lt;bos&gt;
                    it = fc_feats.data.new(batch_size).long().zero_()
                elif sample_max:
                    sampleLogprobs, it = torch.max(logprobs.data, 1)
                    it = it.view(-1).long()
                else:
                    if temperature == 1.0:
                        prob_prev = torch.exp(logprobs.data).cpu() &#47&#47 fetch prev distribution: shape Nx(M+1)
                    else:
                        &#47&#47 scale logprobs by temperature
                        prob_prev = torch.exp(torch.div(logprobs.data, temperature)).cpu()
                    it = torch.multinomial(prob_prev, 1).cuda()
                    sampleLogprobs = logprobs.gather(1, it) &#47&#47 gather the logprobs at sampled positions
                    it = it.view(-1).long() &#47&#47 and flatten indices for downstream processing

                xt = self.embed(it)

            if t &gt;= 2:
                &#47&#47 stop when all finished
                if t == 2:
                    unfinished = it &gt; 0
                else:
                    unfinished = unfinished * (it &gt; 0)
                if unfinished.sum() == 0:
                    break
                it = it * unfinished.type_as(it)
                seq.append(it) &#47&#47seq[t] the input of t+2 time step
                seqLogprobs.append(sampleLogprobs.view(-1))

            output, state = self.core(xt.unsqueeze(0), state)
            logprobs = F.log_softmax(self.logit(self.dropout(output.squeeze(0))), dim=1)

        <a id="change">return torch.cat([_.unsqueeze(1) for _ in seq], 1), torch.cat([_.unsqueeze(1) for _ in seqLogprobs], 1)</a></code></pre><h3>After Change</h3><pre><code class='java'>

        batch_size = fc_feats.size(0)
        state = self.init_hidden(batch_size)
        <a id="change">seq = fc_feats.new_zeros(batch_size, self.seq_length, dtype=torch.long)</a>
        <a id="change">seqLogprobs = fc_feats.new_zeros(batch_size, self.seq_length)</a>
        for t in range(self.seq_length + 2):
            if t == 0:
                xt = self.img_embed(fc_feats)
            else:
                if t == 1: &#47&#47 input &lt;bos&gt;
                    it = fc_feats.data.new(batch_size).long().zero_()
                xt = self.embed(it)

            output, state = self.core(xt.unsqueeze(0), state)
            logprobs = F.log_softmax(self.logit(self.dropout(output.squeeze(0))), dim=1)

            &#47&#47 sample the next word
            if t == self.seq_length + 1: &#47&#47 skip if we achieve maximum length
                break
            if sample_max:
                sampleLogprobs, it = torch.max(logprobs.data, 1)
                it = it.view(-1).long()
            else:
                if temperature == 1.0:
                    prob_prev = torch.exp(logprobs.data).cpu() &#47&#47 fetch prev distribution: shape Nx(M+1)
                else:
                    &#47&#47 scale logprobs by temperature
                    prob_prev = torch.exp(torch.div(logprobs.data, temperature)).cpu()
                it = torch.multinomial(prob_prev, 1).cuda()
                sampleLogprobs = logprobs.gather(1, it) &#47&#47 gather the logprobs at sampled positions
                it = it.view(-1).long() &#47&#47 and flatten indices for downstream processing

            if t &gt;= 1:
                &#47&#47 stop when all finished
                if t == 1:
                    unfinished = it &gt; 0
                else:
                    unfinished = unfinished * (it &gt; 0)
                it = it * unfinished.type_as(it)
                seq[:,t-1] = it &#47&#47seq[t] the input of t+2 time step
                <a id="change">seqLogprobs[:,t-1] = sampleLogprobs.view(-1)</a>
                if unfinished.sum() == 0:
                    break

        <a id="change">return seq, seqLogprobs</a></code></pre><img src="260522143.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 16</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/8fae3aef46180186d420db3ec88fc747261f0d5c#diff-eae75c2309aabbe47eeda73d20e6b7289e1224f02d5308a6a7c91ca7c1b39f62L120' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: 8fae3aef46180186d420db3ec88fc747261f0d5c</div><div id='time'> Time: 2018-05-30</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/ShowTellModel.py</div><div id='class'> Class Name: ShowTellModel</div><div id='method'> Method Name: _sample</div><BR><BR><div id='link'><a href='https://github.com/rusty1s/pytorch_geometric/commit/f6532b3c4c329e6d5d5fb846acc441df47616c4c#diff-1aa03e878f3ce35a0166420e4b8f01de806875b6fe302c2c415bc92539178ffeL51' target='_blank'>Link</a></div><div id='project'> Project Name: rusty1s/pytorch_geometric</div><div id='commit'> Commit Name: f6532b3c4c329e6d5d5fb846acc441df47616c4c</div><div id='time'> Time: 2020-03-22</div><div id='author'> Author: matthias.fey@tu-dortmund.de</div><div id='file'> File Name: torch_geometric/nn/models/gnn_explainer.py</div><div id='class'> Class Name: GNNExplainer</div><div id='method'> Method Name: explain_node</div><BR><BR><div id='link'><a href='https://github.com/ruotianluo/self-critical.pytorch/commit/8fae3aef46180186d420db3ec88fc747261f0d5c#diff-eae75c2309aabbe47eeda73d20e6b7289e1224f02d5308a6a7c91ca7c1b39f62L120' target='_blank'>Link</a></div><div id='project'> Project Name: ruotianluo/self-critical.pytorch</div><div id='commit'> Commit Name: 8fae3aef46180186d420db3ec88fc747261f0d5c</div><div id='time'> Time: 2018-05-30</div><div id='author'> Author: rluo@ttic.edu</div><div id='file'> File Name: models/ShowTellModel.py</div><div id='class'> Class Name: ShowTellModel</div><div id='method'> Method Name: _sample</div><BR>