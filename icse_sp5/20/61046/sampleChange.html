<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        core.learn(n_steps=n_steps, n_steps_per_fit=1)
        dataset = core.evaluate(n_steps=n_steps_test, render=False)
        J = compute_J(dataset, gamma)
        <a id="change">print(&quotJ: &quot, np.mean(J))</a>

    print(&quotPress a button to visualize pendulum&quot)
    input()
    core.evaluate(n_episodes=5, render=True)</code></pre><h3>After Change</h3><pre><code class='java'>
def experiment(alg, n_epochs, n_steps, n_steps_test):
    np.random.seed()

    <a id="change">logger = Logger(alg.__name__, results_dir=None)</a>
    <a id="change">logger.strong_line()</a>
    <a id="change">logger.info(&quotExperiment Algorithm: &quot + alg.__name__)</a>

    use_cuda = torch.cuda.is_available()

    &#47&#47 MDP
    horizon = 200
    gamma = 0.99
    mdp = Gym(&quotPendulum-v0&quot, horizon, gamma)

    &#47&#47 Policy
    policy_class = OrnsteinUhlenbeckPolicy
    policy_params = dict(sigma=np.ones(1) * .2, theta=.15, dt=1e-2)

    &#47&#47 Settings
    initial_replay_size = 500
    max_replay_size = 5000
    batch_size = 200
    n_features = 80
    tau = .001

    &#47&#47 Approximator
    actor_input_shape = mdp.info.observation_space.shape
    actor_params = dict(network=ActorNetwork,
                        n_features=n_features,
                        input_shape=actor_input_shape,
                        output_shape=mdp.info.action_space.shape,
                        use_cuda=use_cuda)

    actor_optimizer = {&quotclass&quot: optim.Adam,
                       &quotparams&quot: {&quotlr&quot: .001}}

    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)
    critic_params = dict(network=CriticNetwork,
                         optimizer={&quotclass&quot: optim.Adam,
                                    &quotparams&quot: {&quotlr&quot: .001}},
                         loss=F.mse_loss,
                         n_features=n_features,
                         input_shape=critic_input_shape,
                         output_shape=(1,),
                         use_cuda=use_cuda)

    &#47&#47 Agent
    agent = alg(mdp.info, policy_class, policy_params,
                actor_params, actor_optimizer, critic_params, batch_size,
                initial_replay_size, max_replay_size, tau)

    &#47&#47 Algorithm
    core = Core(agent, mdp)

    core.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)

    &#47&#47 RUN
    dataset = core.evaluate(n_steps=n_steps_test, render=False)
    J = np.mean(compute_J(dataset, gamma))
    R = np.mean(compute_J(dataset))

    <a id="change">logger.epoch_info(0, J=J, R=R)</a>

    for n in trange(n_epochs, leave=False):
        core.learn(n_steps=n_steps, n_steps_per_fit=1)
        dataset = core.evaluate(n_steps=n_steps_test, render=False)
        J = np.mean(compute_J(dataset, gamma))
        R = np.mean(compute_J(dataset))

        <a id="change">logger.epoch_info(n+1, J=J, R=R)</a>

    logger.info(&quotPress a button to visualize pendulum&quot)
    input()
    core.evaluate(n_episodes=5, render=True)</code></pre>