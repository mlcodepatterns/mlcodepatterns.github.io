<html><h3>00726c8b57ad409363ede0e754c0137d3fcc71cc,chapter04/gamblers_problem.py,,figure_4_3,#,23
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                action_returns.append(
                    HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action])
            new_value = np.max(action_returns)
            <a id="change">delta += np.abs(state_value[state] - new_value)</a>
            &#47&#47 update state value
            state_value[state] = new_value
        if delta &lt; 1e-9:
            <a id="change">break</a>

    &#47&#47 compute the optimal policy
    policy = np.zeros(GOAL + 1)
    for state in STATES[1:GOAL]:</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 value iteration
    while True:
        <a id="change">old_state_value = state_value.copy()</a>
        sweeps_history.append(old_state_value)

        for state in STATES[1:GOAL]:
            &#47&#47 get possilbe actions for current state
            actions = np.arange(min(state, GOAL - state) + 1)
            action_returns = []
            for action in actions:
                action_returns.append(
                    HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action])
            new_value = np.max(action_returns)
            state_value[state] = new_value
        delta = <a id="change">abs</a>(state_value - old_state_value).max()
        if delta &lt; 1e-9:
            sweeps_history.append(state_value)
            break</code></pre><img src="64491528.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/commit/00726c8b57ad409363ede0e754c0137d3fcc71cc#diff-171645f18d5fd150c3077d3de64e5e9d2f0b1ae59451688945ccb7ef4b7f6961L25' target='_blank'>Link</a></div><div id='project'> Project Name: ShangtongZhang/reinforcement-learning-an-introduction</div><div id='commit'> Commit Name: 00726c8b57ad409363ede0e754c0137d3fcc71cc</div><div id='time'> Time: 2019-06-12</div><div id='author'> Author: wlbksy@126.com</div><div id='file'> File Name: chapter04/gamblers_problem.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: figure_4_3</div><BR><BR><div id='link'><a href='https://github.com/nilearn/nilearn/commit/dd7c34ea3480f2ffd8843171676aaa22b1777bd8#diff-7fe63c1c525822b2fd7ac99b31a612b1a8f7bad39d70c355402384b442ad427dL30' target='_blank'>Link</a></div><div id='project'> Project Name: nilearn/nilearn</div><div id='commit'> Commit Name: dd7c34ea3480f2ffd8843171676aaa22b1777bd8</div><div id='time'> Time: 2014-05-28</div><div id='author'> Author: bertrand.thirion@inria.fr</div><div id='file'> File Name: nilearn/decomposition/tests/test_canica.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: test_canica_square_img</div><BR><BR><div id='link'><a href='https://github.com/studioml/studio/commit/5064e583f4d372e3a4038df3123efe454d34fd7d#diff-b4335406276da96186925a4cd042b89c1263e788a91396aa9dcc98a02fede50dL353' target='_blank'>Link</a></div><div id='project'> Project Name: studioml/studio</div><div id='commit'> Commit Name: 5064e583f4d372e3a4038df3123efe454d34fd7d</div><div id='time'> Time: 2018-06-29</div><div id='author'> Author: karlmutch@users.noreply.github.com</div><div id='file'> File Name: studio/rabbit_queue.py</div><div id='class'> Class Name: RMQueue</div><div id='method'> Method Name: enqueue</div><BR>