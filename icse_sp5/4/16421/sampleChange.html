<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
				&#47&#47 If there are no non-trivial unitaries for the data point v0, 
				&#47&#47 calculate the positive phase of regular (i.e. non-complex RBM)
				&#47&#47 gradient. Use the actual data point, v0.
				g_weights_amp -= torch.einsum("i,j-&gt;ij",(<a id="change">h0_amp_batch[row_count]</a>, v0)) / batch_size 
				g_vb_amp      -= v0 / batch_size
				g_hb_amp      -= h0_amp_batch[row_count] / batch_size
</code></pre><h3>After Change</h3><pre><code class='java'>
				&#47&#47 If there are no non-trivial unitaries for the data point v0, 
				&#47&#47 calculate the positive phase of regular (i.e. non-complex RBM)
				&#47&#47 gradient. Use the actual data point, v0.
				<a id="change">prob_amp = F.sigmoid(F.linear(v0, self.rbm_amp.weights, self.rbm_amp.hidden_bias))</a>
				g_weights_amp -= torch.einsum("i,j-&gt;ij", (prob_amp, v0)) / batch_size
				g_vb_amp      -= v0 / batch_size
				g_hb_amp      -= prob_amp / batch_size
</code></pre>