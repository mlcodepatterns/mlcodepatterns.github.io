<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            entropy_regularization=entropy_regularization, **kwargs
        )

        <a id="change">action_spec = next(iter(self.actions_spec.values()))</a>
        if len(self.actions_spec) &gt; 1 or action_spec.type != &quotfloat&quot or \
                (action_spec.shape != () and action_spec.shape != (1,)):
            raise TensorforceError.value(
                name=&quotDeterministicPolicyGradient&quot, argument=&quotactions&quot, value=actions,</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Config, saver, summarizer, recorder
        config=None, saver=None, summarizer=None, recorder=None,
        &#47&#47 Deprecated
        estimate_terminal=None, critic_network=None, **kwargs<a id="change">
    ):
        raise TensorforceError(message=&quotTemporarily broken.&quot)
        if estimate_terminal is not None:
            raise TensorforceError.deprecated(
                n</a>ame=&quotDPG&quot, argument=&quotestimate_terminal&quot, replacement=&quotpredict_terminal_values&quot
            )
        if critic_network is not None:
            raise TensorforceError.deprecated(
                name=&quotDPG&quot, argument=&quotcritic_network&quot, replacement=&quotcritic&quot
            )

        self.spec = OrderedDict(
            agent=&quotdpg&quot,
            states=states, actions=actions, memory=memory, batch_size=batch_size,
            max_episode_timesteps=max_episode_timesteps,
            network=network, use_beta_distribution=use_beta_distribution,
            update_frequency=update_frequency, start_updating=start_updating,
            learning_rate=learning_rate,
            horizon=horizon, discount=discount, predict_terminal_values=predict_terminal_values,
            critic=critic, critic_optimizer=critic_optimizer,
            preprocessing=preprocessing,
            exploration=exploration, variable_noise=variable_noise,
            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,
            parallel<a id="change">_interactions=parallel_interactions,
            config=config, saver=saver, summarizer=summarizer, recorder=recorder
        )

        policy =</a> dict(
            type=&quotparametrized_distributions&quot, network=network, temperature=0.0,
            use_beta_distribution=use_beta_distribution
        )

        memory = dict(type=&quotreplay&quot, capacity=memory)

        update = dict(unit=&quottimesteps&quot, batch_size=batch_size)
        if update_frequency != &quotbatch_size&quot:
            update[&quotfrequency&quot] = update_frequency
        if start_updating is not None:
            update[&quotstart&quot] = start_updating

        optimizer = dict(type=&quotadam&quot, learning_rate=learning_rat<a id="change">e)
        objective = &quotdeterministic_policy_gradient&quot

        </a>reward_estimation = dict(
            horizon=horizon, discount=discount, predict_horizon_values=&quotlate&quot,
            estimate_advantage=False, predict_action_values=True,
            predict_terminal_values=predict_terminal_values</code></pre>