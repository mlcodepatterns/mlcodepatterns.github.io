<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.weight_initializer = weight_initializer
        self.bias_initializer = bias_initializer
        self.layer_norm_eps = layer_norm_eps
        <a id="change">with self.name_scope():
            &#47&#47 Construct BertTransformer
            self.encoder = BertTransformer(
                units=units,
                hidden_size=hidden_size,
                num_layers=num_layers,
                num_heads=num_heads,
                attention_dropout_prob=attention_dropout_prob,
                hidden_dropout_prob=hidden_dropout_prob,
                output_attention=False,
                output_all_encodings=False,
                activation=activation,
                layer_norm_eps=layer_norm_eps,
                weight_initializer=weight_initializer,
                bias_initializer=bias_initializer,
                dtype=dtype,
                prefix=&quotenc_&quot,
            )
            self.encoder.hybridize()
            &#47&#47 Construct word embedding
            self.word_embed = nn.Embedding(input_dim=vocab_size,
                                           output_dim=units,
                                           weight_initializer=embed_initializer,
                                           dtype=dtype,
                                           prefix=&quotword_embed_&quot)
            self.embed_layer_norm = nn.LayerNorm(epsilon=self.layer_norm_eps,
                                                 prefix=&quotembed_ln_&quot)
            self.embed_dropout = nn.Dropout(hidden_dropout_prob)
            &#47&#47 Construct token type embedding
            self.token_type_embed = nn.Embedding(input_dim=num_token_types,
                                                 output_dim=units,
                                                 weight_initializer=weight_initializer,
                                                 prefix=&quottoken_type_embed_&quot)
            self.token_pos_embed = PositionalEmbedding(units=units,
                                                       max_length=max_length,
                                                       dtype=self._dtype,
                                                       method=pos_embed_type,
                                                       prefix=&quottoken_pos_embed_&quot)
            if self.use_pooler:
                &#47&#47 Construct pooler
                self.pooler = nn.Dense(units=units,
                                       in_units=units,
                                       flatten=False,
                                       activation=&quottanh&quot,
                                       weight_initializer=weight_initializer,
                                       bias_initializer=bias_initializer,
                                       prefix=&quotpooler_&quot)

   </a> def hybrid_forward(self, F, inputs, token_types, valid_length):
        &#47&#47 pylint: disable=arguments-differ
        Generate the representation given the inputs.
</code></pre><h3>After Change</h3><pre><code class='java'>
                 hidden_dropout_prob=0.,
                 attention_dropout_prob=0.,
                 num_token_types=2,
                 pos_embed_t<a id="change">ype=&quotlearned&quot,
                 activation=&quotgelu&quot,
                 layer_norm_eps=1E-12,
                 embed_initializer=TruncNorm(stdev=0.02),
                 weight_initializer=TruncNorm(stdev=0.02),
                 bias_initializer=&quotzeros&quot,
                 dtype=&quotfloat32&quot,
                 use_pooler=True):
        super().__init__()
        self._dtype = dtype
        self.use_pooler = use_pooler
        self.pos_embed_type = pos_embed_type
        self.num_token_types = num_token_types
        self.vocab_size = vocab_size
        self.units = units
        self.max_length = </a>max_lengt<a id="change">h
        self.activatio</a>n = activation
        self.embed_initializer = embed_initializer
        self.weight_initializer = weight_initializer
        self.bias_initializer = bias_initializer</code></pre>