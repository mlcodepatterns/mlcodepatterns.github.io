<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            tensor_utils.pad_tensor_dict(p, max_path_length) for p in env_infos
        ])

        terminals = <a id="change">[path[&quotdones&quot] for path in paths]</a>

        valids = [np.ones_like(path[&quotreturns&quot]) for path in paths]
        valids = tensor_utils.pad_tensor_n(valids, max_path_length)

        lengths = np.asarray([v.sum() for v in valids])

        ent = np.sum(self.policy.distribution.entropy(agent_infos) *
                     valids) / np.sum(valids)

        undiscounted_returns = self.evaluate_performance(
            itr,
            <a id="change">dict(env_spec=None,
                 observations=obs,
                 actions=actions,
                 rewards=rewards,
                 terminals=terminals,
                 env_infos=env_infos,
                 agent_infos=agent_infos,
                 lengths=lengths,
                 discount=self.discount)</a>)

        self.episode_reward_mean.extend(undiscounted_returns)
</code></pre><h3>After Change</h3><pre><code class='java'>

        undiscounted_returns = log_performance(
            itr,
            <a id="change">TrajectoryBatch.from_trajectory_list(self.env_spec, paths)</a>,
            discount=self.discount)

        self.episode_reward_mean.extend(undiscounted_returns)</code></pre>