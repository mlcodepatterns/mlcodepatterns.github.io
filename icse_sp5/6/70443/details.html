<html><h3>0ad33d606682537466f3430fc6d6ac7d47460f1a,beginner_source/examples_autograd/two_layer_net_autograd.py,,,#,24
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
&#47&#47 Create random Tensors for weights, and wrap them in Variables.
&#47&#47 Setting requires_grad=True indicates that we want to compute gradients with
&#47&#47 respect to these Variables during the backward pass.
w1 = Variable(<a id="change">torch.randn(D_in, H).type(dtype)</a>, requires_grad=True)
w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)

learning_rate = 1e-6</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 Create random Tensors to hold input and outputs.
&#47&#47 Setting requires_grad=False indicates that we do not need to compute gradients
&#47&#47 with respect to these Tensors during the backward pass.
x = <a id="change">torch.randn(N, D_in, device=device, dtype=dtype)</a>
y = <a id="change">torch.randn(N, D_out, device=device, dtype=dtype)</a>

&#47&#47 Create random Tensors for weights.
&#47&#47 Setting requires_grad=True indicates that we want to compute gradients with
&#47&#47 respect to these Tensors during the backward pass.
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    &#47&#47 Forward pass: compute predicted y using operations on Tensors; these
    &#47&#47 are exactly the same operations we used to compute the forward pass using
    &#47&#47 Tensors, but we do not need to keep references to intermediate values since
    &#47&#47 we are not implementing the backward pass by hand.
    y_pred = x.mm(w1).clamp(min=0).mm(w2)

    &#47&#47 Compute and print loss using operations on Tensors.
    &#47&#47 Now loss is a Tensor of shape (1,)
    &#47&#47 loss.item() gets the a scalar value held in the loss.
    loss = (y_pred - y).pow(2).sum()
    print(t, <a id="change">loss.item()</a>)

    &#47&#47 Use autograd to compute the backward pass. This call will compute the
    &#47&#47 gradient of loss with respect to all Tensors with requires_grad=True.</code></pre><img src="325568357.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/tutorials/commit/0ad33d606682537466f3430fc6d6ac7d47460f1a#diff-6f75901e5ea00aaf492e2f0b10b7efc60e263832ef83e09c1b550d5a5dfc4e9dL19' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/tutorials</div><div id='commit'> Commit Name: 0ad33d606682537466f3430fc6d6ac7d47460f1a</div><div id='time'> Time: 2018-04-24</div><div id='author'> Author: soumith@gmail.com</div><div id='file'> File Name: beginner_source/examples_autograd/two_layer_net_autograd.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/pytorch/tutorials/commit/0ad33d606682537466f3430fc6d6ac7d47460f1a#diff-402873d5e15ccb7ebca5fc94a74d1a4ad252190483ce86ca9dd661a55e9da556L50' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/tutorials</div><div id='commit'> Commit Name: 0ad33d606682537466f3430fc6d6ac7d47460f1a</div><div id='time'> Time: 2018-04-24</div><div id='author'> Author: soumith@gmail.com</div><div id='file'> File Name: beginner_source/examples_autograd/two_layer_net_custom_function.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/pytorch/tutorials/commit/0ad33d606682537466f3430fc6d6ac7d47460f1a#diff-6f75901e5ea00aaf492e2f0b10b7efc60e263832ef83e09c1b550d5a5dfc4e9dL24' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/tutorials</div><div id='commit'> Commit Name: 0ad33d606682537466f3430fc6d6ac7d47460f1a</div><div id='time'> Time: 2018-04-24</div><div id='author'> Author: soumith@gmail.com</div><div id='file'> File Name: beginner_source/examples_autograd/two_layer_net_autograd.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR><BR><div id='link'><a href='https://github.com/pytorch/tutorials/commit/0ad33d606682537466f3430fc6d6ac7d47460f1a#diff-fb3c04ed9f147a00eb109942c6174798281adc0069518f7f347622ace42f74d9L24' target='_blank'>Link</a></div><div id='project'> Project Name: pytorch/tutorials</div><div id='commit'> Commit Name: 0ad33d606682537466f3430fc6d6ac7d47460f1a</div><div id='time'> Time: 2018-04-24</div><div id='author'> Author: soumith@gmail.com</div><div id='file'> File Name: beginner_source/examples_tensor/two_layer_net_tensor.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>