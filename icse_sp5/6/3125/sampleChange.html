<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                              self.KERNEL,
                              self.IN_CHANNELS,
                              self.HIDDEN_CHANNELS], 1.)
    <a id="change">bconv1 = bias_variable([1, 1, self.HIDDEN_CHANNELS])</a>
    wfc1 = weight_variable([self.HIDDEN_FC1, self.HIDDEN_FC2], 1.)
    bfc1 = bias_variable([self.HIDDEN_FC2])
    wfc2 = weight_variable([self.HIDDEN_FC2, self.OUT_N], 1.)
    bfc2 = bias_variable([self.OUT_N])
    params = [wconv1, bconv1, wfc1, bfc1, wfc2, bfc2]

    &#47&#47 optimizer and data pipeline
    optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)

    &#47&#47 training loop
    def loop_body(i: tf.Tensor,
                  max_iter: tf.Tensor,
                  nb_epochs: tf.Tensor,
                  avg_loss: tf.Tensor) -&gt; Tuple[tf.Tensor, tf.Tensor]:
      Main model training loop.
      &#47&#47 get next batch
      x, y = training_data.get_next()

      &#47&#47 model construction
      x = tf.reshape(x, [-1, self.IN_DIM, self.IN_DIM, 1])
      layer1 = pooling(tf.nn.relu(conv2d(x, Wconv1, self.STRIDE) + bconv1))
      layer1 = tf.reshape(layer1, [-1, self.HIDDEN_FC1])
      layer2 = tf.nn.relu(tf.matmul(layer1, Wfc1) + bfc1)
      logits = tf.matmul(layer2, Wfc2) + bfc2

      loss = tf.reduce_mean(
          tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=y))

      is_end_epoch = tf.equal(i % max_iter, 0)

      def true_fn() -&gt; tf.Tensor:
        return loss

      def false_fn() -&gt; tf.Tensor:
        prev_loss = tf.cast(i - 1, tf.float32) * avg_loss
        return (prev_loss + loss) / tf.cast(i, tf.float32)

      with tf.control_dependencies([optimizer.minimize(loss)]):
        terminal_cond = tf.cond(is_end_epoch, true_fn, false_fn)
        return i + 1, max_iter, nb_epochs, terminal_cond

    loop, _, _, _ = tf.while_loop(
        self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.])

    &#47&#47 return model parameters after training
    loop = tf.print("Training complete", loop)
    with tf.control_dependencies([loop]):
      return <a id="change">[param.read_value() for param in params]</a>

  def provide_input(self) -&gt; List[tf.Tensor]:
    with tf.name_scope(&quotloading&quot):
      training_data = get_data_from_tfrecord(</code></pre><h3>After Change</h3><pre><code class='java'>
    model.add(keras.layers.Activation(&quotrelu&quot))
    model.add(keras.layers.AveragePooling2D())
    model.add(keras.layers.Flatten())
    <a id="change">model.add(keras.layers.Dense(self.HIDDEN_FC2))</a>
    model.add(keras.layers.Activation(&quotrelu&quot))
    model.add(keras.layers.Dense(self.OUT_N))

    &#47&#47 optimizer and data pipeline</code></pre>