<html><h3>066ef66950b6729a1cde321364b368d2fc3a9e1b,texar/modules/decoders/transformer_decoders.py,TransformerDecoder,_build,#TransformerDecoder#Any#Any#Any#,104
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                preds: [batch_size, target_length]
        
        logits = None
        <a id="change">if targets is not None:
            decoder_self_attention_bias = (
                attentions.attention_bias_lower_triangle(
                    layers.shape_list(targets)[1]))
            decoder_self_attention_bias = tf.Print(decoder_self_attention_bias,
                [tf.shape(decoder_self_attention_bias), decoder_self_attention_bias],
                message=&quotdecoder self attention bias&quot,
                summarize=2048)
            target_inputs = tf.nn.embedding_lookup(self._embedding, targets)
            if self._hparams.multiply_embedding_mode == &quotsqrt_depth&quot:
                target_inputs = target_inputs * \
                    (self._embedding.shape.as_list()[-1]**0.5)
            logits = self.decode(
                target_inputs,
                encoder_output,
                encoder_decoder_attention_bias,
                decoder_self_attention_bias,
            )
       </a> preds = tf.to_int32(tf.argmax(logits, axis=-1))
        return logits, preds

    def dynamic_decode(self, encoder_output, encoder_decoder_attention_bias):</code></pre><h3>After Change</h3><pre><code class='java'>
            cache=None,
        )

        <a id="change">logits = self.output_layer(decoder_output)</a>
        preds = tf.to_int32(tf.argmax(logits, axis=-1))
        return logits, preds

    def dynamic_decode(self, encoder_output, encoder_decoder_attention_bias):</code></pre><img src="78318762.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/asyml/texar/commit/066ef66950b6729a1cde321364b368d2fc3a9e1b#diff-503bcdf232acbf72c937e1f5dc030a70d6b8090987f9b11e3e25650f962b56f4L104' target='_blank'>Link</a></div><div id='project'> Project Name: asyml/texar</div><div id='commit'> Commit Name: 066ef66950b6729a1cde321364b368d2fc3a9e1b</div><div id='time'> Time: 2018-04-28</div><div id='author'> Author: shore@pku.edu.cn</div><div id='file'> File Name: texar/modules/decoders/transformer_decoders.py</div><div id='class'> Class Name: TransformerDecoder</div><div id='method'> Method Name: _build</div><BR><BR><div id='link'><a href='https://github.com/OpenNMT/OpenNMT-tf/commit/8566b142ddf39eb999e6765a216d54c957f526a3#diff-caec3954c2d581edfd0a318027078d4044f144df0757d2cfdb88d5d6fddfa7c2L44' target='_blank'>Link</a></div><div id='project'> Project Name: OpenNMT/OpenNMT-tf</div><div id='commit'> Commit Name: 8566b142ddf39eb999e6765a216d54c957f526a3</div><div id='time'> Time: 2019-04-01</div><div id='author'> Author: guillaume.klein@systrangroup.com</div><div id='file'> File Name: opennmt/models/sequence_classifier.py</div><div id='class'> Class Name: SequenceClassifier</div><div id='method'> Method Name: _call</div><BR><BR><div id='link'><a href='https://github.com/autonomio/talos/commit/a5cf5911849412e56788324fddb989eec0d56978#diff-69247b0a100878e0cbc76aedcb6cde3c2f95ab0a83a5575e358c5aeb329f5fffL47' target='_blank'>Link</a></div><div id='project'> Project Name: autonomio/talos</div><div id='commit'> Commit Name: a5cf5911849412e56788324fddb989eec0d56978</div><div id='time'> Time: 2019-03-11</div><div id='author'> Author: mailme@mikkokotila.com</div><div id='file'> File Name: talos/commands/kerasmodel.py</div><div id='class'> Class Name: KerasModel</div><div id='method'> Method Name: _create_input_model</div><BR>