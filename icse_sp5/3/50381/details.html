<html><h3>447885e15243dd18d906e2e35ac34ec6dcf9a600,Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py,PolicyGradient,_build_net,#PolicyGradient#,129
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        with tf.variable_scope(&quoteval_net&quot):
            self.q_eval = self._build_layers(self.s, self.n_actions, trainable=True)
            with tf.name_scope(&quotloss&quot):
                self.loss = tf.reduce_sum(<a id="change">tf.square(self.q_target - self.q_eval)</a>)
            with tf.name_scope(&quottrain&quot):
                self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)
</code></pre><h3>After Change</h3><pre><code class='java'>
        l1 = self._add_layer(&quothidden0&quot, self.x_inputs, self.n_features, 10, tf.nn.relu)     &#47&#47 hidden layer 1
        self.prediction = self._add_layer(&quotoutput&quot, l1, 10, 1, tf.nn.sigmoid)  &#47&#47 predicting for action 0
        with tf.name_scope(&quotloss&quot):
            loglik = self.fake_targets*<a id="change">tf.log(self.prediction)</a> + (1 - self.fake_targets)*tf.log(1-self.prediction)  &#47&#47
            self.loss = -tf.reduce_mean(loglik * self.advantages)
        with tf.name_scope(&quottrain&quot):
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)</code></pre><img src="234482088.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/MorvanZhou/tutorials/commit/447885e15243dd18d906e2e35ac34ec6dcf9a600#diff-0b950c40f343f7642bc3f76d52ac955592df4322a1c323d918cc8a630787ead7L100' target='_blank'>Link</a></div><div id='project'> Project Name: MorvanZhou/tutorials</div><div id='commit'> Commit Name: 447885e15243dd18d906e2e35ac34ec6dcf9a600</div><div id='time'> Time: 2016-12-30</div><div id='author'> Author: morvanzhou@hotmail.com</div><div id='file'> File Name: Reinforcement_learning_TUT/7_Policy_gradient/RL_brain.py</div><div id='class'> Class Name: PolicyGradient</div><div id='method'> Method Name: _build_net</div><BR><BR><div id='link'><a href='https://github.com/reinforceio/tensorforce/commit/208e5d91d7f88cc1ab10ab1a4bfdd856b2691671#diff-e9f7c2fd5c6d7511db37aadde687bdccbc3c4cc4d0e9a313c085f8e1414df6b7L60' target='_blank'>Link</a></div><div id='project'> Project Name: reinforceio/tensorforce</div><div id='commit'> Commit Name: 208e5d91d7f88cc1ab10ab1a4bfdd856b2691671</div><div id='time'> Time: 2017-05-22</div><div id='author'> Author: aok25@cl.cam.ac.uk</div><div id='file'> File Name: tensorforce/core/distributions/gaussian.py</div><div id='class'> Class Name: Gaussian</div><div id='method'> Method Name: log_probability</div><BR><BR><div id='link'><a href='https://github.com/jostmey/rwa/commit/6c2c4e18d28032ac547aa27806d581bc5691b090#diff-27e421b87f69b4c02403a4485730123bcdcab610493aba32ffafc40b53c2e5b8L34' target='_blank'>Link</a></div><div id='project'> Project Name: jostmey/rwa</div><div id='commit'> Commit Name: 6c2c4e18d28032ac547aa27806d581bc5691b090</div><div id='time'> Time: 2017-03-18</div><div id='author'> Author: hima.p.mehta@gmail.com</div><div id='file'> File Name: adding_problem_100/rwa_model/train.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: </div><BR>