<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        super(TransformerEncoder, self).__init__(name=name)
        if d_ff is None:
            d_ff = 4 * d_model
        self.ln1 = <a id="change">LayerNorm(name=&quotln_1&quot)</a>
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale)
        self.dropout = tf.keras.layers.Dropout(pdrop)
        self.ln2 = LayerNorm(name=&quotln_2&quot)
        self.feed_forward = FFN(d_model, pdrop, activation, d_ff, name=&quotffn&quot)</code></pre><h3>After Change</h3><pre><code class='java'>

class TransformerEncoder(tf.keras.layers.Layer):
    def __init__(self, num_heads, d_model, pdrop, scale=True, activation_type=&quotrelu&quot, d_ff=None, name=None):
        <a id="change">super().__init__(name=name)</a>
        self.d_model = d_model
        self.d_ff = d_ff if d_ff is not None else 4 * d_model
        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)
        self.ffn = FFN(d_model, pdrop, activation_type, d_ff, name=&quotffn&quot)</code></pre>