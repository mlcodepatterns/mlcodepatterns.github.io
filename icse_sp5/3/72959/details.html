<html><h3>582921ffe3b04ff502e1c3a05088ba2902e0f5bd,rl_coach/agents/value_optimization_agent.py,ValueOptimizationAgent,run_off_policy_evaluation,#ValueOptimizationAgent#,104
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        assert self.ope_manager
        dataset_as_episodes = self.call_memory(&quotget_all_complete_episodes_from_to&quot,
                                               (<a id="change">self.call_memory(&quotget_last_training_set_episode_id&quot)</a> + 1,
                                                self.call_memory(&quotnum_complete_episodes&quot)))
        if len(dataset_as_episodes) == 0:
            raise ValueError(&quottrain_to_eval_ratio is too high causing the evaluation set to be empty. &quot</code></pre><h3>After Change</h3><pre><code class='java'>
        
        assert self.ope_manager

        <a id="change">if not isinstance(self.pre_network_filter, NoInputFilter) and len(self.pre_network_filter.reward_filters) != 0:
            raise ValueError("Defining a pre-network reward filter when OPEs are calculated will result in a mismatch "
                             "between q values (which are scaled), and actual rewards, which are not. It is advisable "
                             "to use an input_filter, if possible, instead, which will filter the transitions directly "
                             "in the replay buffer, affecting both the q_values and the rewards themselves. ")

       </a> ips, dm, dr, seq_dr, wis = self.ope_manager.evaluate(
                                  evaluation_dataset_as_episodes=self.memory.evaluation_dataset_as_episodes,
                                  evaluation_dataset_as_transitions=self.memory.evaluation_dataset_as_transitions,
                                  batch_size=self.ap.network_wrappers[&quotmain&quot].batch_size,</code></pre><img src="333715884.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/582921ffe3b04ff502e1c3a05088ba2902e0f5bd#diff-5a0bd12475101005b68e5715dda2d46f0e4f3ca6c7ebecb32fd62babea8a5c41L104' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 582921ffe3b04ff502e1c3a05088ba2902e0f5bd</div><div id='time'> Time: 2019-05-02</div><div id='author'> Author: gal.leibovich@intel.com</div><div id='file'> File Name: rl_coach/agents/value_optimization_agent.py</div><div id='class'> Class Name: ValueOptimizationAgent</div><div id='method'> Method Name: run_off_policy_evaluation</div><BR><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/a7f5442015df3693dc4fe86755ea84ef697c76f4#diff-0c98d1a1202990548e6ee3acec4390c4ad7c11930fe78c716f27a925a553f812L541' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: a7f5442015df3693dc4fe86755ea84ef697c76f4</div><div id='time'> Time: 2018-10-23</div><div id='author'> Author: ajay.deshpande@intel.com</div><div id='file'> File Name: rl_coach/agents/agent.py</div><div id='class'> Class Name: Agent</div><div id='method'> Method Name: _should_train</div><BR><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/72a1d9d426004269997f8b40bdd64f8ee582d91e#diff-9be55ccde8ff567667fd1bae737d38e12549d0cf1a514bb0c43774ed3e83621bL90' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 72a1d9d426004269997f8b40bdd64f8ee582d91e</div><div id='time'> Time: 2018-09-04</div><div id='author'> Author: 30383381+itaicaspi-intel@users.noreply.github.com</div><div id='file'> File Name: rl_coach/agents/policy_optimization_agent.py</div><div id='class'> Class Name: PolicyOptimizationAgent</div><div id='method'> Method Name: train</div><BR>