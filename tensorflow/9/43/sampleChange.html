<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 According to Faster RCNN paper we need to initialize layers with
        &#47&#47 "from a zero-mean Gaussian distribution with standard deviation 0.01"
        &#47&#47 we use the truncated version (redraws when more than 2 std from mean.)
        <a id="change">self._initializer</a> = <a id="change">tf.contrib.layers.variance_scaling_initializer(
            factor=1., uniform=True, mode=&quotFAN_AVG&quot
        )</a>

        &#47&#47 We could use normal relu without any problems.
        self._rpn_activation = tf.nn.relu6
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 According to Faster RCNN paper we need to initialize layers with
        &#47&#47 "from a zero-mean Gaussian distribution with standard deviation 0.0
        <a id="change">self._initializer</a> = <a id="change">tf.random_normal_initializer(mean=0.0, stddev=0.01)</a>

        &#47&#47 We could use normal relu without any problems.
        self._rpn_activation = tf.nn.relu6
</code></pre>