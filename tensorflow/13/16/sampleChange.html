<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        bi_interaction = inner_product

        <a id="change">attention_temp = Dense(self.attention_factor,&quotrelu&quot,kernel_regularizer=l2(self.l2_reg_w))(bi_interaction)</a>
        attention_weight = softmax(K.dot(attention_temp, self.projection_h),axis=1)

        attention_output = K.sum(attention_weight*bi_interaction,axis=1)
        attention_output = tf.nn.dropout(attention_output,self.keep_prob,seed=1024)</code></pre><h3>After Change</h3><pre><code class='java'>
        inner_product = p * q

        bi_interaction = inner_product
        <a id="change">attention_temp =  tf.nn.relu(tf.nn.bias_add(tf.tensordot(bi_interaction,self.attention_W,axes=(-1,0)),self.attention_b))</a>
        &#47&#47  Dense(self.attention_factor,&quotrelu&quot,kernel_regularizer=l2(self.l2_reg_w))(bi_interaction)
        attention_weight =<a id="change">tf</a>.nn.softmax(tf.tensordot(attention_temp,self.projection_h,axes=(-1,0)),dim=1)
        attention_output = tf.reduce_sum(attention_weight*bi_interaction,axis=1)

        attention_output = tf.nn.dropout(attention_output,self.keep_prob,seed=1024)</code></pre>