<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      self._tgt_vocab_size, use_bias=False,
    )

    <a id="change">cell_params = copy.deepcopy(self.params)</a>
    <a id="change">cell_params["num_units"] = self.params[&quotdecoder_cell_units&quot]</a>

    if self._mode == "train":
      dp_input_keep_prob = self.params[&quotdecoder_dp_input_keep_prob&quot]
      dp_output_keep_prob = self.params[&quotdecoder_dp_output_keep_prob&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
    residual_connections = self.params[&quotdecoder_use_skip_connections&quot]

    &#47&#47 list of cells
    self._decoder_cells = <a id="change">[
      single_cell(cell_class=self.params[&quotcore_cell&quot],
                  cell_params=self.params.get(&quotcore_cell_params&quot, {}),
                  dp_input_keep_prob=dp_input_keep_prob,
                  dp_output_keep_prob=dp_output_keep_prob,
                  &#47&#47 residual connections are added a little differently for GNMT
                  residual_connections=False if self.params[&quotattention_type&quot].startswith(&quotgnmt&quot) else residual_connections,
                  ) for _ in range(self.params[&quotdecoder_layers&quot])]</a>

    attention_mechanism = self._build_attention(
      encoder_outputs,
      enc_src_lengths,
    )
    if self.params[&quotattention_type&quot].startswith(&quotgnmt&quot):
      attention_cell = self._decoder_cells.pop(0)
      attention_cell = AttentionWrapper(
        attention_cell,
        attention_mechanism=attention_mechanism,
        attention_layer_size=None,
        output_attention=False,
        name="gnmt_attention")
      attentive_decoder_cell = GNMTAttentionMultiCell(
        attention_cell, self._add_residual_wrapper(self._decoder_cells) if residual_connections else self._decoder_cells,
        use_new_attention=(self.params[&quotattention_type&quot] == &quotgnmt_v2&quot))
    else:
      &#47&#47 attentive_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
      attentive_decoder_cell = AttentionWrapper(
        cell=<a id="change">tf.contrib.rnn.MultiRNNCell(self._decoder_cells)</a>,
        attention_mechanism=attention_mechanism,
      )
    if self._mode == "train":</code></pre>